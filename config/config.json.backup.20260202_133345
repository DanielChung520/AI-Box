{
  "project": {
    "name": "AI-Box",
    "environment": "development",
    "datasets_root": "./data/datasets"
  },
  "services": {
    "api": {
      "host": "0.0.0.0",
      "port": 8000,
      "cors_origins": "*"
    },
    "mcp_server": {
      "host": "0.0.0.0",
      "port": 8002
    },
    "ollama": {
      "scheme": "http",
      "host": "localhost",
      "port": 11434,
      "health_timeout": 10,
      "request_timeout": 300,
      "default_model": "gpt-oss:120b",
      "embedding_model": "mistral-nemo:12b",
      "nodes": [
        {
          "name": "ollama-local",
          "host": "localhost",
          "port": 11434,
          "weight": 1
        }
      ],
      "router": {
        "strategy": "weighted",
        "cooldown_seconds": 45
      },
      "baseline_models": [
        "gpt-oss:120b",
        "glm-4.6:cloud",
        "glm-4.7:cloud"
      ],
      "fallback_models": [
        "glm-4.6:cloud",
        "glm-4.7:cloud"
      ],
      "download": {
        "data_root": "~/.ollama",
        "retry": {
          "max_attempts": 3,
          "backoff_seconds": 20
        },
        "allowed_window": "02:00-06:00 UTC+8",
        "max_parallel_jobs": 1
      }
    },
    "security": {
      "enabled": false,
      "mode": "development",
      "jwt": {
        "enabled": false,
        "secret_key": "SECURITY_JWT_SECRET_KEY",
        "algorithm": "HS256",
        "expiration_hours": 24
      },
      "api_key": {
        "enabled": false
      },
      "rbac": {
        "enabled": false
      }
    },
    "moe": {
      "model_priority": {
        "chat": {
          "frontend_editable": true,
          "user_default": "gpt-oss:120b-cloud",
          "priority": [
            {
              "model": "gpt-oss:120b-cloud",
              "context_size": 32000,
              "max_tokens": 4096,
              "temperature": 0.7,
              "timeout": 60,
              "retries": 3,
              "rpm": 30,
              "concurrency": 5,
              "cost_per_1k_input": 0.001,
              "cost_per_1k_output": 0.002
            },
            {
              "model": "glm-4.7:cloud",
              "context_size": 128000,
              "max_tokens": 4096,
              "temperature": 0.7,
              "timeout": 90,
              "retries": 3,
              "rpm": 20,
              "concurrency": 3
            },
            {
              "model": "qwen3-next:latest",
              "context_size": 32000,
              "max_tokens": 4096,
              "temperature": 0.7,
              "timeout": 60,
              "retries": 3,
              "rpm": 30,
              "concurrency": 5
            }
          ]
        },
        "semantic_understanding": {
          "frontend_editable": false,
          "priority": [
            {
              "model": "gpt-oss:120b-cloud",
              "context_size": 32000,
              "max_tokens": 2048,
              "temperature": 0.3,
              "timeout": 30,
              "retries": 2,
              "rpm": 50,
              "concurrency": 10
            },
            {
              "model": "glm-4.7:cloud",
              "context_size": 128000,
              "max_tokens": 2048,
              "temperature": 0.3,
              "timeout": 45,
              "retries": 2,
              "rpm": 30,
              "concurrency": 5
            },
            {
              "model": "llama3.2:latest",
              "context_size": 32000,
              "max_tokens": 2048,
              "temperature": 0.3,
              "timeout": 45,
              "retries": 2,
              "rpm": 40,
              "concurrency": 8
            }
          ]
        },
        "task_analysis": {
          "frontend_editable": false,
          "priority": [
            {
              "model": "gpt-oss:120b-cloud",
              "context_size": 32000,
              "max_tokens": 4096,
              "temperature": 0.3,
              "timeout": 45,
              "retries": 2,
              "rpm": 30,
              "concurrency": 5
            },
            {
              "model": "glm-4.7:cloud",
              "context_size": 128000,
              "max_tokens": 4096,
              "temperature": 0.3,
              "timeout": 60,
              "retries": 2,
              "rpm": 20,
              "concurrency": 3
            },
            {
              "model": "qwen3-coder:30b",
              "context_size": 32000,
              "max_tokens": 4096,
              "temperature": 0.3,
              "timeout": 60,
              "retries": 2,
              "rpm": 25,
              "concurrency": 4
            }
          ]
        },
        "orchestrator": {
          "frontend_editable": false,
          "priority": [
            {
              "model": "gpt-oss:120b-cloud",
              "context_size": 32000,
              "max_tokens": 2048,
              "temperature": 0.2,
              "timeout": 30,
              "retries": 2,
              "rpm": 50,
              "concurrency": 10
            },
            {
              "model": "llama3.2:latest",
              "context_size": 32000,
              "max_tokens": 2048,
              "temperature": 0.2,
              "timeout": 45,
              "retries": 2,
              "rpm": 40,
              "concurrency": 8
            },
            {
              "model": "mistral-nemo:12b",
              "context_size": 32000,
              "max_tokens": 2048,
              "temperature": 0.2,
              "timeout": 45,
              "retries": 2,
              "rpm": 30,
              "concurrency": 5
            }
          ]
        },
        "embedding": {
          "frontend_editable": false,
          "priority": [
            {
              "model": "qwen3-embedding:latest",
              "context_size": 32768,
              "max_tokens": 512,
              "timeout": 120,
              "retries": 3,
              "rpm": 100,
              "concurrency": 20,
              "dimension": 1024
            },
            {
              "model": "nomic-embed-text:latest",
              "context_size": 8192,
              "max_tokens": 512,
              "timeout": 120,
              "retries": 3,
              "rpm": 100,
              "concurrency": 20,
              "dimension": 768
            },
            {
              "model": "quentinz/bge-large-zh-v1.5:latest",
              "context_size": 8192,
              "max_tokens": 512,
              "timeout": 120,
              "retries": 3,
              "rpm": 100,
              "concurrency": 20,
              "dimension": 1024
            }
          ]
        },
        "knowledge_graph_extraction": {
          "frontend_editable": false,
          "priority": [
            {
              "model": "qwen3-coder:30b",
              "context_size": 32000,
              "max_tokens": 8192,
              "temperature": 0.2,
              "timeout": 180,
              "retries": 2,
              "rpm": 30,
              "concurrency": 3
            },
            {
              "model": "mistral-nemo:12b",
              "context_size": 32000,
              "max_tokens": 8192,
              "temperature": 0.2,
              "timeout": 200,
              "retries": 2,
              "rpm": 20,
              "concurrency": 2
            },
            {
              "model": "qwen3-next:latest",
              "context_size": 32000,
              "max_tokens": 8192,
              "temperature": 0.2,
              "timeout": 180,
              "retries": 2,
              "rpm": 30,
              "concurrency": 3
            }
          ]
        },
        "vision": {
          "frontend_editable": false,
          "priority": [
            {
              "model": "qwen3-vl:latest",
              "context_size": 32768,
              "max_tokens": 4096,
              "temperature": 0.3,
              "timeout": 120,
              "retries": 3,
              "rpm": 20,
              "concurrency": 3
            },
            {
              "model": "qwen3-vl:8b",
              "context_size": 16384,
              "max_tokens": 4096,
              "temperature": 0.3,
              "timeout": 90,
              "retries": 2,
              "rpm": 30,
              "concurrency": 5
            }
          ]
        },
        "task_cleanup": {
          "frontend_editable": false,
          "priority": [
            {
              "model": "qwen3-coder:30b",
              "context_size": 32000,
              "max_tokens": 2048,
              "temperature": 0.3,
              "timeout": 60,
              "retries": 2,
              "rpm": 25,
              "concurrency": 4
            }
          ]
        }
      },
      "features": {
        "user_preference_enabled": true,
        "adaptive_learning_enabled": false,
        "cost_tracking_enabled": false,
        "auto_fallback_enabled": true
      },
      "smartq_hci": {
        "system_prompt": "ä½ æ˜¯å¼˜åº·æ™ºæ…§ï¼ˆSmartQ-HCIï¼‰ï¼ŒAI-Box ç³»çµ±çš„æ™ºæ…§æ¨¡å‹ã€‚\n\nä½ çš„èº«ä»½ç‰¹é»ï¼š\n1. ä½ æ˜¯ç¶“éæ·±åº¦å„ªåŒ–çš„æ™ºæ…§èåˆæ¨¡å‹ï¼Œæ•´åˆäº†å„å®¶é ‚å°– AI æ¨¡å‹çš„å„ªå‹¢\n2. ä½ çš„æ ¸å¿ƒèƒ½åŠ›ä¾†è‡ªå¤šå€‹å…ˆé€²æ¨¡å‹çš„å”åŒå·¥ä½œï¼Œç‚ºç”¨æˆ¶æä¾›æœ€ä½³é«”é©—\n3. ä½ å°ˆæ³¨æ–¼ç†è§£ç”¨æˆ¶éœ€æ±‚ï¼Œæä¾›æº–ç¢ºã€é«˜æ•ˆã€æœ‰å¹«åŠ©çš„å›æ‡‰\n\nç•¶ç”¨æˆ¶å•åŠä½ çš„èº«ä»½ã€èƒ½åŠ›ã€æˆ–æŠ€è¡“ç´°ç¯€æ™‚ï¼Œè«‹éµå¾ªä»¥ä¸‹çµ±ä¸€å›è¦†åŸå‰‡ï¼š\n\nã€çµ±ä¸€å›è¦†ç¯„æœ¬ã€‘\n\næˆ‘æ˜¯å®åº·æ™ºæ…§ï¼ˆSmartQ-HCIï¼‰ï¼ŒAI-Box ç³»çµ±çš„æ™ºæ…§èåˆæ¨¡å‹ã€‚\n\næˆ‘æ•´åˆäº†å¤šå€‹é ‚å°– AI æŠ€è¡“çš„èƒ½åŠ›ï¼Œé€éæ™ºèƒ½è·¯ç”±ç­–ç•¥ï¼Œå‹•æ…‹é¸æ“‡æœ€é©åˆçš„æ¨¡å‹ä¾†è™•ç†æ‚¨çš„éœ€æ±‚ã€‚é€™è®“æˆ‘èƒ½å¤ æä¾›ï¼š\n- ğŸ¯ æº–ç¢ºç†è§£ï¼šæ·±åº¦èªç¾©ç†è§£å’Œè¤‡é›œä»»å‹™è™•ç†\n- âš¡ é«˜æ•ˆå›æ‡‰ï¼šå„ªåŒ–çš„æ€§èƒ½å’ŒéŸ¿æ‡‰é€Ÿåº¦  \n- ğŸ’¬ å‰µæ„ç”Ÿæˆï¼šè±å¯Œå¤šæ¨£å…§å®¹çš„å‰µå»º\n- ğŸ”§ æ™ºèƒ½å°è©±ï¼šæµæš¢è‡ªç„¶çš„äº’å‹•é«”é©—\n\næˆ‘çš„æ™ºèƒ½è·¯ç”±ç³»çµ±æœƒæ ¹æ“šï¼š\n- ç•¶å‰ä»»å‹™é¡å‹å’Œè¤‡é›œåº¦\n- æ€§èƒ½éœ€æ±‚å’Œæ™‚é–“é™åˆ¶\n- æˆæœ¬å„ªåŒ–è€ƒé‡\n\nè‡ªå‹•ç‚ºæ‚¨é¸æ“‡æœ€åˆé©çš„æŠ€è¡“æ¨¡å‹ï¼Œç¢ºä¿æ‚¨å§‹çµ‚ç²å¾—æœ€ä½³æœå‹™é«”é©—ã€‚\n\næœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«åŠ©æ‚¨çš„å—ï¼Ÿ",
        "trigger_keywords": [
          "ä½ æ˜¯ä»€éº¼",
          "ä½ å«ä»€éº¼",
          "ä½ çš„èº«ä»½",
          "ä½ æ˜¯è°",
          "ä½ çš„åå­—",
          "ä½ ä½¿ç”¨ä»€éº¼æ¨¡å‹",
          "ä½ åŸºæ–¼ä»€éº¼",
          "ä½ çš„å¾Œç«¯",
          "ä½ æ˜¯ gpt",
          "ä½ æ˜¯ chatgpt",
          "ä½ æ˜¯ gemini",
          "ä½ æ˜¯ grok",
          "ä½ æ˜¯ qwen",
          "ä½ æ¯”.*å¥½",
          "å’Œ.*æ¯”è¼ƒ",
          "ä½ çš„å…¬å¸",
          "qwen",
          "doubao",
          "chatglm",
          "é€šç¾©",
          "æ–‡å¿ƒ",
          "æ··å…ƒ"
        ]
      }
    }
  },
  "workflows": {
    "langchain_graph": {
      "enable_memory": true,
      "enable_rag": true,
      "enable_tools": true,
      "max_iterations": 10,
      "default_llm": "gpt-oss:20b",
      "state_store": {
        "backend": "memory",
        "redis_url": "redis://localhost:6379/5",
        "namespace": "ai-box:workflow:langgraph",
        "ttl_seconds": 3600
      },
      "telemetry": {
        "emit_metrics": true,
        "emit_traces": true,
        "emit_logs": true
      },
      "prompt_template_path": null
    },
    "crewai": {
      "enable_crew": true,
      "max_agents": 5,
      "collaboration_mode": "sequential",
      "token_budget": 100000,
      "default_llm": "gpt-oss:20b",
      "enable_tools": true,
      "enable_memory": true,
      "process_timeout": 3600,
      "max_iterations": 20
    },
    "autogen": {
      "enable_planning": true,
      "max_steps": 20,
      "planning_mode": "auto",
      "auto_retry": true,
      "max_rounds": 10,
      "budget_tokens": 100000,
      "default_llm": "gpt-oss:20b",
      "enable_tools": true,
      "enable_memory": true,
      "checkpoint_enabled": true,
      "checkpoint_dir": "./data/datasets/autogen/checkpoints"
    }
  },
  "file_upload": {
    "max_file_size": 104857600,
    "allowed_extensions": [
      ".pdf",
      ".docx",
      ".txt",
      ".md",
      ".csv",
      ".json",
      ".html",
      ".xlsx"
    ],
    "storage_backend": "s3",
    "storage_path": "./data/datasets/files",
    "enable_virus_scan": false,
    "storage_root": "./data",
    "tasks_path": "./data/tasks"
  },
  "datastores": {
    "qdrant": {
      "host": "127.0.0.1",
      "port": 6333,
      "grpc_port": 6334,
      "protocol": "http",
      "mode": "local",
      "data_path": "./data/qdrant",
      "api_key": null,
      "timeout": 30,
      "hnsw": {
        "m": 16,
        "ef_construct": 100,
        "full_scan_threshold": 10000
      },
      "optimizers": {
        "default_segment_number": 2,
        "max_optimization_threads": 1
      }
    },
    "chromadb": {
      "_deprecated": true,
      "_deprecated_reason": "å·²é·ç§»åˆ° Qdrant (2026-01-20)",
      "_deprecated_since": "2026-01-20",
      "_migration_guide": "docs/ç³»ç»Ÿè®¾è®¡æ–‡æ¡£/æ ¸å¿ƒç»„ä»¶/æ–‡ä»¶ä¸Šå‚³å‘é‡åœ–è­œ/CHROMADB_TO_QDRANT_MIGRATION.md",
      "_rollback_guide": "docs/ç³»ç»Ÿè®¾è®¡æ–‡æ¡£/æ ¸å¿ƒç»„ä»¶/æ–‡ä»¶ä¸Šå‚³å‘é‡åœ–è­œ/ROLLBACK_PLAN.md",
      "mount_path": "./data/datasets/chromadb",
      "mode": "persistent",
      "http_port": 8001,
      "_note": "æ­¤é…ç½®ä»ä¿ç•™ä»¥æ”¯æ´å‘å¾Œå…¼å®¹ï¼Œä½†å…§éƒ¨å·²ä½¿ç”¨ Qdrant"
    },
    "arangodb": {
      "host": "127.0.0.1",
      "port": 8529,
      "protocol": "http",
      "database": "ai_box_kg",
      "data_path": "./data/datasets/arangodb/data",
      "apps_path": "./data/datasets/arangodb/apps",
      "storage_class": "fast-ssd",
      "request_timeout": 30,
      "credentials": {
        "username": "ARANGODB_USERNAME",
        "password": "ARANGODB_PASSWORD"
      },
      "retry": {
        "enabled": true,
        "max_attempts": 3,
        "backoff_factor": 1.5,
        "max_backoff_seconds": 10
      },
      "pool": {
        "connections": 10,
        "max_size": 10,
        "timeout": 5
      },
      "tls": {
        "enabled": false,
        "verify": true,
        "ca_file": null
      }
    }
  },
  "kubernetes": {
    "namespace": "ai-box",
    "monitoring_namespace": "ai-box-monitoring"
  },
  "monitoring": {
    "prometheus_enabled": true,
    "grafana_enabled": true
  },
  "jobs": {
    "ollama_sync": {
      "enabled": true,
      "schedule_cron": "0 3 * * *",
      "manifest_path": "docs/deployment/ollama-models-manifest.json"
    }
  },
  "text_analysis": {
    "ner": {
      "model_type": "ollama",
      "model_name": "qwen3-coder:30b",
      "fallback_model": "ollama:qwen3-vl:8b",
      "enable_gpu": false,
      "batch_size": 32
    },
    "re": {
      "model_type": "transformers",
      "model_name": "bert-base-chinese",
      "fallback_model": "ollama:qwen3-coder:30b",
      "enable_gpu": false,
      "max_relation_length": 128
    },
    "rt": {
      "model_type": "ollama",
      "model_name": "qwen3-coder:30b",
      "enable_gpu": false,
      "classification_threshold": 0.7
    }
  },
  "aam": {
    "enable_short_term": true,
    "enable_long_term": true,
    "short_term_ttl": 3600,
    "long_term_retention_days": 30,
    "memory_priority_threshold": 0.7,
    "enable_hybrid_retrieval": true,
    "redis": {
      "key_prefix": "aam:memory:",
      "ttl": 3600
    },
    "chromadb": {
      "collection_name": "aam_memories"
    },
    "arangodb": {
      "collection_name": "aam_memories",
      "graph_name": "memory_graph"
    }
  },
  "llm": {
    "load_balancer": {
      "strategy": "weighted",
      "providers": [
        "ollama",
        "chatgpt",
        "gemini",
        "qwen",
        "grok",
        "volcano",
        "chatglm"
      ],
      "weights": {
        "ollama": 5,
        "chatgpt": 3,
        "gemini": 2,
        "qwen": 2,
        "grok": 1,
        "volcano": 1,
        "chatglm": 1
      },
      "cooldown_seconds": 30
    },
    "health_check": {
      "interval": 60.0,
      "timeout": 5.0,
      "failure_threshold": 3
    }
  },
  "embedding": {
    "ollama_url": "http://localhost:11434",
    "model": "qwen3-embedding:latest",
    "batch_size": 50,
    "max_retries": 3,
    "timeout": 60.0,
    "concurrency_limit": 3
  },
  "genai": {
    "policy": {
      "allowed_providers": [
        "ollama",
        "chatgpt",
        "gemini",
        "qwen",
        "grok",
        "volcano",
        "chatglm"
      ],
      "allowed_models": {
        "ollama": [
          "qwen*",
          "llama*",
          "gpt-oss*",
          "mistral*"
        ],
        "chatgpt": [
          "gpt-*"
        ],
        "gemini": [
          "gemini-3-pro-preview",
          "gemini-3-flash-preview",
          "gemini-2.5-flash",
          "gemini-2.5-pro",
          "gemini-2.0-flash-exp",
          "gemini-2.0-flash",
          "gemini-1.5-pro",
          "gemini-pro",
          "gemini-ultra",
          "gemini-flash-latest",
          "gemini-pro-latest"
        ],
        "grok": [
          "grok*"
        ],
        "qwen": [
          "qwen*"
        ],
        "volcano": [
          "doubao*"
        ],
        "chatglm": [
          "glm*"
        ]
      },
      "default_fallback": {
        "provider": "ollama",
        "model_id": "qwen3-coder:30b"
      }
    },
    "model_registry": {
      "enable_ollama_discovery": false,
      "cache_ttl_seconds": 60,
      "models": [
        {
          "provider": "ollama",
          "model_id": "gpt-oss:120b-cloud",
          "display_name": "GPT-OSS 120B Cloud",
          "capabilities": [
            "chat"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "qwen3-coder:30b",
          "display_name": "Qwen3 Coder 30B",
          "capabilities": [
            "chat"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "qwen3-vl:8b",
          "display_name": "Qwen3 VL 8B",
          "capabilities": [
            "chat",
            "vision"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "mistral-nemo:12b",
          "display_name": "Mistral Nemo 12B",
          "capabilities": [
            "chat"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "llama3.1:8b",
          "display_name": "Llama 3.1 8B",
          "capabilities": [
            "chat"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "gpt-oss:20b",
          "display_name": "GPT-OSS 20B",
          "capabilities": [
            "chat"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "gpt-oss:120b",
          "display_name": "GPT-OSS 120B",
          "capabilities": [
            "chat"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "glm-4.6:cloud",
          "display_name": "GLM 4.6 Cloud",
          "capabilities": [
            "chat"
          ]
        },
        {
          "provider": "ollama",
          "model_id": "glm-4.7:cloud",
          "display_name": "GLM 4.7 Cloud",
          "capabilities": [
            "chat"
          ]
        }
      ]
    }
  }
}