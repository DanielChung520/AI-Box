#!/usr/bin/env python3
# ä»£ç¢¼åŠŸèƒ½èªªæ˜: LLM æ¨¡å‹é·ç§»åˆ° ArangoDB é·ç§»è…³æœ¬
# å‰µå»ºæ—¥æœŸ: 2025-12-20
# å‰µå»ºäºº: Daniel Chung
# æœ€å¾Œä¿®æ”¹æ—¥æœŸ: 2026-01-22

"""
å°‡å‰ç«¯ç¡¬ç·¨ç¢¼çš„ LLM æ¨¡å‹åˆ—è¡¨é·ç§»åˆ° ArangoDB
ä¸¦æ›´æ–°ç‹€æ…‹ç‚º ACTIVE

ä½¿ç”¨æ–¹æ³•:
    python -m services.api.services.migrations.migrate_llm_models
"""

import sys
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from services.api.models.llm_model import (  # noqa: E402
    LLMModelCreate,
    LLMModelUpdate,
    LLMProvider,
    ModelCapability,
    ModelStatus,
)
from services.api.services.llm_model_service import get_llm_model_service  # noqa: E402

# å®Œæ•´çš„æ¨¡å‹åˆ—è¡¨æ•¸æ“šï¼ˆå¾å‰ç«¯ç¡¬ç·¨ç¢¼é·ç§»ä¸¦æ“´å±•ï¼‰
LLM_MODELS_DATA = [
    # Auto é¸é …ï¼ˆç‰¹æ®Šè™•ç†ï¼‰
    {
        "model_id": "auto",
        "name": "Auto",
        "provider": LLMProvider.AUTO,
        "description": "è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹",
        "capabilities": [ModelCapability.CHAT],
        "status": ModelStatus.ACTIVE,
        "order": 0,
        "icon": "fa-magic",
        "color": "text-purple-400",
        "is_default": True,
    },
    # Ollama æ¨¡å‹ (æ‰‹å‹•æ·»åŠ )
    {
        "model_id": "gpt-oss:120b-cloud",
        "name": "GPT-OSS 120B Cloud",
        "provider": LLMProvider.OLLAMA,
        "description": "GPT-OSS 120B é›²ç«¯è¨—ç®¡ç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "order": 5,
        "icon": "fa-cloud",
        "color": "text-blue-500",
    },
    {
        "model_id": "gpt-oss:20b",
        "name": "GPT-OSS 20B",
        "provider": LLMProvider.OLLAMA,
        "description": "GPT-OSS 20B æœ¬åœ°ç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "order": 6,
        "icon": "fa-microchip",
        "color": "text-blue-400",
    },
    # OpenAI (ChatGPT) æ¨¡å‹
    {
        "model_id": "gpt-4o",
        "name": "GPT-4o",
        "provider": LLMProvider.OPENAI,
        "description": "GPT-4 Optimized - æœ€æ–°çš„ GPT-4 å„ªåŒ–ç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.MULTIMODAL,
            ModelCapability.VISION,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 128000,
        "parameters": "~1.8T",
        "order": 30,
        "icon": "fa-robot",
        "color": "text-green-400",
        "is_default": True,
    },
    {
        "model_id": "gpt-4-turbo",
        "name": "GPT-4 Turbo",
        "provider": LLMProvider.OPENAI,
        "description": "GPT-4 Turbo - å¿«é€ŸéŸ¿æ‡‰ç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.VISION,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 128000,
        "parameters": "~1.8T",
        "order": 40,
        "icon": "fa-robot",
        "color": "text-green-400",
    },
    # Google (Gemini) æ¨¡å‹
    {
        "model_id": "gemini-3-pro-preview",
        "name": "Gemini 3 Pro (Preview)",
        "provider": LLMProvider.GOOGLE,
        "description": "Gemini 3 Pro - æœ€æ–°æ——è‰¦æ¨¡å‹",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.MULTIMODAL,
            ModelCapability.VISION,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 2000000,
        "parameters": "~540B",
        "order": 65,
        "icon": "fa-gem",
        "color": "text-blue-400",
        "is_default": True,
    },
    # Anthropic (Claude) æ¨¡å‹
    {
        "model_id": "claude-3.5-sonnet",
        "name": "Claude 3.5 Sonnet",
        "provider": LLMProvider.ANTHROPIC,
        "description": "Claude 3.5 Sonnet - å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.VISION,
            ModelCapability.REASONING,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 200000,
        "parameters": "~250B",
        "order": 110,
        "icon": "fa-brain",
        "color": "text-orange-400",
        "is_default": True,
    },
    # é˜¿é‡Œå·´å·´ (Qwen) æ¨¡å‹
    {
        "model_id": "qwen-2.5-72b-instruct",
        "name": "Qwen 2.5 72B Instruct",
        "provider": LLMProvider.ALIBABA,
        "description": "Qwen 2.5 72B - å¤§å‹æŒ‡ä»¤æ¨¡å‹",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 32768,
        "parameters": "72B",
        "order": 150,
        "icon": "fa-code",
        "color": "text-orange-400",
    },
    {
        "model_id": "qwen-plus",
        "name": "Qwen Plus",
        "provider": LLMProvider.ALIBABA,
        "description": "Qwen Plus - å¢å¼·ç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 32000,
        "order": 160,
        "icon": "fa-code",
        "color": "text-orange-400",
        "is_default": True,
    },
    {
        "model_id": "qwen-turbo",
        "name": "Qwen Turbo",
        "provider": LLMProvider.ALIBABA,
        "description": "Qwen Turbo - å¿«é€Ÿç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 8000,
        "order": 170,
        "icon": "fa-code",
        "color": "text-orange-400",
    },
    # xAI (Grok) æ¨¡å‹
    {
        "model_id": "grok-2",
        "name": "Grok-2",
        "provider": LLMProvider.XAI,
        "description": "Grok-2 - æœ€æ–°ç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.REASONING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 131072,
        "parameters": "~314B",
        "order": 190,
        "icon": "fa-bolt",
        "color": "text-yellow-400",
        "is_default": True,
    },
    # æ™ºè­œ AI (ChatGLM) æ¨¡å‹
    {
        "model_id": "glm-4",
        "name": "GLM-4",
        "provider": LLMProvider.CHATGLM,
        "description": "GLM-4 - æ™ºè­œ AI æœ€æ–°å°è©±æ¨¡å‹",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 128000,
        "order": 310,
        "icon": "fa-brain",
        "color": "text-blue-600",
        "is_default": True,
    },
    {
        "model_id": "glm-4v",
        "name": "GLM-4V",
        "provider": LLMProvider.CHATGLM,
        "description": "GLM-4V - æ™ºè­œ AI å¤šæ¨¡æ…‹è¦–è¦ºæ¨¡å‹",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.MULTIMODAL,
            ModelCapability.VISION,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 128000,
        "order": 320,
        "icon": "fa-eye",
        "color": "text-blue-600",
    },
    {
        "model_id": "glm-3-turbo",
        "name": "GLM-3 Turbo",
        "provider": LLMProvider.CHATGLM,
        "description": "GLM-3 Turbo - å¿«é€Ÿç‰ˆæœ¬",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 32000,
        "order": 330,
        "icon": "fa-bolt",
        "color": "text-blue-600",
    },
    # å­—ç¯€è·³å‹•ç«å±±å¼•æ“ (Volcano Engine / Doubao) æ¨¡å‹
    {
        "model_id": "doubao-pro-4k",
        "name": "è±†åŒ… Pro 4K",
        "provider": LLMProvider.VOLCANO,
        "description": "è±†åŒ… Pro 4K - ç«å±±å¼•æ“å°ˆæ¥­ç‰ˆæ¨¡å‹ï¼ˆ4K ä¸Šä¸‹æ–‡ï¼‰",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 4096,
        "order": 340,
        "icon": "fa-fire",
        "color": "text-orange-500",
        "is_default": True,
    },
    {
        "model_id": "doubao-pro-32k",
        "name": "è±†åŒ… Pro 32K",
        "provider": LLMProvider.VOLCANO,
        "description": "è±†åŒ… Pro 32K - ç«å±±å¼•æ“å°ˆæ¥­ç‰ˆæ¨¡å‹ï¼ˆ32K ä¸Šä¸‹æ–‡ï¼‰",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.CODE,
            ModelCapability.FUNCTION_CALLING,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 32768,
        "order": 350,
        "icon": "fa-fire",
        "color": "text-orange-500",
    },
    {
        "model_id": "doubao-lite-4k",
        "name": "è±†åŒ… Lite 4K",
        "provider": LLMProvider.VOLCANO,
        "description": "è±†åŒ… Lite 4K - ç«å±±å¼•æ“è¼•é‡ç‰ˆæ¨¡å‹ï¼ˆ4K ä¸Šä¸‹æ–‡ï¼‰",
        "capabilities": [
            ModelCapability.CHAT,
            ModelCapability.COMPLETION,
            ModelCapability.STREAMING,
        ],
        "status": ModelStatus.ACTIVE,
        "context_window": 4096,
        "order": 360,
        "icon": "fa-fire",
        "color": "text-orange-400",
    },
]


def migrate():
    """åŸ·è¡Œé·ç§»"""
    print("é–‹å§‹é·ç§»ä¸¦æ¿€æ´» LLM æ¨¡å‹åˆ° ArangoDB...")
    service = get_llm_model_service()

    created_count = 0
    updated_count = 0
    error_count = 0

    for model_data in LLM_MODELS_DATA:
        try:
            model_id = model_data["model_id"]

            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
            existing = service.get_by_id(model_id)
            if existing:
                # æ›´æ–°ç¾æœ‰æ¨¡å‹ç‚º ACTIVE ä¸¦æ›´æ–°å…¶ä»–å±¬æ€§
                update_req = LLMModelUpdate(
                    **{k: v for k, v in model_data.items() if k != "model_id"}
                )
                update_req.status = ModelStatus.ACTIVE
                service.update(model_id, update_req)
                print(f"  ğŸ”„ æ›´æ–°ä¸¦æ¿€æ´»æ¨¡å‹: {model_id}")
                updated_count += 1
                continue

            # å‰µå»ºæ¨¡å‹
            model_create = LLMModelCreate(**model_data)
            service.create(model_create)
            print(f"  âœ… å‰µå»ºä¸¦æ¿€æ´»æ¨¡å‹: {model_id} ({model_data['name']})")
            created_count += 1

        except Exception as e:
            print(f"  âŒ è™•ç†æ¨¡å‹å¤±æ•— {model_data.get('model_id', 'unknown')}: {e}")
            error_count += 1

    print("\né·ç§»èˆ‡æ¿€æ´»å®Œæˆ!")
    print(f"  âœ… å‰µå»º: {created_count} å€‹æ¨¡å‹")
    print(f"  ğŸ”„ æ›´æ–°: {updated_count} å€‹æ¨¡å‹")
    print(f"  âŒ éŒ¯èª¤: {error_count} å€‹æ¨¡å‹")


if __name__ == "__main__":
    migrate()
