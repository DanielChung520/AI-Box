# ä»£ç¢¼åŠŸèƒ½èªªæ˜: ä¸Šå‚³æœ¬åœ°æ¸¬è©¦æ•¸æ“šåˆ° SeaweedFS Datalake
# å‰µå»ºæ—¥æœŸ: 2026-01-13
# å‰µå»ºäºº: Daniel Chung
# æœ€å¾Œä¿®æ”¹æ—¥æœŸ: 2026-01-13

"""ä¸Šå‚³æœ¬åœ°æ¸¬è©¦æ•¸æ“šåˆ° SeaweedFS Datalake

æ­¤è…³æœ¬å¾æœ¬åœ°æ–‡ä»¶è®€å–æ¸¬è©¦æ•¸æ“šä¸¦ä¸Šå‚³åˆ° SeaweedFS Datalakeã€‚
æœ¬åœ°æ•¸æ“šç”± init_datalake_test_data.py ç”Ÿæˆåœ¨ scripts/datalake_test_data/ ç›®éŒ„ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/upload_datalake_test_data.py
"""

import os
import sys
from pathlib import Path

from dotenv import load_dotenv

project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))
env_path = project_root / ".env"
load_dotenv(dotenv_path=env_path)

try:
    from storage.s3_storage import S3FileStorage, SeaweedFSService
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥ S3FileStorage: {e}")
    print("è«‹ç¢ºä¿å·²å®‰è£æ‰€éœ€ä¾è³´ï¼špip install boto3")
    sys.exit(1)


def upload_local_data_to_seaweedfs():
    """å¾æœ¬åœ°æ–‡ä»¶ä¸Šå‚³æ•¸æ“šåˆ° SeaweedFS"""
    local_data_dir = project_root / "scripts" / "datalake_test_data"

    if not local_data_dir.exists():
        print(f"âŒ æœ¬åœ°æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {local_data_dir}")
        print("   è«‹å…ˆé‹è¡Œè…³æœ¬ç”Ÿæˆæœ¬åœ°æ¸¬è©¦æ•¸æ“š")
        return False

    print("ğŸš€ é–‹å§‹ä¸Šå‚³æœ¬åœ°æ¸¬è©¦æ•¸æ“šåˆ° SeaweedFS Datalake...")
    print("=" * 60)

    # ç²å–ç’°å¢ƒè®Šæ•¸
    endpoint = os.getenv("DATALAKE_SEAWEEDFS_S3_ENDPOINT")
    access_key = os.getenv("DATALAKE_SEAWEEDFS_S3_ACCESS_KEY", "")
    secret_key = os.getenv("DATALAKE_SEAWEEDFS_S3_SECRET_KEY", "")
    use_ssl = os.getenv("DATALAKE_SEAWEEDFS_USE_SSL", "false").lower() == "true"

    if not endpoint:
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­ç½® DATALAKE_SEAWEEDFS_S3_ENDPOINT ç’°å¢ƒè®Šæ•¸")
        return False

    # å‰µå»ºå­˜å„²å¯¦ä¾‹
    try:
        storage = S3FileStorage(
            endpoint=endpoint,
            access_key=access_key,
            secret_key=secret_key,
            use_ssl=use_ssl,
            service_type=SeaweedFSService.DATALAKE,
        )
        print(f"âœ… æˆåŠŸé€£æ¥åˆ° SeaweedFS Datalake: {endpoint}")
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° SeaweedFS Datalake: {e}")
        return False

    # Bucket é…ç½®
    assets_bucket = "bucket-datalake-assets"
    dictionary_bucket = "bucket-datalake-dictionary"
    schema_bucket = "bucket-datalake-schema"

    # ç¢ºä¿ Buckets å­˜åœ¨
    print("\nğŸ“¦ ç¢ºä¿ Buckets å­˜åœ¨...")
    for bucket_name in [assets_bucket, dictionary_bucket, schema_bucket]:
        try:
            storage.s3_client.head_bucket(Bucket=bucket_name)
            print(f"  âœ… Bucket '{bucket_name}' å·²å­˜åœ¨")
        except Exception:
            try:
                storage.s3_client.create_bucket(Bucket=bucket_name)
                print(f"  âœ… Bucket '{bucket_name}' å·²å‰µå»º")
            except Exception as e:
                print(f"  âš ï¸  ç„¡æ³•å‰µå»º Bucket '{bucket_name}': {e}")

    success_count = 0
    error_count = 0

    # 1. ä¸Šå‚³ç‰©æ–™æ•¸æ“š
    print("\nğŸ“¦ ä¸Šå‚³ç‰©æ–™æ•¸æ“š...")
    parts_dir = local_data_dir / "parts"
    if parts_dir.exists():
        for part_file in parts_dir.glob("*.json"):
            try:
                with open(part_file, "r", encoding="utf-8") as f:
                    content = f.read()
                key = f"parts/{part_file.name}"
                storage.s3_client.put_object(
                    Bucket=assets_bucket,
                    Key=key,
                    Body=content.encode("utf-8"),
                    ContentType="application/json",
                )
                print(f"  âœ… {part_file.name}")
                success_count += 1
            except Exception as e:
                print(f"  âŒ {part_file.name}: {e}")
                error_count += 1

    # 2. ä¸Šå‚³åº«å­˜æ•¸æ“š
    print("\nğŸ“Š ä¸Šå‚³åº«å­˜æ•¸æ“š...")
    stock_dir = local_data_dir / "stock"
    if stock_dir.exists():
        for stock_file in stock_dir.glob("*.json"):
            try:
                with open(stock_file, "r", encoding="utf-8") as f:
                    content = f.read()
                key = f"stock/{stock_file.name}"
                storage.s3_client.put_object(
                    Bucket=assets_bucket,
                    Key=key,
                    Body=content.encode("utf-8"),
                    ContentType="application/json",
                )
                print(f"  âœ… {stock_file.name}")
                success_count += 1
            except Exception as e:
                print(f"  âŒ {stock_file.name}: {e}")
                error_count += 1

    # 3. ä¸Šå‚³åº«å­˜æ­·å²è¨˜éŒ„
    print("\nğŸ“œ ä¸Šå‚³åº«å­˜æ­·å²è¨˜éŒ„...")
    history_dir = local_data_dir / "stock_history"
    if history_dir.exists():
        for history_file in history_dir.glob("*.jsonl"):
            try:
                with open(history_file, "r", encoding="utf-8") as f:
                    content = f.read()
                key = f"stock_history/{history_file.name}"
                storage.s3_client.put_object(
                    Bucket=assets_bucket,
                    Key=key,
                    Body=content.encode("utf-8"),
                    ContentType="application/x-ndjson",
                )
                # è¨ˆç®—è¨˜éŒ„æ•¸
                line_count = len(content.strip().split("\n"))
                print(f"  âœ… {history_file.name}: {line_count} ç­†è¨˜éŒ„")
                success_count += line_count
            except Exception as e:
                print(f"  âŒ {history_file.name}: {e}")
                error_count += 50  # ä¼°ç®—

    # 4. ä¸Šå‚³æ•¸æ“šå­—å…¸
    print("\nğŸ“š ä¸Šå‚³æ•¸æ“šå­—å…¸...")
    dict_file = local_data_dir / "dictionary" / "warehouse.json"
    if dict_file.exists():
        try:
            with open(dict_file, "r", encoding="utf-8") as f:
                content = f.read()
            storage.s3_client.put_object(
                Bucket=dictionary_bucket,
                Key="warehouse.json",
                Body=content.encode("utf-8"),
                ContentType="application/json",
            )
            print("  âœ… warehouse.json")
            success_count += 1
        except Exception as e:
            print(f"  âŒ warehouse.json: {e}")
            error_count += 1

    # 5. ä¸Šå‚³ Schema å®šç¾©
    print("\nğŸ“‹ ä¸Šå‚³ Schema å®šç¾©...")
    schema_dir = local_data_dir / "schema"
    if schema_dir.exists():
        for schema_file in schema_dir.glob("*.json"):
            try:
                with open(schema_file, "r", encoding="utf-8") as f:
                    content = f.read()
                storage.s3_client.put_object(
                    Bucket=schema_bucket,
                    Key=schema_file.name,
                    Body=content.encode("utf-8"),
                    ContentType="application/json",
                )
                print(f"  âœ… {schema_file.name}")
                success_count += 1
            except Exception as e:
                print(f"  âŒ {schema_file.name}: {e}")
                error_count += 1

    # ç¸½çµ
    print("\n" + "=" * 60)
    print(f"âœ… æˆåŠŸä¸Šå‚³: {success_count} ç­†æ•¸æ“š")
    if error_count > 0:
        print(f"âŒ å¤±æ•—: {error_count} ç­†æ•¸æ“š")
    print("=" * 60)

    return error_count == 0


if __name__ == "__main__":
    success = upload_local_data_to_seaweedfs()
    sys.exit(0 if success else 1)
