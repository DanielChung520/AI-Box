#!/usr/bin/env python3
# ä»£ç¢¼åŠŸèƒ½èªªæ˜: æ‰¹é‡è™•ç†ç³»çµ±è¨­è¨ˆæ–‡æª”ä¸¦è¨˜éŒ„è©³ç´°é€²åº¦
# å‰µå»ºæ—¥æœŸ: 2025-12-31
# å‰µå»ºäºº: Daniel Chung
# æœ€å¾Œä¿®æ”¹æ—¥æœŸ: 2025-12-31

"""æ‰¹é‡è™•ç†ç³»çµ±è¨­è¨ˆæ–‡æª”ä¸¦è¨˜éŒ„è©³ç´°é€²åº¦è¡¨

ä½¿ç”¨æ–¹æ³•:
    python scripts/kg_extract_all_with_progress.py
"""

import sys
import json
import time
import requests
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from collections import defaultdict

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.resolve()
sys.path.insert(0, str(project_root))

# API é…ç½®
API_BASE_URL = "http://localhost:8000/api/v1"
API_USERNAME = "test"
API_PASSWORD = "test"

# æ–‡æª”ç›®éŒ„
DOCS_DIR = project_root / "docs/ç³»ç»Ÿè®¾è®¡æ–‡æ¡£"

# é€²åº¦è¡¨æ–‡ä»¶
PROGRESS_FILE = project_root / "scripts/kg_extract_progress.json"


class ProcessingProgressTracker:
    """è™•ç†é€²åº¦è¿½è¹¤å™¨"""
    
    def __init__(self, progress_file: Path):
        self.progress_file = progress_file
        self.progress_data = self._load_progress()
    
    def _load_progress(self) -> Dict[str, Any]:
        """åŠ è¼‰é€²åº¦æ•¸æ“š"""
        # å…ˆå®šä¹‰é»˜è®¤æ•°æ®ç»“æ„
        default_data = {

"created_at": datetime.now().isoformat(),

"files": {},

"summary": {

    "total_files": 0,

    "processed_files": 0,

    "failed_files": 0,

    "total_entities": 0,

    "total_relations": 0,

    "total_vectorization_time": 0.0,

    "total_kg_extraction_time": 0.0,

    "total_processing_time": 0.0,

    "total_chunk_count": 0,

    "total_ner_count": 0,

    "total_re_count": 0,

    "total_rt_count": 0,

}
        }
        if self.progress_file.exists():

            try:

                with open(self.progress_file, 'r', encoding='utf-8') as f:

                return json.load(f)

                except Exception as e:

    print(f"âš ï¸  åŠ è¼‰é€²åº¦æ–‡ä»¶å¤±æ•—: {e}")
        return {

"created_at": datetime.now().isoformat(),

"files": {},

"summary": {

    "total_files": 0,

    "processed_files": 0,

    "failed_files": 0,

    "total_entities": 0,

    "total_relations": 0,

    "total_vectorization_time": 0.0,

    "total_kg_extraction_time": 0.0,

    "total_processing_time": 0.0,

    "total_chunk_count": 0,

    "total_ner_count": 0,

    "total_re_count": 0,

    "total_rt_count": 0,

}
        }
    
    def save_progress(self):
        """ä¿å­˜é€²åº¦æ•¸æ“š"""
        self.progress_data["updated_at"] = datetime.now().isoformat()
        try:

with open(self.progress_file, 'w', encoding='utf-8') as f:

    json.dump(self.progress_data, f, indent=2, ensure_ascii=False)
        except Exception as e:

print(f"âš ï¸  ä¿å­˜é€²åº¦æ–‡ä»¶å¤±æ•—: {e}")
    
    def add_file_record(
        self,
        filename: str,
        file_size: int,
        file_id: Optional[str] = None,
        status: str = "pending",
        error: Optional[str] = None,
        vectorization_time: Optional[float] = None,
        kg_extraction_time: Optional[float] = None,
        total_time: Optional[float] = None,
        entities_count: Optional[int] = None,
        relations_count: Optional[int] = None,
        chunk_count: Optional[int] = None,
        ner_count: Optional[int] = None,
        re_count: Optional[int] = None,
        rt_count: Optional[int] = None,
    ):
        """æ·»åŠ æ–‡ä»¶è¨˜éŒ„"""
        if filename not in self.progress_data["files"]:

self.progress_data["files"][filename] = {

    "filename": filename,

    "file_size": file_size,

    "file_size_mb": round(file_size / (1024 * 1024), 2),

    "file_id": file_id,

    "status": status,

    "error": error,

    "vectorization_time": vectorization_time,

    "kg_extraction_time": kg_extraction_time,

    "total_time": total_time,

    "entities_count": entities_count,

    "relations_count": relations_count,

    "chunk_count": chunk_count,

    "ner_count": ner_count,

    "re_count": re_count,

    "rt_count": rt_count,

    "created_at": datetime.now().isoformat(),

    "updated_at": datetime.now().isoformat(),

}
        else:

# æ›´æ–°ç¾æœ‰è¨˜éŒ„

record = self.progress_data["files"][filename]

if file_id:

    record["file_id"] = file_id

if status:

    record["status"] = status

if error is not None:

    record["error"] = error

if vectorization_time is not None:

    record["vectorization_time"] = vectorization_time

if kg_extraction_time is not None:

    record["kg_extraction_time"] = kg_extraction_time

if total_time is not None:

    record["total_time"] = total_time

if entities_count is not None:

    record["entities_count"] = entities_count

if relations_count is not None:

    record["relations_count"] = relations_count

if chunk_count is not None:

    record["chunk_count"] = chunk_count

if ner_count is not None:

    record["ner_count"] = ner_count

if re_count is not None:

    record["re_count"] = re_count

if rt_count is not None:

    record["rt_count"] = rt_count

record["updated_at"] = datetime.now().isoformat()
        
        self._update_summary()
        self.save_progress()
    
    def _update_summary(self):
        """æ›´æ–°æ‘˜è¦çµ±è¨ˆ"""
        files = self.progress_data["files"]
        summary = self.progress_data["summary"]
        
        summary["total_files"] = len(files)
        summary["processed_files"] = sum(1 for f in files.values() if f["status"] == "completed")
        summary["failed_files"] = sum(1 for f in files.values() if f["status"] == "failed")
        summary["total_entities"] = sum(f.get("entities_count", 0) or 0 for f in files.values())
        summary["total_relations"] = sum(f.get("relations_count", 0) or 0 for f in files.values())
        summary["total_vectorization_time"] = sum(f.get("vectorization_time", 0) or 0 for f in files.values())
        summary["total_kg_extraction_time"] = sum(f.get("kg_extraction_time", 0) or 0 for f in files.values())
        summary["total_processing_time"] = sum(f.get("total_time", 0) or 0 for f in files.values())
        summary["total_chunk_count"] = sum(f.get("chunk_count", 0) or 0 for f in files.values())
        summary["total_ner_count"] = sum(f.get("ner_count", 0) or 0 for f in files.values())
        summary["total_re_count"] = sum(f.get("re_count", 0) or 0 for f in files.values())
        summary["total_rt_count"] = sum(f.get("rt_count", 0) or 0 for f in files.values())


def authenticate() -> Optional[str]:
    """ç™»éŒ„ç²å–èªè­‰ token"""
    try:
        response = requests.post(

f"{API_BASE_URL}/auth/login",

json={"username": API_USERNAME, "password": API_PASSWORD},

timeout=30
        )
        if response.status_code == 200:

data = response.json()

return data.get("data", {}).get("access_token")
        else:

print(f"âŒ ç™»éŒ„å¤±æ•—: {response.status_code} - {response.text}")

return None
    except Exception as e:
        print(f"âŒ ç™»éŒ„éŒ¯èª¤: {e}")
        return None


def upload_file(file_path: Path, token: str) -> Optional[str]:
    """ä¸Šå‚³æ–‡ä»¶"""
    try:
        with open(file_path, 'rb') as f:

files = {"files": (file_path.name, f, "text/markdown")}

headers = {'Authorization': f'Bearer {token}'}

response = requests.post(

    f"{API_BASE_URL}/files/upload",

    files=files,

    headers=headers,

    timeout=30

)

if response.status_code == 200:

    data = response.json()

    if data.get("success"):

        # API è¿”å›æ ¼å¼: data.uploaded[0].file_id

        uploaded_files = data.get("data", {}).get("uploaded", [])

        if uploaded_files:


return uploaded_files[0].get("file_id")

    return None

else:

    print(f"âŒ æ–‡ä»¶ä¸Šå‚³å¤±æ•—: {response.status_code} - {response.text}")

    return None
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ä¸Šå‚³éŒ¯èª¤: {e}")
        return None


def get_processing_status(file_id: str, token: str) -> Optional[Dict[str, Any]]:
    """ç²å–è™•ç†ç‹€æ…‹"""
    try:
        headers = {'Authorization': f'Bearer {token}'}
        response = requests.get(

f"{API_BASE_URL}/files/{file_id}/processing-status",

headers=headers,

timeout=30
        )
        if response.status_code == 200:

return response.json().get("data", {})
        else:

return None
    except Exception as e:
        print(f"âš ï¸  ç²å–ç‹€æ…‹éŒ¯èª¤: {e}")
        return None


def wait_for_processing(file_id: str, token: str, timeout: int = 600) -> Dict[str, Any]:
    """ç­‰å¾…è™•ç†å®Œæˆï¼Œè¿”å›è©³ç´°æ™‚é–“ä¿¡æ¯"""
    start_time = time.time()
    vectorization_start = None
    vectorization_end = None
    kg_extraction_start = None
    kg_extraction_end = None
    
    last_vectorization_progress = 0
    last_kg_progress = 0
    
    while time.time() - start_time < timeout:
        status = get_processing_status(file_id, token)
        if not status:

time.sleep(2)

continue
        
        overall_status = status.get("overall_status", "")
        vectorization_status = status.get("vectorization", {}).get("status", "")
        kg_status = status.get("kg_extraction", {}).get("status", "")
        vectorization_progress = status.get("vectorization", {}).get("progress", 0)
        kg_progress = status.get("kg_extraction", {}).get("progress", 0)
        
        # è¨˜éŒ„å‘é‡åŒ–é–‹å§‹æ™‚é–“
        if vectorization_start is None and vectorization_progress > 0:

vectorization_start = time.time()
        
        # è¨˜éŒ„å‘é‡åŒ–çµæŸæ™‚é–“
        if vectorization_end is None and vectorization_status == "completed":

vectorization_end = time.time()
        
        # è¨˜éŒ„åœ–è­œæå–é–‹å§‹æ™‚é–“
        if kg_extraction_start is None and kg_progress > 0:

kg_extraction_start = time.time()
        
        # è¨˜éŒ„åœ–è­œæå–çµæŸæ™‚é–“
        if kg_extraction_end is None and kg_status == "completed":

kg_extraction_end = time.time()
        
        # æ‰“å°é€²åº¦
        if vectorization_progress != last_vectorization_progress or kg_progress != last_kg_progress:

print(f"  å‘é‡åŒ–: {vectorization_progress}% ({vectorization_status}), KGæå–: {kg_progress}% ({kg_status})")

last_vectorization_progress = vectorization_progress

last_kg_progress = kg_progress
        
        if overall_status == "completed":

end_time = time.time()



# è¨ˆç®—å„éšæ®µæ™‚é–“

vectorization_time = None

kg_extraction_time = None

total_time = end_time - start_time



if vectorization_start and vectorization_end:

    vectorization_time = vectorization_end - vectorization_start

elif vectorization_start:

    # å¦‚æœå‘é‡åŒ–å·²é–‹å§‹ä½†æœªè¨˜éŒ„çµæŸæ™‚é–“ï¼Œä½¿ç”¨ç¸½æ™‚é–“çš„ä¸€éƒ¨åˆ†ä¼°ç®—

    vectorization_time = (end_time - vectorization_start) * 0.5



if kg_extraction_start and kg_extraction_end:

    kg_extraction_time = kg_extraction_end - kg_extraction_start

elif kg_extraction_start:

    # å¦‚æœåœ–è­œæå–å·²é–‹å§‹ä½†æœªè¨˜éŒ„çµæŸæ™‚é–“ï¼Œä½¿ç”¨ç¸½æ™‚é–“çš„ä¸€éƒ¨åˆ†ä¼°ç®—

    kg_extraction_time = (end_time - kg_extraction_start) * 0.5



return {

    "status": "completed",

    "vectorization_time": vectorization_time,

    "kg_extraction_time": kg_extraction_time,

    "total_time": total_time,

    "final_status": status,

    "chunk_count": status.get("chunking", {}).get("chunk_count"),

}
        elif overall_status == "failed":

return {

    "status": "failed",

    "error": status.get("error", "Unknown error"),

    "total_time": time.time() - start_time

}
        
        time.sleep(3)
    
    return {
        "status": "timeout",
        "error": f"è™•ç†è¶…æ™‚ï¼ˆè¶…é {timeout} ç§’ï¼‰",
        "total_time": timeout
    }


def get_kg_results(file_id: str, token: str) -> Dict[str, Any]:
    """ç²å–çŸ¥è­˜åœ–è­œæå–çµæœ"""
    try:
        headers = {'Authorization': f'Bearer {token}'}
        response = requests.get(

f"{API_BASE_URL}/files/{file_id}/kg",

headers=headers,

timeout=30
        )
        if response.status_code == 200:

return response.json().get("data", {})
        else:

return {}
    except Exception as e:
        print(f"âš ï¸  ç²å–KGçµæœéŒ¯èª¤: {e}")
        return {}


def find_markdown_files(directory: Path) -> List[Path]:
    """æŸ¥æ‰¾æ‰€æœ‰ Markdown æ–‡ä»¶"""
    markdown_files = []
    for file_path in directory.rglob("*.md"):
        if file_path.is_file():

markdown_files.append(file_path)
    return sorted(markdown_files, key=lambda p: p.stat().st_size)


def main():
    print("=" * 60)
    print("ç³»çµ±è¨­è¨ˆæ–‡æª”æ‰¹é‡è™•ç†ï¼ˆå¸¶é€²åº¦è¿½è¹¤ï¼‰")
    print("=" * 60)
    
    # åˆå§‹åŒ–é€²åº¦è¿½è¹¤å™¨
    tracker = ProcessingProgressTracker(PROGRESS_FILE)
    
    # æŸ¥æ‰¾æ‰€æœ‰ Markdown æ–‡ä»¶
    print(f"\nğŸ“ æƒææ–‡æª”ç›®éŒ„: {DOCS_DIR}")
    markdown_files = find_markdown_files(DOCS_DIR)
    print(f"âœ… æ‰¾åˆ° {len(markdown_files)} å€‹ Markdown æ–‡ä»¶")
    
    # ç™»éŒ„
    print("\nğŸ” ç™»éŒ„ç²å–èªè­‰ token...")
    token = authenticate()
    if not token:
        print("âŒ ç™»éŒ„å¤±æ•—ï¼Œé€€å‡º")
        return
    print("âœ… ç™»éŒ„æˆåŠŸ")
    
    # è™•ç†æ¯å€‹æ–‡ä»¶
    total_files = len(markdown_files)
    for idx, file_path in enumerate(markdown_files, 1):
        filename = file_path.name
        file_size = file_path.stat().st_size
        
        print(f"\n[{idx}/{total_files}] è™•ç†æ–‡ä»¶: {filename}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")
        
        # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
        if filename in tracker.progress_data["files"]:

existing = tracker.progress_data["files"][filename]

if existing["status"] == "completed":

    print(f"   â­ï¸  è·³éï¼ˆå·²è™•ç†å®Œæˆï¼‰")

    continue
        
        # è¨˜éŒ„é–‹å§‹è™•ç†
        tracker.add_file_record(filename, file_size, status="processing")
        
        try:

# ä¸Šå‚³æ–‡ä»¶

print(f"   ğŸ“¤ ä¸Šå‚³æ–‡ä»¶...")

file_id = upload_file(file_path, token)

if not file_id:

    tracker.add_file_record(filename, file_size, status="failed", error="æ–‡ä»¶ä¸Šå‚³å¤±æ•—")

    continue



print(f"   âœ… æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼Œfile_id: {file_id}")

tracker.add_file_record(filename, file_size, file_id=file_id, status="processing")



# ç­‰å¾…è™•ç†å®Œæˆ

print(f"   â³ ç­‰å¾…è™•ç†å®Œæˆ...")

# å‹•æ…‹ç²å– worker.job_timeout

try:

    from services.api.services.config_store_service import ConfigStoreService

    config_service = ConfigStoreService()

    config = config_service.get_config("worker", tenant_id=None)

    if config and config.config_data:

        worker_timeout = config.config_data.get("job_timeout", 900)

    else:

        worker_timeout = 900

except Exception:

    worker_timeout = 900



# ç­‰å¾…è¶…æ™‚ = worker_timeout + 60 ç§’ç·©è¡

wait_timeout = worker_timeout + 60



if processing_result["status"] == "completed":

    print(f"   âœ… è™•ç†å®Œæˆ")

    print(f"      å‘é‡åŒ–æ™‚é–“: {processing_result.get('vectorization_time', 0):.2f} ç§’")

    print(f"      åœ–è­œæå–æ™‚é–“: {processing_result.get('kg_extraction_time', 0):.2f} ç§’")

    print(f"      ç¸½è™•ç†æ™‚é–“: {processing_result.get('total_time', 0):.2f} ç§’")

    

    # ç²å–KGçµæœ

    kg_results = get_kg_results(file_id, token)

    entities_count = kg_results.get("entities_count", 0)

    relations_count = kg_results.get("relations_count", 0)

    

    # æå–æ›´å¤šæŒ‡æ ‡

    chunk_count = processing_result.get("final_status", {}).get("chunking", {}).get("chunk_count")

    ner_count = entities_count

    re_count = relations_count

    rt_count = relations_count

    

    print(f"      åˆ†å¡Šæ•¸é‡: {chunk_count or "N/A"}")

    print(f"      å¯¦é«”æ•¸é‡ (NER): {entities_count}")

    print(f"      é—œä¿‚æ•¸é‡ (RE): {relations_count}")

    print(f"      é—œä¿‚é¡å‹æ•¸ (RT): {rt_count}")

    

    # æ›´æ–°é€²åº¦

    # æå–æ›´å¤šæŒ‡æ ‡æ•°æ®

    chunk_count = processing_result.get("final_status", {}).get("chunking", {}).get("chunk_count")

    # NERã€REã€RT çš„æ•°é‡å¯ä»¥ä» entities_count å’Œ relations_count æ¨æ–­

    # æˆ–è€…ä» kg_results ä¸­è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯

    ner_count = entities_count  # NER æå–çš„å®ä½“æ•°

    re_count = relations_count  # RE æå–çš„å…³ç³»æ•°

    rt_count = relations_count  # RT è¯†åˆ«çš„å…³ç³»ç±»å‹æ•°ï¼ˆé€šå¸¸ç­‰äºå…³ç³»æ•°ï¼‰

    

    tracker.add_file_record(

        filename,

        file_size,

        file_id=file_id,

        status="completed",

        vectorization_time=processing_result.get("vectorization_time"),

        kg_extraction_time=processing_result.get("kg_extraction_time"),

        total_time=processing_result.get("total_time"),

        entities_count=entities_count,

        relations_count=relations_count,

        chunk_count=chunk_count,

        ner_count=ner_count,

        re_count=re_count,

        rt_count=rt_count

    )

else:

    error = processing_result.get("error", "æœªçŸ¥éŒ¯èª¤")

    print(f"   âŒ è™•ç†å¤±æ•—: {error}")

    tracker.add_file_record(

        filename,

        file_size,

        file_id=file_id,

        status="failed",

        error=error,

        total_time=processing_result.get("total_time")

    )
        
        except Exception as e:

print(f"   âŒ è™•ç†éŒ¯èª¤: {e}")

tracker.add_file_record(filename, file_size, status="failed", error=str(e))
        
        # ä¿å­˜é€²åº¦
        tracker.save_progress()
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("è™•ç†å®Œæˆæ‘˜è¦")
    print("=" * 60)
    summary = tracker.progress_data["summary"]
    print(f"ç¸½æ–‡ä»¶æ•¸: {summary['total_files']}")
    print(f"å·²è™•ç†: {summary['processed_files']}")
    print(f"å¤±æ•—: {summary['failed_files']}")
    print(f"ç¸½å¯¦é«”æ•¸: {summary['total_entities']}")
    print(f"ç¸½é—œä¿‚æ•¸: {summary['total_relations']}")
    print(f"ç¸½å‘é‡åŒ–æ™‚é–“: {summary['total_vectorization_time']:.2f} ç§’")
    print(f"ç¸½åœ–è­œæå–æ™‚é–“: {summary['total_kg_extraction_time']:.2f} ç§’")
    print(f"ç¸½è™•ç†æ™‚é–“: {summary['total_processing_time']:.2f} ç§’")
    print(f"ç¸½åˆ†å¡Šæ•¸: {summary['total_chunk_count']}")
    print(f"ç¸½NERå¯¦é«”æ•¸: {summary['total_ner_count']}")
    print(f"ç¸½REé—œä¿‚æ•¸: {summary['total_re_count']}")
    print(f"ç¸½RTé—œä¿‚é¡å‹æ•¸: {summary['total_rt_count']}")
    print(f"\nğŸ“Š è©³ç´°é€²åº¦å·²ä¿å­˜åˆ°: {PROGRESS_FILE}")


if __name__ == "__main__":
    main()
