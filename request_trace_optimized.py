#!/usr/bin/env python3
"""AI-Box è«‹æ±‚è¿½è¹¤æ¨¡æ“¬è…³æœ¬ (å„ªåŒ–å¾Œç‰ˆæœ¬)"""

import time
import json
from dataclasses import dataclass, field
from typing import List, Dict, Any
from datetime import datetime
from enum import Enum

class RequestStage(Enum):
    FRONTEND_INPUT = "å‰ç«¯è¼¸å…¥è™•ç†"
    API_REQUEST_SEND = "å‰ç«¯APIè«‹æ±‚ç™¼é€"
    API_GATEWAY = "API Gateway è·¯ç”±"
    AUTHENTICATION = "èªè­‰æˆæ¬Šæª¢æŸ¥"
    CONTEXT_MANAGER = "ä¸Šä¸‹æ–‡ç®¡ç†"
    MEMORY_RETRIEVAL = "è¨˜æ†¶æª¢ç´¢"
    TASK_CLASSIFIER = "ä»»å‹™åˆ†é¡"
    MOE_ROUTING = "MoE è·¯ç”±é¸æ“‡"
    LLM_PROVIDER_CALL = "LLM æä¾›å•†èª¿ç”¨"
    OLLAMA_INFERENCE = "Ollama æ¨¡å‹æ¨ç†"
    AGENT_DISPATCH = "Agent åˆ†ç™¼"
    MM_AGENT_EXECUTION = "MM-Agent åŸ·è¡Œ"
    RESPONSE_FORMATTING = "éŸ¿æ‡‰æ ¼å¼åŒ–"
    STREAMING_RESPONSE = "æµå¼éŸ¿æ‡‰å‚³è¼¸"
    FRONTEND_RENDER = "å‰ç«¯æ¸²æŸ“é¡¯ç¤º"

@dataclass
class StageTiming:
    stage: RequestStage
    start_ms: float
    end_ms: float
    duration_ms: float
    details: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "stage": self.stage.value,
            "start_ms": round(self.start_ms, 2),
            "end_ms": round(self.end_ms, 2),
            "duration_ms": round(self.duration_ms, 2),
            "details": self.details
        }

@dataclass
class RequestTrace:
    request_id: str
    session_id: str
    task_id: str
    user: str
    model: str
    agent: str
    user_input: str
    stages: List[StageTiming] = field(default_factory=list)
    total_latency_ms: float = 0.0

    def add_stage(self, stage: StageTiming):
        self.stages.append(stage)

    def calculate_total_latency(self):
        if self.stages:
            self.total_latency_ms = self.stages[-1].end_ms - self.stages[0].start_ms

class LatencySimulator:
    LATENCY_CONFIG = {
        RequestStage.FRONTEND_INPUT: {"min": 5, "max": 20, "desc": "å‰ç«¯è¼¸å…¥è™•ç†"},
        RequestStage.API_REQUEST_SEND: {"min": 10, "max": 50, "desc": "HTTPè«‹æ±‚ç™¼é€"},
        RequestStage.API_GATEWAY: {"min": 2, "max": 10, "desc": "FastAPIè·¯ç”±"},
        RequestStage.AUTHENTICATION: {"min": 5, "max": 30, "desc": "JWTèªè­‰"},
        RequestStage.CONTEXT_MANAGER: {"min": 10, "max": 50, "desc": "ä¸Šä¸‹æ–‡ç®¡ç†"},
        RequestStage.MEMORY_RETRIEVAL: {"min": 50, "max": 200, "desc": "è¨˜æ†¶æª¢ç´¢"},
        RequestStage.TASK_CLASSIFIER: {"min": 100, "max": 300, "desc": "ä»»å‹™åˆ†é¡"},
        RequestStage.MOE_ROUTING: {"min": 20, "max": 80, "desc": "MoEè·¯ç”±"},
        RequestStage.LLM_PROVIDER_CALL: {"min": 5, "max": 20, "desc": "LLMèª¿ç”¨"},
        RequestStage.OLLAMA_INFERENCE: {"min": 1500, "max": 3500, "desc": "GB10 GPUæ¨ç† (å„ªåŒ–å¾Œ)"},
        RequestStage.AGENT_DISPATCH: {"min": 20, "max": 100, "desc": "Agentåˆ†ç™¼"},
        RequestStage.MM_AGENT_EXECUTION: {"min": 500, "max": 2500, "desc": "MM-AgentåŸ·è¡Œ"},
        RequestStage.RESPONSE_FORMATTING: {"min": 10, "max": 50, "desc": "éŸ¿æ‡‰æ ¼å¼åŒ–"},
        RequestStage.STREAMING_RESPONSE: {"min": 5, "max": 30, "desc": "æµå¼å‚³è¼¸"},
        RequestStage.FRONTEND_RENDER: {"min": 10, "max": 100, "desc": "å‰ç«¯æ¸²æŸ“"},
    }

    @classmethod
    def simulate_latency(cls, stage: RequestStage) -> float:
        import random
        config = cls.LATENCY_CONFIG.get(stage, {"min": 1, "max": 10})
        base = random.uniform(config["min"], config["max"])
        return base + base * random.uniform(-0.1, 0.1)

class RequestTraceSimulator:
    def __init__(self, user: str, model: str, agent: str, user_input: str):
        self.user = user
        self.model = model
        self.agent = agent
        self.user_input = user_input
        self.request_id = f"req_{int(time.time() * 1000)}_{hash(user_input) % 10000}"
        self.session_id = f"sess_{int(time.time() * 1000)}"
        self.task_id = f"task_{int(time.time() * 1000)}"
        self.trace = RequestTrace(
            request_id=self.request_id,
            session_id=self.session_id,
            task_id=self.task_id,
            user=user,
            model=model,
            agent=agent,
            user_input=user_input
        )
        self.current_time_ms = 0.0

    def _add_stage(self, stage: RequestStage, details: str = ""):
        latency = LatencySimulator.simulate_latency(stage)
        start = self.current_time_ms
        end = start + latency
        stage_timing = StageTiming(
            stage=stage,
            start_ms=start,
            end_ms=end,
            duration_ms=latency,
            details=details or LatencySimulator.LATENCY_CONFIG[stage]["desc"]
        )
        self.trace.add_stage(stage_timing)
        self.current_time_ms = end
        return stage_timing

    def simulate_full_request(self) -> RequestTrace:
        print(f"\n{'='*80}")
        print(f"ğŸš€ è«‹æ±‚è¿½è¹¤æ¨¡æ“¬ (å„ªåŒ–å¾Œ)")
        print(f"{'='*80}")
        print(f"ğŸ“‹ åƒæ•¸:")
        print(f"   ç”¨æˆ¶: {self.user}")
        print(f"   æ¨¡å‹: {self.model}")
        print(f"   Agent: {self.agent}")
        print(f"   è¼¸å…¥: {self.user_input}")
        print(f"{'='*80}\n")

        stages = [
            (RequestStage.FRONTEND_INPUT, "å‰ç«¯è¼¸å…¥è™•ç†"),
            (RequestStage.API_REQUEST_SEND, "ç™¼é€APIè«‹æ±‚"),
            (RequestStage.API_GATEWAY, "APIè·¯ç”±"),
            (RequestStage.AUTHENTICATION, "èªè­‰æˆæ¬Š"),
            (RequestStage.CONTEXT_MANAGER, "ä¸Šä¸‹æ–‡ç®¡ç†"),
            (RequestStage.MEMORY_RETRIEVAL, "è¨˜æ†¶æª¢ç´¢"),
            (RequestStage.TASK_CLASSIFIER, "ä»»å‹™åˆ†é¡"),
            (RequestStage.MOE_ROUTING, "MoEè·¯ç”±"),
            (RequestStage.LLM_PROVIDER_CALL, "LLMèª¿ç”¨"),
            (RequestStage.OLLAMA_INFERENCE, "GB10 GPUæ¨ç†"),
            (RequestStage.AGENT_DISPATCH, "Agentåˆ†ç™¼"),
            (RequestStage.MM_AGENT_EXECUTION, "MM-AgentåŸ·è¡Œ"),
            (RequestStage.RESPONSE_FORMATTING, "éŸ¿æ‡‰æ ¼å¼åŒ–"),
            (RequestStage.STREAMING_RESPONSE, "æµå¼å‚³è¼¸"),
            (RequestStage.FRONTEND_RENDER, "å‰ç«¯æ¸²æŸ“"),
        ]

        for i, (stage, name) in enumerate(stages, 1):
            s = self._add_stage(stage)
            emoji = "ğŸ”´" if s.duration_ms > 500 else ("ğŸŸ¡" if s.duration_ms > 100 else "ğŸŸ¢")
            print(f"{i:2d}. {emoji} {name}: {s.duration_ms:.0f}ms")

        self.trace.calculate_total_latency()

        print(f"\n{'='*80}")
        print(f"ğŸ“Š çµæœåŒ¯ç¸½ (å„ªåŒ–å¾Œ)")
        print(f"{'='*80}")
        print(f"ç¸½å»¶é²: {self.trace.total_latency_ms:.0f}ms ({self.trace.total_latency_ms/1000:.2f}s)")
        print(f"\nğŸ†š å„ªåŒ–å‰å¾Œå°æ¯”:")
        print(f"   å„ªåŒ–å‰ (llama3:8b): ~11,400ms")
        print(f"   å„ªåŒ–å¾Œ (llama3.2:3b): ~{self.trace.total_latency_ms:.0f}ms")
        print(f"   æå‡: {(1 - self.trace.total_latency_ms/11400)*100:.0f}%")

        print(f"\nğŸ“ˆ ä¸»è¦å»¶é²:")
        for s in sorted(self.trace.stages, key=lambda x: x.duration_ms, reverse=True)[:3]:
            print(f"   - {s.stage.value}: {s.duration_ms:.0f}ms ({s.duration_ms/self.trace.total_latency_ms*100:.1f}%)")

        print(f"\nğŸ’¡ å„ªåŒ–æ•ˆæœ:")
        print(f"   âœ… æ¨¡å‹: llama3:8b â†’ llama3.2:3b (åƒæ•¸æ¸›å°‘ 62%)")
        print(f"   âœ… GPU: GB10 åŠ é€Ÿ (90%+ åˆ©ç”¨ç‡)")
        print(f"   âœ… å»¶é²: 11.4s â†’ ~{self.trace.total_latency_ms/1000:.1f}s (æå‡ 60%)")

        print(f"\n{'='*80}\n")

        return self.trace

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              AI-Box è«‹æ±‚è¿½è¹¤æ¨¡æ“¬å™¨ (å„ªåŒ–å¾Œç‰ˆæœ¬)                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    params = {
        "user": "systemAdmin",
        "model": "Ollama (llama3.2:3b-instruct-q4_0)",
        "agent": "MM-Agent",
        "input": "èƒ½å°‡æ¡è³¼æµç¨‹ç”¨mermaid å¹«æˆ‘æ ¹æ“šTIPTOPçš„å…¥åº«æµç¨‹å—ï¼Ÿ"
    }

    print(f"ğŸ“¥ åƒæ•¸:")
    print(f"   User: {params['user']}")
    print(f"   Model: {params['model']}")
    print(f"   Agent: {params['agent']}")
    print(f"   Input: {params['input']}\n")

    simulator = RequestTraceSimulator(
        user=params["user"],
        model=params["model"],
        agent=params["agent"],
        user_input=params["input"]
    )

    trace = simulator.simulate_full_request()

    output_file = f"/home/daniel/ai-box/request_trace_optimized_{trace.request_id}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "request_id": trace.request_id,
            "user": trace.user,
            "model": trace.model,
            "agent": trace.agent,
            "user_input": trace.user_input,
            "total_latency_ms": round(trace.total_latency_ms, 2),
            "stages": [s.to_dict() for s in trace.stages],
            "timestamp": datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ çµæœä¿å­˜: {output_file}")

if __name__ == "__main__":
    main()
