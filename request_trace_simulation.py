#!/usr/bin/env python3
"""
ä»£ç¢¼åŠŸèƒ½èªªæ˜: AI-Box è«‹æ±‚è¿½è¹¤æ¨¡æ“¬è…³æœ¬ - æ¨¡æ“¬å¾å‰ç«¯è¼¸å…¥åˆ°å¾Œç«¯å›æ‡‰çš„å®Œæ•´è«‹æ±‚éˆè·¯åŠæ™‚é–“æ¶ˆè€—
å‰µå»ºæ—¥æœŸ: 2026-02-02
å‰µå»ºäºº: OpenCode AI
æœ€å¾Œä¿®æ”¹æ—¥æœŸ: 2026-02-02

ä½¿ç”¨åƒæ•¸:
  - user: systemAdmin
  - æ¨¡å‹: Ollama 4
  - ä»»å‹™: MM-Agent
  - è¼¸å…¥: "èƒ½å°‡æ¡è³¼æµç¨‹ç”¨mermaid å¹«æˆ‘æ ¹æ“šTIPTOPçš„å…¥åº«æµç¨‹å—ï¼Ÿ"
"""

import time
import json
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from datetime import datetime
from enum import Enum

# ============================================================
# æ¨¡æ“¬æ•¸æ“šçµæ§‹
# ============================================================

class RequestStage(Enum):
    FRONTEND_INPUT = "å‰ç«¯è¼¸å…¥è™•ç†"
    API_REQUEST_SEND = "å‰ç«¯APIè«‹æ±‚ç™¼é€"
    API_GATEWAY = "API Gateway è·¯ç”±"
    AUTHENTICATION = "èªè­‰æˆæ¬Šæª¢æŸ¥"
    CONTEXT_MANAGER = "ä¸Šä¸‹æ–‡ç®¡ç†"
    MEMORY_RETRIEVAL = "è¨˜æ†¶æª¢ç´¢"
    TASK_CLASSIFIER = "ä»»å‹™åˆ†é¡"
    MOE_ROUTING = "MoE è·¯ç”±é¸æ“‡"
    LLM_PROVIDER_CALL = "LLM æä¾›å•†èª¿ç”¨"
    OLLAMA_INFERENCE = "Ollama æ¨¡å‹æ¨ç†"
    AGENT_DISPATCH = "Agent åˆ†ç™¼"
    MM_AGENT_EXECUTION = "MM-Agent åŸ·è¡Œ"
    RESPONSE_FORMATTING = "éŸ¿æ‡‰æ ¼å¼åŒ–"
    STREAMING_RESPONSE = "æµå¼éŸ¿æ‡‰å‚³è¼¸"
    FRONTEND_RENDER = "å‰ç«¯æ¸²æŸ“é¡¯ç¤º"


@dataclass
class StageTiming:
    """éšæ®µæ™‚é–“è¨˜éŒ„"""
    stage: RequestStage
    start_ms: float
    end_ms: float
    duration_ms: float
    details: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "stage": self.stage.value,
            "start_ms": round(self.start_ms, 2),
            "end_ms": round(self.end_ms, 2),
            "duration_ms": round(self.duration_ms, 2),
            "details": self.details
        }


@dataclass
class RequestTrace:
    """å®Œæ•´è«‹æ±‚è¿½è¹¤è¨˜éŒ„"""
    request_id: str
    session_id: str
    task_id: str
    user: str
    model: str
    agent: str
    user_input: str
    stages: List[StageTiming] = field(default_factory=list)
    total_latency_ms: float = 0.0

    def add_stage(self, stage: StageTiming):
        self.stages.append(stage)

    def calculate_total_latency(self):
        if self.stages:
            self.total_latency_ms = self.stages[-1].end_ms - self.stages[0].start_ms

    def to_dict(self) -> Dict[str, Any]:
        return {
            "request_id": self.request_id,
            "session_id": self.session_id,
            "task_id": self.task_id,
            "user": self.user,
            "model": self.model,
            "agent": self.agent,
            "user_input": self.user_input,
            "total_latency_ms": round(self.total_latency_ms, 2),
            "stages": [s.to_dict() for s in self.stages],
            "timestamp": datetime.now().isoformat()
        }


# ============================================================
# æ¨¡æ“¬å„éšæ®µå»¶é²
# ============================================================

class LatencySimulator:
    """å»¶é²æ¨¡æ“¬å™¨ - æ ¹æ“šå¯¦éš›ç³»çµ±ç‰¹æ€§æ¨¡æ“¬å„éšæ®µæ™‚é–“æ¶ˆè€—"""

    # å„éšæ®µå»¶é²é…ç½® (ms)
    LATENCY_CONFIG = {
        RequestStage.FRONTEND_INPUT: {
            "min": 5, "max": 20,  # è¼¸å…¥è§£æã€é©—è­‰
            "description": "å‰ç«¯è¼¸å…¥è™•ç†å’ŒJSONåºåˆ—åŒ–"
        },
        RequestStage.API_REQUEST_SEND: {
            "min": 10, "max": 50,  # ç¶²çµ¡è«‹æ±‚ç™¼é€
            "description": "HTTPè«‹æ±‚å»ºç«‹å’Œç™¼é€"
        },
        RequestStage.API_GATEWAY: {
            "min": 2, "max": 10,  # Gatewayè·¯ç”±
            "description": "FastAPIè·¯ç”±åŒ¹é…å’Œä¸­é–“ä»¶è™•ç†"
        },
        RequestStage.AUTHENTICATION: {
            "min": 5, "max": 30,  # JWTé©—è­‰
            "description": "Tokenè§£æã€ç”¨æˆ¶èªè­‰ã€æ¬Šé™æª¢æŸ¥"
        },
        RequestStage.CONTEXT_MANAGER: {
            "min": 10, "max": 50,  # ä¸Šä¸‹æ–‡çª—å£ç®¡ç†
            "description": "ç²å–æœƒè©±ä¸Šä¸‹æ–‡ã€æ¶ˆæ¯çª—å£è£å‰ª"
        },
        RequestStage.MEMORY_RETRIEVAL: {
            "min": 50, "max": 200,  # å‘é‡æª¢ç´¢
            "description": "é•·æœŸè¨˜æ†¶æª¢ç´¢ã€å‘é‡ç›¸ä¼¼åº¦æœç´¢(ChromaDB/ArangoDB)"
        },
        RequestStage.TASK_CLASSIFIER: {
            "min": 100, "max": 300,  # Taskåˆ†é¡
            "description": "TaskAnalyzeræ„åœ–åˆ†é¡ã€Router LLMèª¿ç”¨"
        },
        RequestStage.MOE_ROUTING: {
            "min": 20, "max": 80,  # MoEè·¯ç”±
            "description": "MoE Manageræ¨¡å‹é¸æ“‡ã€provideréæ¿¾"
        },
        RequestStage.LLM_PROVIDER_CALL: {
            "min": 5, "max": 20,  # LLMèª¿ç”¨é–‹éŠ·
            "description": "HTTPå®¢æˆ¶ç«¯è«‹æ±‚æº–å‚™ã€é‡è©¦é‚è¼¯"
        },
        RequestStage.OLLAMA_INFERENCE: {
            "min": 2000, "max": 8000,  # Ollamaæ¨ç† (ä¸»è¦ç“¶é ¸)
            "description": "Ollamaæœ¬åœ°æ¨¡å‹æ¨ç†ã€tokenç”Ÿæˆ"
        },
        RequestStage.AGENT_DISPATCH: {
            "min": 20, "max": 100,  # Agentåˆ†ç™¼
            "description": "Agenté¸æ“‡ã€å”è­°è½‰æ›ã€ä»»å‹™å°è£"
        },
        RequestStage.MM_AGENT_EXECUTION: {
            "min": 500, "max": 3000,  # MM-AgentåŸ·è¡Œ
            "description": "MM-Agentæ¥­å‹™é‚è¼¯ã€æ•¸æ“šåº«æŸ¥è©¢ã€APIèª¿ç”¨"
        },
        RequestStage.RESPONSE_FORMATTING: {
            "min": 10, "max": 50,  # éŸ¿æ‡‰æ ¼å¼åŒ–
            "description": "éŸ¿æ‡‰åºåˆ—åŒ–ã€Markdownæ¸²æŸ“"
        },
        RequestStage.STREAMING_RESPONSE: {
            "min": 5, "max": 30,  # æµå¼å‚³è¼¸
            "description": "SSEæµå¼éŸ¿æ‡‰å‚³è¼¸"
        },
        RequestStage.FRONTEND_RENDER: {
            "min": 10, "max": 100,  # å‰ç«¯æ¸²æŸ“
            "description": "Reactæ¸²æŸ“ã€Markdownè§£æã€mermaidæ¸²æŸ“"
        },
    }

    @classmethod
    def simulate_latency(cls, stage: RequestStage) -> float:
        """æ¨¡æ“¬éšæ®µå»¶é²"""
        config = cls.LATENCY_CONFIG.get(stage, {"min": 1, "max": 10})
        # æ·»åŠ éš¨æ©Ÿæ³¢å‹•
        import random
        base_latency = random.uniform(config["min"], config["max"])
        # æ·»åŠ ç¶²çµ¡æ³¢å‹• (Â±10%)
        fluctuation = base_latency * random.uniform(-0.1, 0.1)
        return base_latency + fluctuation


# ============================================================
# è«‹æ±‚è¿½è¹¤æ¨¡æ“¬å™¨
# ============================================================

class RequestTraceSimulator:
    """è«‹æ±‚è¿½è¹¤æ¨¡æ“¬å™¨"""

    def __init__(self, user: str, model: str, agent: str, user_input: str):
        self.user = user
        self.model = model
        self.agent = agent
        self.user_input = user_input
        self.request_id = f"req_{int(time.time() * 1000)}_{hash(user_input) % 10000}"
        self.session_id = f"sess_{int(time.time() * 1000)}"
        self.task_id = f"task_{int(time.time() * 1000)}"
        self.trace = RequestTrace(
            request_id=self.request_id,
            session_id=self.session_id,
            task_id=self.task_id,
            user=user,
            model=model,
            agent=agent,
            user_input=user_input
        )
        self.current_time_ms = 0.0

    def _add_stage(self, stage: RequestStage, details: str = ""):
        """æ·»åŠ éšæ®µè¨˜éŒ„"""
        latency = LatencySimulator.simulate_latency(stage)
        start = self.current_time_ms
        end = start + latency

        stage_timing = StageTiming(
            stage=stage,
            start_ms=start,
            end_ms=end,
            duration_ms=latency,
            details=details or LatencySimulator.LATENCY_CONFIG[stage]["description"]
        )

        self.trace.add_stage(stage_timing)
        self.current_time_ms = end

        return stage_timing

    def _get_model_specific_latency(self) -> Dict[str, float]:
        """æ ¹æ“šæ¨¡å‹ç²å–ç‰¹å®šå»¶é²é…ç½®"""
        if "ollama" in self.model.lower():
            return {
                "llm_inference": 3000,  # Ollamaæœ¬åœ°æ¨ç†
                "agent_execution": 1500,  # AgentåŸ·è¡Œ
            }
        elif "gpt" in self.model.lower():
            return {
                "llm_inference": 1500,  # GPTé›²ç«¯æ¨ç†
                "agent_execution": 1000,
            }
        else:
            return {
                "llm_inference": 2000,
                "agent_execution": 2000,
            }

    def simulate_full_request(self) -> RequestTrace:
        """æ¨¡æ“¬å®Œæ•´è«‹æ±‚æµç¨‹"""

        print(f"\n{'='*80}")
        print(f"ğŸš€ è«‹æ±‚è¿½è¹¤æ¨¡æ“¬é–‹å§‹")
        print(f"{'='*80}")
        print(f"ğŸ“‹ è«‹æ±‚åƒæ•¸:")
        print(f"   - ç”¨æˆ¶: {self.user}")
        print(f"   - æ¨¡å‹: {self.model}")
        print(f"   - Agent: {self.agent}")
        print(f"   - è¼¸å…¥: {self.user_input[:50]}...")
        print(f"   - Request ID: {self.request_id}")
        print(f"{'='*80}\n")

        # 1. å‰ç«¯è¼¸å…¥è™•ç†
        stage = self._add_stage(
            RequestStage.FRONTEND_INPUT,
            f"è§£æç”¨æˆ¶è¼¸å…¥ '{self.user_input[:30]}...'ï¼Œæ§‹å»ºè«‹æ±‚é«”"
        )
        print(f"1ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 2. å‰ç«¯APIè«‹æ±‚ç™¼é€
        stage = self._add_stage(
            RequestStage.API_REQUEST_SEND,
            f"ç™¼é€ POST /api/v1/chat è«‹æ±‚"
        )
        print(f"2ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 3. API Gatewayè·¯ç”±
        stage = self._add_stage(
            RequestStage.API_GATEWAY,
            "FastAPIè·¯ç”±åŒ¹é…ï¼Œè¯·æ±‚é‡å®šå‘åˆ° /chat ç«¯ç‚¹"
        )
        print(f"3ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 4. èªè­‰æˆæ¬Š
        stage = self._add_stage(
            RequestStage.AUTHENTICATION,
            f"JWT Tokené©—è­‰ï¼Œç”¨æˆ¶: {self.user}ï¼Œæ¬Šé™æª¢æŸ¥"
        )
        print(f"4ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 5. ä¸Šä¸‹æ–‡ç®¡ç†
        stage = self._add_stage(
            RequestStage.CONTEXT_MANAGER,
            f"ç²å–æœƒè©±ä¸Šä¸‹æ–‡ {self.session_id}ï¼Œæ¶ˆæ¯çª—å£è£å‰ª"
        )
        print(f"5ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 6. è¨˜æ†¶æª¢ç´¢
        stage = self._add_stage(
            RequestStage.MEMORY_RETRIEVAL,
            "é•·æœŸè¨˜æ†¶æª¢ç´¢ï¼Œå‘é‡ç›¸ä¼¼åº¦æœç´¢ (ChromaDB/ArangoDB)"
        )
        print(f"6ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 7. ä»»å‹™åˆ†é¡ (TaskAnalyzer)
        stage = self._add_stage(
            RequestStage.TASK_CLASSIFIER,
            f"TaskAnalyzeræ„åœ–åˆ†é¡: è­˜åˆ¥ç‚º '{self.agent}' ä»»å‹™"
        )
        print(f"7ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 8. MoEè·¯ç”±
        stage = self._add_stage(
            RequestStage.MOE_ROUTING,
            f"MoE Manageré¸æ“‡æ¨¡å‹: {self.model}"
        )
        print(f"8ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 9. LLMæä¾›å•†èª¿ç”¨
        stage = self._add_stage(
            RequestStage.LLM_PROVIDER_CALL,
            f"æº–å‚™ {self.model} APIè«‹æ±‚ï¼Œè¨­ç½®è¶…æ™‚å’Œé‡è©¦"
        )
        print(f"9ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 10. Ollamaæ¨¡å‹æ¨ç† (ä¸»è¦ç“¶é ¸)
        model_latency = self._get_model_specific_latency()
        stage = self._add_stage(
            RequestStage.OLLAMA_INFERENCE,
            f"Ollama {self.model} æ¨¡å‹æ¨ç†: '{self.user_input[:30]}...' â†’ Mermaidæµç¨‹åœ–"
        )
        print(f"ğŸ”Ÿ {stage.stage.value}: {stage.duration_ms:.1f}ms â±ï¸ **ä¸»è¦ç“¶é ¸**")
        print(f"   ğŸ“ {stage.details}")
        print(f"   ğŸ’¡ å»ºè­°å„ªåŒ–: å¢åŠ  Ollama æ‰¹è™•ç†å¤§å°ã€ä½¿ç”¨ GPU åŠ é€Ÿ")

        # 11. Agentåˆ†ç™¼
        stage = self._add_stage(
            RequestStage.AGENT_DISPATCH,
            f"åˆ†ç™¼åˆ° {self.agent} Agentï¼Œå”è­°è½‰æ›"
        )
        print(f"1ï¸âƒ£1ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 12. MM-AgentåŸ·è¡Œ
        stage = self._add_stage(
            RequestStage.MM_AGENT_EXECUTION,
            f"MM-AgentåŸ·è¡Œ TIPTOP å…¥åº«æµç¨‹åˆ†æï¼Œç”Ÿæˆ Mermaid ä»£ç¢¼"
        )
        print(f"1ï¸âƒ£2ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")
        print(f"   ğŸ“Š MM-Agent å…§éƒ¨æµç¨‹:")
        print(f"      - èªç¾©åˆ†æ: è§£æç”¨æˆ¶éœ€æ±‚")
        print(f"      - æ•¸æ“šæŸ¥è©¢: æŸ¥è©¢ TIPTOP å…¥åº«æµç¨‹")
        print(f"      - æµç¨‹åœ–ç”Ÿæˆ: ç”Ÿæˆ Mermaid ä»£ç¢¼")
        print(f"      - çµæœé©—è­‰: é©—è­‰æµç¨‹åœ–æ­£ç¢ºæ€§")

        # 13. éŸ¿æ‡‰æ ¼å¼åŒ–
        stage = self._add_stage(
            RequestStage.RESPONSE_FORMATTING,
            "åºåˆ—åŒ–éŸ¿æ‡‰ï¼ŒMarkdown + Mermaid æ¸²æŸ“"
        )
        print(f"1ï¸âƒ£3ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 14. æµå¼éŸ¿æ‡‰å‚³è¼¸
        stage = self._add_stage(
            RequestStage.STREAMING_RESPONSE,
            "SSE æµå¼å‚³è¼¸ Mermaid ä»£ç¢¼å¡Š"
        )
        print(f"1ï¸âƒ£4ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # 15. å‰ç«¯æ¸²æŸ“
        stage = self._add_stage(
            RequestStage.FRONTEND_RENDER,
            "Reactæ¸²æŸ“çµ„ä»¶ï¼ŒMermaidæµç¨‹åœ–é¡¯ç¤º"
        )
        print(f"1ï¸âƒ£5ï¸âƒ£ {stage.stage.value}: {stage.duration_ms:.1f}ms")
        print(f"   ğŸ“ {stage.details}")

        # è¨ˆç®—ç¸½å»¶é²
        self.trace.calculate_total_latency()

        # æ‰“å°åŒ¯ç¸½
        print(f"\n{'='*80}")
        print(f"ğŸ“Š è«‹æ±‚è¿½è¹¤åŒ¯ç¸½")
        print(f"{'='*80}")
        print(f"ç¸½å»¶é²: {self.trace.total_latency_ms:.1f}ms ({self.trace.total_latency_ms/1000:.2f}s)")
        print(f"\nç“¶é ¸åˆ†æ:")
        print(f"  ğŸ”´ é«˜å»¶é²éšæ®µ:")
        for stage_timing in sorted(self.trace.stages, key=lambda x: x.duration_ms, reverse=True)[:5]:
            if stage_timing.duration_ms > 500:
                print(f"     - {stage_timing.stage.value}: {stage_timing.duration_ms:.1f}ms ({stage_timing.duration_ms/self.trace.total_latency_ms*100:.1f}%)")

        print(f"\nğŸŸ¡ ä¸­ç­‰å»¶é²éšæ®µ:")
        for stage_timing in sorted(self.trace.stages, key=lambda x: x.duration_ms, reverse=True):
            if 100 < stage_timing.duration_ms <= 500:
                print(f"     - {stage_timing.stage.value}: {stage_timing.duration_ms:.1f}ms ({stage_timing.duration_ms/self.trace.total_latency_ms*100:.1f}%)")

        print(f"\nğŸŸ¢ ä½å»¶é²éšæ®µ:")
        for stage_timing in sorted(self.trace.stages, key=lambda x: x.duration_ms)[:3]:
            if stage_timing.duration_ms <= 100:
                print(f"     - {stage_timing.stage.value}: {stage_timing.duration_ms:.1f}ms ({stage_timing.duration_ms/self.trace.total_latency_ms*100:.1f}%)")

        print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
        print(f"  1. Ollama æ¨¡å‹æ¨ç†æ˜¯æœ€ä¸»è¦ç“¶é ¸ï¼Œå»ºè­°:")
        print(f"     - ä½¿ç”¨ GPU åŠ é€Ÿ Ollama")
        print(f"     - å¢åŠ  Ollama æ‰¹è™•ç†å¤§å°")
        print(f"     - è€ƒæ…®ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é‡åŒ–ç‰ˆæœ¬")
        print(f"  2. MM-Agent åŸ·è¡Œæ™‚é–“è¼ƒé•·ï¼Œå»ºè­°:")
        print(f"     - å„ªåŒ– TIPTOP æµç¨‹æŸ¥è©¢")
        print(f"     - æ·»åŠ çµæœç·©å­˜")
        print(f"     - ä¸¦è¡ŒåŒ–ç¨ç«‹çš„æŸ¥è©¢æ“ä½œ")
        print(f"  3. è¨˜æ†¶æª¢ç´¢å¯è€ƒæ…®:")
        print(f"     - å¢åŠ ç´¢å¼•ç·©å­˜")
        print(f"     - ä½¿ç”¨æ›´å¿«çš„å‘é‡æ•¸æ“šåº«")

        print(f"\n{'='*80}")
        print(f"âœ¨ æ¨¡æ“¬å®Œæˆ")
        print(f"{'='*80}\n")

        return self.trace


# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    """ä¸»ç¨‹åºå…¥å£"""

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      AI-Box è«‹æ±‚è¿½è¹¤æ¨¡æ“¬å™¨                                     â•‘
â•‘                   Request Trace Simulator for AI-Bot                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # æ¨¡æ“¬åƒæ•¸
    params = {
        "user": "systemAdmin",
        "model": "Ollama 4",  # ä¾‹å¦‚: llama3.2, qwen2.5
        "agent": "MM-Agent",
        "input": "èƒ½å°‡æ¡è³¼æµç¨‹ç”¨mermaid å¹«æˆ‘æ ¹æ“šTIPTOPçš„å…¥åº«æµç¨‹å—ï¼Ÿ"
    }

    print(f"ğŸ“¥ è¼¸å…¥åƒæ•¸:")
    print(f"   - User: {params['user']}")
    print(f"   - Model: {params['model']}")
    print(f"   - Agent: {params['agent']}")
    print(f"   - Input: {params['input']}")
    print()

    # å‰µå»ºæ¨¡æ“¬å™¨ä¸¦åŸ·è¡Œ
    simulator = RequestTraceSimulator(
        user=params["user"],
        model=params["model"],
        agent=params["agent"],
        user_input=params["input"]
    )

    trace = simulator.simulate_full_request()

    # ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶
    output_file = f"/home/daniel/ai-box/request_trace_{trace.request_id}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(trace.to_dict(), f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ è¿½è¹¤çµæœå·²ä¿å­˜åˆ°: {output_file}")

    return trace


if __name__ == "__main__":
    main()
