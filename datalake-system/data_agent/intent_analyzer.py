#!/usr/bin/env python3
"""
æ„åœ–åˆ†ææœå‹™

åŠŸèƒ½ï¼š
1. å®Œæ•´æ€§æª¢æŸ¥ - ç¢ºèªæŸ¥è©¢æ˜¯å¦åŒ…å«å¿…è¦è³‡è¨Š
2. Tokenization - åˆ†è©è™•ç†
3. æ„åœ–è­˜åˆ¥ - è­˜åˆ¥æ¨¡ç³Šè©å½™ä¸¦è½‰æ›ç‚ºç²¾ç¢ºæ„åœ–
4. èªç¾©æè¿° - ç”Ÿæˆç²¾ç¢ºçš„æŸ¥è©¢æè¿°
5. RAG æ•´åˆ - ä½¿ç”¨å‘é‡è³‡æ–™åº«å¢å¼·æ„åœ–è­˜åˆ¥

ä½¿ç”¨æ–¹å¼ï¼š
    from data_agent.intent_analyzer import IntentAnalyzer

    analyzer = IntentAnalyzer()
    result = analyzer.analyze("æŸ¥W01åº«æˆ¿æ¯å€‹æ–™è™Ÿå­˜é‡")
"""

import re
import requests
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance


class QueryIntent(Enum):
    """æŸ¥è©¢æ„åœ–é¡å‹"""

    QUERY_INVENTORY = "query_inventory"
    QUERY_ORDER = "query_order"
    QUERY_PRICE = "query_price"
    QUERY_TRANSACTION = "query_transaction"
    QUERY_ITEM = "query_item"
    CALCULATE_SUM = "calculate_sum"
    CALCULATE_AVG = "calculate_avg"
    CALCULATE_COUNT = "calculate_count"
    CALCULATE_MAX = "calculate_max"
    CALCULATE_MIN = "calculate_min"
    STATISTICS = "statistics"
    UNKNOWN = "unknown"


RAG_CONFIG = {
    "qdrant_url": "http://localhost:6333",
    "collection_name": "data_agent_intents",
    "embedding_endpoint": "http://localhost:11434/api/embeddings",
    "embedding_model": "qwen3-embedding:latest",
    "vector_dim": 4096,
    "similarity_threshold": 0.75,
}


class TimeGranularity(Enum):
    """æ™‚é–“ç²’åº¦"""

    DAY = "day"
    WEEK = "week"
    MONTH = "month"
    QUARTER = "quarter"
    YEAR = "year"
    RANGE = "range"
    NONE = "none"


@dataclass
class Token:
    """åˆ†è©çµæœ"""

    word: str
    pos: str  # è©æ€§
    meaning: str = ""  # èªç¾©
    is_ambiguous: bool = False  # æ˜¯å¦ç‚ºæ¨¡ç³Šè©å½™


@dataclass
class IntentAnalysisResult:
    """æ„åœ–åˆ†æçµæœ"""

    original_query: str
    tokens: List[Token] = field(default_factory=list)

    query_intent: QueryIntent = QueryIntent.UNKNOWN
    intent_description: str = ""
    table: str = ""
    subject: str = ""
    subject_value: str = ""
    warehouse: str = ""
    warehouse_condition: str = ""

    aggregation: str = ""
    has_group_by: bool = False
    group_by_field: str = ""
    has_order_by: bool = False
    order_by_field: str = ""
    order_direction: str = ""
    limit: int = 0

    has_time_filter: bool = False
    time_granularity: TimeGranularity = TimeGranularity.NONE
    time_start: str = ""
    time_end: str = ""

    is_complete: bool = True
    missing_info: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    precise_description: str = ""

    rag_score: float = 0.0
    rag_matched_template: str = ""
    used_rag: bool = False

    def to_dict(self) -> Dict[str, Any]:
        return {
            "original_query": self.original_query,
            "query_intent": self.query_intent.value,
            "intent_description": self.intent_description,
            "table": self.table,
            "subject": self.subject,
            "subject_value": self.subject_value,
            "warehouse": self.warehouse,
            "aggregation": self.aggregation,
            "has_group_by": self.has_group_by,
            "is_complete": self.is_complete,
            "missing_info": self.missing_info,
            "precise_description": self.precise_description,
            "rag_score": self.rag_score,
            "rag_matched_template": self.rag_matched_template,
            "used_rag": self.used_rag,
        }


class IntentAnalyzer:
    """æ„åœ–åˆ†æå™¨"""

    def __init__(self):
        from data_agent.code_dictionary import CodeDictionary

        self._rag_client = None
        self._rag_initialized = False
        self.code_dict = CodeDictionary()

        self.ambiguous_mappings = {
            # æ•¸é‡ç›¸é—œ
            "ç¸½è¨ˆ": {"agg": "SUM", "meaning": "è¨ˆç®—ç¸½å’Œ"},
            "ç¸½å…±": {"agg": "SUM", "meaning": "è¨ˆç®—ç¸½å’Œ"},
            "ç¸½é‡": {"agg": "SUM", "meaning": "è¨ˆç®—ç¸½å’Œ"},
            "ç¸½æ•¸": {"agg": "SUM", "meaning": "è¨ˆç®—ç¸½å’Œ"},
            "åˆè¨ˆ": {"agg": "SUM", "meaning": "è¨ˆç®—ç¸½å’Œ"},
            # å¹³å‡ç›¸é—œ
            "å¹³å‡": {"agg": "AVG", "meaning": "è¨ˆç®—å¹³å‡å€¼"},
            "å‡": {"agg": "AVG", "meaning": "è¨ˆç®—å¹³å‡å€¼"},
            # æ•¸é‡ç›¸é—œ
            "å¹¾å€‹": {"agg": "COUNT", "meaning": "è¨ˆç®—æ•¸é‡"},
            "å¤šå°‘ç­†": {"agg": "COUNT", "meaning": "è¨ˆç®—ç­†æ•¸"},
            "ç­†æ•¸": {"agg": "COUNT", "meaning": "è¨ˆç®—ç­†æ•¸"},
            "æœ‰å¹¾ç­†": {"agg": "COUNT", "meaning": "è¨ˆç®—ç­†æ•¸"},
            # æœ€å¤§æœ€å°
            "æœ€å¤š": {"agg": "MAX", "meaning": "æ‰¾å‡ºæœ€å¤§å€¼"},
            "æœ€é«˜": {"agg": "MAX", "meaning": "æ‰¾å‡ºæœ€å¤§å€¼"},
            "æœ€å¤§": {"agg": "MAX", "meaning": "æ‰¾å‡ºæœ€å¤§å€¼"},
            "æœ€å°‘": {"agg": "MIN", "meaning": "æ‰¾å‡ºæœ€å°å€¼"},
            "æœ€ä½": {"agg": "MIN", "meaning": "æ‰¾å‡ºæœ€å°å€¼"},
            "æœ€å°": {"agg": "MIN", "meaning": "æ‰¾å‡ºæœ€å°å€¼"},
            # æ’åºç›¸é—œ
            "å‰å¹¾ç­†": {"order": "DESC", "limit": 5, "meaning": "å–å‰ N ç­†"},
            "å‰åå": {"order": "DESC", "limit": 10, "meaning": "å–å‰ 10 å"},
            "å‰äº”å": {"order": "DESC", "limit": 5, "meaning": "å–å‰ 5 å"},
            "æœ€å¾Œ": {"order": "ASC", "limit": 1, "meaning": "å–æœ€å¾Œä¸€ç­†"},
            "æœ€æ–°": {"order": "DESC", "limit": 1, "meaning": "å–æœ€æ–°ä¸€ç­†"},
            # åº«å­˜ç›¸é—œ
            "åº«å­˜é‡": {"field": "img10", "meaning": "åº«å­˜æ•¸é‡æ¬„ä½"},
            "åº«å­˜": {"table": "img_file", "meaning": "åº«å­˜è¡¨"},
            "å­˜é‡": {"field": "img10", "meaning": "åº«å­˜æ•¸é‡"},
            "æ–™è™Ÿ": {"field": "img01", "meaning": "æ–™è™Ÿæ¬„ä½"},
            "å“å": {"field": "ima02", "meaning": "å“åæ¬„ä½"},
            "åº«æˆ¿": {"meaning": "å€‰åº«"},
            "æ¯å€‹": {"meaning": "åˆ†çµ„æŸ¥è©¢"},
        }

        # å€‰åº«ä»£ç¢¼æ˜ å°„
        self.warehouse_patterns = [
            (r"W0[1-5]", "W01", "W02", "W03", "W04", "W05"),
            (r"åŸæ–™å€‰", "W01", "åŸæ–™å€‰åº«"),
            (r"æˆå“å€‰", "W03", "æˆå“å€‰åº«"),
            (r"åŠæˆå“å€‰", "W02", "åŠæˆå“å€‰åº«"),
        ]

        self.table_keywords = {
            "img_file": ["åº«å­˜", "å­˜é‡", "å€‰", "w0", "w1", "w2", "w3"],
            "ima_file": ["å“å", "è¦æ ¼", "æ–™ä»¶", "ç‰©æ–™"],
            "tlf_file": [
                "äº¤æ˜“",
                "ç•°å‹•",
                "æ¡è³¼",
                "é€²è²¨",
                "æ”¶æ–™",
                "æ”¶è²¨",
                "å‡ºåº«",
                "å…¥åº«",
                "é ˜æ–™",
                "å ±å»¢",
            ],
            "coptc_file": ["è¨‚å–®", "å‡ºè²¨", "å®¢æˆ¶"],
            "coptd_file": ["è¨‚å–®æ˜ç´°", "è¨‚å–®é …ç›®"],
            "prc_file": ["å–®åƒ¹", "åƒ¹æ ¼", "è¨‚åƒ¹"],
            "pmm_file": ["æ¡è³¼å–®", "æ¡è³¼"],
            "pmn_file": ["æ¡è³¼å–®èº«", "æ¡è³¼æ˜ç´°"],
            "rvb_file": ["æ”¶æ–™å–®", "æ”¶æ–™"],
            "cmc_file": ["å®¢æˆ¶ä¸»æª”", "å®¢æˆ¶"],
            "pmc_file": ["ä¾›æ‡‰å•†", "Vendor"],
        }

    def _init_rag_client(self):
        """åˆå§‹åŒ– RAG å®¢æˆ¶ç«¯"""
        if self._rag_initialized:
            return

        try:
            self._rag_client = QdrantClient(url=RAG_CONFIG["qdrant_url"])
            self._rag_initialized = True
        except Exception as e:
            self._rag_client = None

    def _get_embedding(self, text: str):
        """ç²å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            payload = {"model": RAG_CONFIG["embedding_model"], "prompt": text}
            response = requests.post(RAG_CONFIG["embedding_endpoint"], json=payload, timeout=60)
            response.raise_for_status()
            return response.json()["embedding"]
        except Exception:
            return None

    def query_rag(self, query: str, top_k: int = 3):
        """æŸ¥è©¢ RAG ç³»çµ±"""
        self._init_rag_client()

        if not self._rag_client:
            return []

        try:
            embedding = self._get_embedding(query)
            if not embedding:
                return []

            results = self._rag_client.query_points(
                collection_name=RAG_CONFIG["collection_name"], query=embedding, limit=top_k
            )

            rag_results = []
            for r in results.points:
                if r.payload:
                    payload = dict(r.payload) if hasattr(r.payload, "items") else r.payload
                    rag_results.append(
                        {
                            "query": str(payload.get("query", "")),
                            "sql": str(payload.get("sql", "")),
                            "type": str(payload.get("type", "")),
                            "score": r.score,
                        }
                    )

            return rag_results
        except Exception:
            return []

    def tokenize(self, query: str) -> List[Token]:
        """åˆ†è©è™•ç† - æ”¯æ´ä¸­è‹±æ–‡æ··åˆæŸ¥è©¢"""
        tokens = []

        # æ¸…ç†æŸ¥è©¢
        query = query.strip()

        # ç§»é™¤å¸¸è¦‹çš„æŸ¥è©¢é–‹é ­è©
        prefix_patterns = [
            r"^æŸ¥è©¢\s*",
            r"^æŸ¥\s*",
            r"^çœ‹\s*",
            r"^é¡¯ç¤º\s*",
            r"^åˆ—å‡º\s*",
            r"^æ‰¾\s*",
        ]
        for pattern in prefix_patterns:
            query = re.sub(pattern, "", query, flags=re.IGNORECASE)

        # åˆ†å‰²ä¸­è‹±æ–‡å’Œæ•¸å­—
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„åˆ†è©ï¼šæŒ‰è©å½™æ¨¡å¼åŒ¹é…
        pattern = r"([Ww]0[1-5])|\b(\d{2}-\d{4})\b|(\d{4}[å¹´/\-]\d{1,2}[æœˆ/\-]?\d{0,2})|([^\s]+)"
        matches = re.findall(pattern, query)

        words = []
        for match in matches:
            # match æ˜¯ tupleï¼Œå–éç©ºçš„éƒ¨åˆ†
            word = next((m for m in match if m), "")
            if word:
                words.append(word)

        for word in words:
            if not word:
                continue

            token = Token(word=word, pos=self._guess_pos(word), meaning="", is_ambiguous=False)

            # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨¡ç³Šè©å½™
            if word in self.ambiguous_mappings:
                mapping = self.ambiguous_mappings[word]
                token.is_ambiguous = True
                token.meaning = mapping.get("meaning", "")
            else:
                # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¨¡ç³Šè©å½™
                for amb_word, mapping in self.ambiguous_mappings.items():
                    if amb_word in word:
                        token.is_ambiguous = True
                        token.meaning = mapping.get("meaning", "")
                        break

            tokens.append(token)

        return tokens

    def _guess_pos(self, word: str) -> str:
        """çŒœæ¸¬è©æ€§"""
        # æ•¸å­—
        if re.match(r"^\d+$", word):
            return "NUMBER"

        # æ—¥æœŸ
        if re.match(r"\d{4}[-/]\d{1,2}[-/]\d{1,2}", word):
            return "DATE"

        # æ–™è™Ÿæ ¼å¼
        if re.match(r"^\d{2}-\d{4}$", word) or re.match(r"^RM\d{2}-\d{3}$", word):
            return "ITEM_CODE"

        # å€‰åº«ä»£ç¢¼
        if re.match(r"^W0[1-5]$", word, re.IGNORECASE):
            return "WAREHOUSE"

        # é—œéµè©
        if word in ["æŸ¥è©¢", "æ‰¾", "çœ‹", "é¡¯ç¤º"]:
            return "VERB_QUERY"

        return "UNKNOWN"

    def analyze(self, query: str, use_rag_fallback: bool = True) -> IntentAnalysisResult:
        """å®Œæ•´æ„åœ–åˆ†æï¼ˆæ”¯æŒ RAG æ··åˆæ¨¡å¼ï¼‰"""
        result = IntentAnalysisResult(original_query=query)

        result.tokens = self.tokenize(query)

        self._recognize_intent(query, result)

        self._recognize_entities(query, result)

        self._recognize_aggregation(query, result)

        self._recognize_time_filter(query, result)

        self._check_completeness(result)

        self._generate_precise_description(result)

        if use_rag_fallback:
            self._apply_rag_fallback(query, result)

        return result

    def _apply_rag_fallback(self, query: str, result: IntentAnalysisResult):
        """æ‡‰ç”¨ RAG ä½œç‚ºè¦å‰‡å¼•æ“çš„ fallback"""
        rag_results = self.query_rag(query, top_k=1)

        if not rag_results:
            return

        best_match = rag_results[0]
        similarity = best_match["score"]

        result.rag_score = similarity
        result.rag_matched_template = best_match["query"]

        if similarity >= RAG_CONFIG["similarity_threshold"]:
            result.used_rag = True
            result.warnings.append(
                f"ğŸ¤– RAG åŒ¹é… (ç›¸ä¼¼åº¦: {similarity:.2%}): ã€Œ{best_match['query']}ã€"
            )

            if best_match["sql"]:
                result.precise_description += f" | RAG-SQL: {best_match['sql']}"

    def _recognize_intent(self, query: str, result: IntentAnalysisResult):
        """è­˜åˆ¥ä¸»è¦æ„åœ–"""
        query_lower = query.lower()

        # åº«å­˜ç›¸é—œæ„åœ–
        if any(kw in query_lower for kw in ["åº«å­˜", "å­˜é‡", "å€‰åº«", "w0", "w1", "w2", "w3"]):
            if any(kw in query_lower for kw in ["ç¸½è¨ˆ", "åˆè¨ˆ", "ç¸½é‡", "å¹³å‡", "çµ±è¨ˆ"]):
                result.query_intent = QueryIntent.STATISTICS
                result.intent_description = "çµ±è¨ˆåº«å­˜"
            else:
                result.query_intent = QueryIntent.QUERY_INVENTORY
                result.intent_description = "æŸ¥è©¢åº«å­˜"

        # æ¡è³¼äº¤æ˜“ç›¸é—œæ„åœ–ï¼ˆéœ€åœ¨è¨‚å–®ä¹‹å‰åˆ¤æ–·ï¼‰
        elif any(kw in query_lower for kw in ["æ¡è³¼", "é€²è²¨", "æ”¶æ–™", "æ”¶è²¨"]):
            result.query_intent = QueryIntent.QUERY_TRANSACTION
            result.intent_description = "æŸ¥è©¢æ¡è³¼äº¤æ˜“"
            result.table = "tlf_file"

        # äº¤æ˜“ç›¸é—œæ„åœ–
        elif any(kw in query_lower for kw in ["äº¤æ˜“", "ç•°å‹•"]):
            result.query_intent = QueryIntent.QUERY_TRANSACTION
            result.intent_description = "æŸ¥è©¢äº¤æ˜“è¨˜éŒ„"
            result.table = "tlf_file"

        # è¨‚å–®ç›¸é—œæ„åœ–
        elif any(kw in query_lower for kw in ["è¨‚å–®", "å‡ºè²¨", "å®¢æˆ¶"]):
            result.query_intent = QueryIntent.QUERY_ORDER
            result.intent_description = "æŸ¥è©¢è¨‚å–®"
            result.table = "coptc_file"

        # åƒ¹æ ¼ç›¸é—œæ„åœ–
        elif any(kw in query_lower for kw in ["å–®åƒ¹", "åƒ¹æ ¼", "è¨‚åƒ¹"]):
            result.query_intent = QueryIntent.QUERY_PRICE
            result.intent_description = "æŸ¥è©¢åƒ¹æ ¼"
            result.table = "prc_file"

        # è¨ˆç®—ç›¸é—œæ„åœ–
        elif any(kw in query_lower for kw in ["ç¸½è¨ˆ", "åˆè¨ˆ", "ç¸½é‡", "ç¸½æ•¸"]):
            result.query_intent = QueryIntent.CALCULATE_SUM
            result.intent_description = "è¨ˆç®—ç¸½å’Œ"
        elif "å¹³å‡" in query_lower:
            result.query_intent = QueryIntent.CALCULATE_AVG
            result.intent_description = "è¨ˆç®—å¹³å‡å€¼"
        elif any(kw in query_lower for kw in ["å¤šå°‘ç­†", "å¹¾å€‹", "ç­†æ•¸"]):
            result.query_intent = QueryIntent.CALCULATE_COUNT
            result.intent_description = "è¨ˆç®—æ•¸é‡"
        elif any(kw in query_lower for kw in ["æœ€é«˜", "æœ€å¤š", "æœ€å¤§"]):
            result.query_intent = QueryIntent.CALCULATE_MAX
            result.intent_description = "è¨ˆç®—æœ€å¤§å€¼"
        elif any(kw in query_lower for kw in ["æœ€ä½", "æœ€å°‘", "æœ€å°"]):
            result.query_intent = QueryIntent.CALCULATE_MIN
            result.intent_description = "è¨ˆç®—æœ€å°å€¼"

        else:
            result.query_intent = QueryIntent.QUERY_INVENTORY
            result.intent_description = "ä¸€èˆ¬æŸ¥è©¢"

    def _recognize_entities(self, query: str, result: IntentAnalysisResult):
        """è­˜åˆ¥å¯¦é«”ï¼ˆè¡¨ã€æ¬„ä½ã€å€¼ï¼‰- æ•´åˆ CodeDictionary"""
        query_upper = query.upper()
        query_lower = query.lower()

        for table, keywords in self.table_keywords.items():
            if any(kw.upper() in query_upper for kw in keywords):
                result.table = table
                break

        if not result.table:
            if result.query_intent in [QueryIntent.QUERY_INVENTORY, QueryIntent.STATISTICS]:
                result.table = "img_file"
            elif result.query_intent == QueryIntent.QUERY_ORDER:
                result.table = "coptc_file"
            elif result.query_intent == QueryIntent.QUERY_PRICE:
                result.table = "prc_file"

        item_codes = re.findall(r"\b\d{6}\b", query)
        if item_codes:
            result.warnings.append(
                f"æ–™è™Ÿæ ¼å¼å¯èƒ½éŒ¯èª¤ï¼š{item_codes[0]}ï¼ˆæ­£ç¢ºæ ¼å¼ï¼šXX-XXXXï¼Œå¦‚ 10-0001ï¼‰"
            )

        item_codes_valid = re.findall(r"\b\d{2}-\d{4}\b", query)
        if item_codes_valid:
            for code in item_codes_valid:
                if re.match(r"^\d{2}-\d{4}$", code):
                    result.subject = "item_code"
                    result.subject_value = code
                    break

        warehouse_match = re.search(r"[Ww]0([1-5])", query)
        if warehouse_match:
            warehouse_num = warehouse_match.group(1)
            code = f"W0{warehouse_num}"
            result.warehouse = code

            code_info = self.code_dict.lookup(code)
            if code_info:
                warehouse_name = code_info.get("name", code_info.get("type", ""))
                result.warnings.append(f"ğŸ“– ä»£ç¢¼å­—å…¸ï¼š{code} â†’ {warehouse_name}")

                if not result.table:
                    result.table = code_info.get("table", "img_file")

            warehouse_context_keywords = ["åº«å­˜", "å­˜é‡", "åº«æˆ¿", "å€‰åº«", "å­˜è²¨", "æ–™è™Ÿ", "ç‰©æ–™"]
            has_context = any(kw in query_lower for kw in warehouse_context_keywords)

            if not has_context:
                result.warnings.append(
                    f"âš ï¸ å·²æ ¹æ“šä¸Šä¸‹æ–‡è­˜åˆ¥ç‚ºå€‰åº«ä»£ç¢¼ï¼š{code}ï¼ˆå»ºè­°æ˜ç¢ºæŒ‡å®šã€Œå€‰åº«ã€æˆ–ã€Œåº«æˆ¿ã€ï¼‰"
                )
        else:
            error_warehouse = re.search(r"[Ww]0([06-9])", query)
            if error_warehouse:
                invalid_code = f"W0{error_warehouse.group(1)}"
                result.warnings.append(f"å€‰åº«ä»£ç¢¼ç„¡æ•ˆï¼š{invalid_code}ï¼ˆæ­£ç¢ºæ ¼å¼ï¼šW01-W05ï¼‰")

        if re.search(r"[Ww]0[1-5].*[,ï¼Œ].*[Ww]0[1-5]", query):
            warehouses = re.findall(r"([Ww])0([1-5])", query)
            result.warehouse_condition = "IN"

    def _recognize_aggregation(self, query: str, result: IntentAnalysisResult):
        """è­˜åˆ¥èšåˆ/è¨ˆç®—æ„åœ–"""
        query_lower = query.lower()

        # è­˜åˆ¥ã€Œå„å€‰åº«ã€- æŒ‰å€‰åº«åˆ†çµ„çµ±è¨ˆ
        if "å„å€‰åº«" in query_lower:
            result.has_group_by = True
            result.group_by_field = "img02"
            result.warehouse = "å„å€‰åº«"
            # å¦‚æœæ²’æœ‰æŒ‡å®šèšåˆï¼Œé è¨­ç‚º SUM
            if not result.aggregation:
                result.aggregation = "SUM"

        # è­˜åˆ¥èšåˆå‡½æ•¸
        if any(kw in query_lower for kw in ["ç¸½è¨ˆ", "åˆè¨ˆ", "ç¸½é‡", "ç¸½æ•¸", "ç¸½"]):
            result.aggregation = "SUM"
            if not result.has_group_by:
                # æ ¹æ“šä¸Šä¸‹æ–‡æ±ºå®šåˆ†çµ„æ¬„ä½
                if "å„å€‰åº«" in query_lower:
                    result.group_by_field = "img02"
                else:
                    result.has_group_by = True
                    result.group_by_field = "img01"
        elif "å¹³å‡" in query_lower:
            result.aggregation = "AVG"
            if not result.has_group_by:
                if "å„å€‰åº«" in query_lower:
                    result.group_by_field = "img02"
                else:
                    result.has_group_by = True
        elif any(kw in query_lower for kw in ["å¤šå°‘ç­†", "å¹¾å€‹", "ç­†æ•¸"]):
            result.aggregation = "COUNT"
            if not result.has_group_by:
                if "å„å€‰åº«" in query_lower:
                    result.group_by_field = "img02"
                else:
                    result.has_group_by = True
        elif any(kw in query_lower for kw in ["æœ€é«˜", "æœ€å¤š", "æœ€å¤§"]):
            result.aggregation = "MAX"
        elif any(kw in query_lower for kw in ["æœ€ä½", "æœ€å°‘", "æœ€å°"]):
            result.aggregation = "MIN"

        # è­˜åˆ¥ã€Œæ¯å€‹ã€- è¡¨ç¤ºéœ€è¦åˆ†çµ„æŸ¥è©¢
        if "æ¯å€‹" in query_lower and not result.has_group_by:
            result.has_group_by = True
            result.group_by_field = "img01"
            if not result.aggregation:
                result.aggregation = "SUM"

        # è­˜åˆ¥æ’åº
        if any(kw in query_lower for kw in ["å‰", "æœ€å¤š", "æœ€é«˜", "æœ€å¤§"]):
            result.has_order_by = True
            result.order_by_field = "img10"
            result.order_direction = "DESC"
            limit_match = re.search(r"å‰(\d+)å€‹", query)
            if limit_match:
                result.limit = int(limit_match.group(1))
            else:
                result.limit = 10
        elif any(kw in query_lower for kw in ["å¾Œ", "æœ€å°‘", "æœ€ä½", "æœ€å°"]):
            result.has_order_by = True
            result.order_by_field = "img10"
            result.order_direction = "ASC"

    def _recognize_time_filter(self, query: str, result: IntentAnalysisResult):
        """è­˜åˆ¥æ™‚é–“éæ¿¾"""
        # ç°¡å–®çš„æ™‚é–“æ¨¡å¼åŒ¹é…
        year_month = re.search(r"(\d{4})[å¹´/\-](\d{1,2})", query)
        if year_month:
            result.has_time_filter = True
            result.time_granularity = TimeGranularity.MONTH
            result.time_start = f"{year_month.group(1)}-{year_month.group(2).zfill(2)}-01"

        # æŸ¥è©¢ã€Œæœ€è¿‘ã€
        if "æœ€è¿‘" in query:
            day_match = re.search(r"æœ€è¿‘(\d+)å¤©", query)
            if day_match:
                result.has_time_filter = True
                result.time_granularity = TimeGranularity.DAY
                result.time_start = f"-{day_match.group(1)} days"

    def _check_completeness(self, result: IntentAnalysisResult):
        """å®Œæ•´æ€§æª¢æŸ¥"""
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        if result.query_intent == QueryIntent.QUERY_INVENTORY:
            if not result.table:
                result.missing_info.append("æœªè­˜åˆ¥åˆ°æŸ¥è©¢çš„è³‡æ–™è¡¨")
            if result.aggregation:
                pass
            else:
                if not result.subject_value and not result.warehouse:
                    result.warnings.append("æœªæŒ‡å®šå…·é«”çš„æ–™è™Ÿæˆ–å€‰åº«ï¼Œå¯èƒ½è¿”å›å¤§é‡æ•¸æ“š")

        # æª¢æŸ¥å±éšªæ“ä½œ
        query = result.original_query.lower()
        dangerous_keywords = ["drop", "delete", "truncate", "update", "insert"]
        if any(kw in query for kw in dangerous_keywords):
            result.is_complete = False
            result.missing_info.append("æª¢æ¸¬åˆ°å±éšªé—œéµå­—ï¼Œè«‹ç¢ºèªæ˜¯å¦ç‚ºèª¤æ“ä½œ")

        # æª¢æŸ¥æ˜¯å¦æœ‰æ ¼å¼è­¦å‘Šï¼ˆå€‰åº«ä»£ç¢¼ç„¡æ•ˆã€æ–™è™Ÿæ ¼å¼éŒ¯èª¤ç­‰ï¼‰
        format_warnings = [w for w in result.warnings if "ç„¡æ•ˆ" in w]
        if format_warnings:
            result.is_complete = False
            for w in format_warnings:
                result.missing_info.append(w)

        # è¨­å®šå®Œæ•´æ€§
        result.is_complete = len(result.missing_info) == 0

    def _generate_precise_description(self, result: IntentAnalysisResult):
        """ç”Ÿæˆç²¾ç¢ºæè¿°"""
        parts = []

        # æ„åœ–
        parts.append(f"æ„åœ–ï¼š{result.intent_description}")

        # æŸ¥è©¢è¡¨
        if result.table:
            parts.append(f"è³‡æ–™è¡¨ï¼š{result.table}")

        # èšåˆ
        if result.aggregation:
            agg_map = {
                "SUM": "è¨ˆç®—ç¸½å’Œ",
                "AVG": "è¨ˆç®—å¹³å‡å€¼",
                "COUNT": "è¨ˆç®—æ•¸é‡",
                "MAX": "æ‰¾å‡ºæœ€å¤§å€¼",
                "MIN": "æ‰¾å‡ºæœ€å°å€¼",
            }
            parts.append(f"èšåˆæ–¹å¼ï¼š{agg_map.get(result.aggregation, result.aggregation)}")

            if result.has_group_by and result.group_by_field:
                parts.append(f"åˆ†çµ„ä¾æ“šï¼š{result.group_by_field}")

        # ç¯©é¸æ¢ä»¶
        filters = []
        if result.warehouse:
            if result.warehouse_condition == "IN":
                filters.append(f"å€‰åº« IN ({result.warehouse})")
            else:
                filters.append(f"å€‰åº« = {result.warehouse}")
        if result.subject_value:
            filters.append(f"æ–™è™Ÿ = {result.subject_value}")

        if filters:
            parts.append(f"ç¯©é¸æ¢ä»¶ï¼š{' AND '.join(filters)}")

        # æ’åº
        if result.has_order_by:
            direction = "é™åº" if result.order_direction == "DESC" else "å‡åº"
            parts.append(f"æ’åºï¼š{result.order_by_field} {direction}")
            if result.limit:
                parts.append(f"é™åˆ¶ï¼šå–å‰ {result.limit} ç­†")

        # æ™‚é–“
        if result.has_time_filter:
            parts.append(f"æ™‚é–“ç¯„åœï¼š{result.time_granularity.value}")

        # è­¦å‘Š
        if result.warnings:
            parts.append(f"âš ï¸ è­¦å‘Šï¼š{'ï¼›'.join(result.warnings)}")

        result.precise_description = " | ".join(parts)


# æ¸¬è©¦
if __name__ == "__main__":
    analyzer = IntentAnalyzer()

    test_queries = [
        "æŸ¥W01 åº«æˆ¿æ¯å€‹æ–™è™Ÿå­˜é‡",
        "è¨ˆç®—æ–™è™Ÿ 10-0001 çš„ç¸½åº«å­˜é‡",
        "æŸ¥è©¢å„å€‰åº«çš„å¹³å‡åº«å­˜é‡",
        "åˆ—å‡ºå‰ 10 å€‹åº«å­˜æœ€å¤šçš„ç‰©æ–™",
        "çµ±è¨ˆ 2025 å¹´ 1 æœˆçš„äº¤æ˜“ç­†æ•¸",
    ]

    for query in test_queries:
        print("=" * 60)
        print(f"æŸ¥è©¢ï¼š{query}")
        print("=" * 60)

        result = analyzer.analyze(query)

        print(f"\nåˆ†è©çµæœï¼š")
        for token in result.tokens:
            amb = " âš ï¸" if token.is_ambiguous else ""
            print(f"  - {token.word} ({token.pos}){amb}")

        print(f"\næ„åœ–åˆ†æï¼š")
        print(f"  - æ„åœ–é¡å‹ï¼š{result.query_intent.value}")
        print(f"  - æ„åœ–æè¿°ï¼š{result.intent_description}")
        print(f"  - è³‡æ–™è¡¨ï¼š{result.table}")
        print(f"  - èšåˆï¼š{result.aggregation}")
        print(f"  - å€‰åº«ï¼š{result.warehouse}")
        print(f"  - åˆ†çµ„ï¼š{result.has_group_by} ({result.group_by_field})")
        print(f"  - æ’åºï¼š{result.has_order_by} {result.order_direction} {result.limit}")
        print(f"  - å®Œæ•´æ€§ï¼š{'âœ… å®Œæ•´' if result.is_complete else 'âŒ ä¸å®Œæ•´'}")
        if result.warnings:
            print(f"  - è­¦å‘Šï¼š{'ï¼›'.join(result.warnings)}")

        print(f"\nç²¾ç¢ºæè¿°ï¼š")
        print(f"  {result.precise_description}")
        print()
