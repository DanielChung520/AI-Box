#!/usr/bin/env python3
# ä»£ç¢¼åŠŸèƒ½èªªæ˜: ç”Ÿæˆ Data-Agent 50 å ´æ™¯è©³ç´°æ¸¬è©¦å ±å‘Š
# å‰µå»ºæ—¥æœŸ: 2026-01-30
# å‰µå»ºäºº: Daniel Chung
# æœ€å¾Œä¿®æ”¹æ—¥æœŸ: 2026-01-30

"""ç”Ÿæˆè©³ç´°çš„ Data-Agent 50 å ´æ™¯æ¸¬è©¦å ±å‘Š

åŸºæ–¼ run_50_scenarios_results.json ç”Ÿæˆè©³ç´°çš„æ¸¬è©¦å ±å‘Šï¼Œ
åŒ…å« SQL èªæ³•æª¢æŸ¥ã€ç•°å¸¸è™•ç†æ©Ÿåˆ¶ã€åŸ·è¡Œæ™‚é–“ã€é¸éŒ¯è¡¨æª¢æ¸¬ç­‰è©³ç´°ä¿¡æ¯ã€‚
"""

import json
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import datetime


def load_results() -> Dict[str, Any]:
    """è®€å–æ¸¬è©¦çµæœ JSON"""
    results_path = Path(__file__).parent / "run_50_scenarios_results.json"
    with open(results_path, "r", encoding="utf-8") as f:
        return json.load(f)


def analyze_sql_syntax(sql: str) -> Dict[str, Any]:
    """åˆ†æ SQL èªæ³•

    Args:
        sql: SQL æŸ¥è©¢å­—ä¸²

    Returns:
        åŒ…å«èªæ³•åˆ†æçš„å­—å…¸
    """
    analysis = {
        "has_syntax_error": False,
        "has_select": "SELECT" in sql.upper(),
        "has_from": "FROM" in sql.upper(),
        "has_where": "WHERE" in sql.upper(),
        "has_join": any(
            k in sql.upper() for k in ["JOIN", "LEFT JOIN", "RIGHT JOIN", "INNER JOIN"]
        ),
        "has_aggregation": any(k in sql.upper() for k in ["SUM", "AVG", "MAX", "MIN", "COUNT"]),
        "has_group_by": "GROUP BY" in sql.upper(),
        "has_order_by": "ORDER BY" in sql.upper(),
        "has_limit": "LIMIT" in sql.upper(),
        "has_parameterized_query": "?" in sql or "$1" in sql or "$2" in sql or "$3" in sql,
        "table_names": [],
        "column_names": [],
        "syntax_warnings": [],
    }

    # æå–è¡¨åå’Œæ¬„ä½åï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
    if "FROM" in sql.upper():
        from_idx = sql.upper().find("FROM")
        after_from = sql[from_idx + 4 :].strip()

        # æå–å¯èƒ½çš„è¡¨å
        tables = []
        for token in ["ima_file", "img_file", "tlf_file", "pmn_file", "stock"]:
            if token in sql:
                tables.append(token)
        analysis["table_names"] = list(set(tables))

        # æå–å¯èƒ½çš„æ¬„ä½å
        columns = []
        for token in [
            "ima01",
            "ima02",
            "ima08",
            "ima25",
            "img01",
            "img02",
            "img04",
            "img10",
            "tlf01",
            "tlf06",
            "tlf10",
            "tlf19",
            "pmn01",
            "pmn02",
            "pmn10",
        ]:
            if token in sql:
                columns.append(token)
        analysis["column_names"] = list(set(columns))

    # èªæ³•æª¢æŸ¥
    if analysis["has_select"] and not analysis["has_from"]:
        analysis["syntax_warnings"].append("âš ï¸ SQL æœ‰ SELECT ä½†ç¼ºå°‘ FROM å­å¥")

    if not analysis["has_select"]:
        analysis["syntax_warnings"].append("âš ï¸ SQL ç¼ºå°‘ SELECT å­å¥")

    return analysis


def detect_wrong_table_usage(scenario_id: str, generated_sql: str, category: str) -> Dict[str, Any]:
    """åµæ¸¬é¸éŒ¯è¡¨çš„å•é¡Œ

    Args:
        scenario_id: å ´æ™¯ ID
        generated_sql: ç”Ÿæˆçš„ SQL
        category: å ´æ™¯åˆ†é¡

    Returns:
        åŒ…å«é¸éŒ¯è¡¨åˆ†æçš„å­—å…¸
    """
    analysis = {
        "has_wrong_table": False,
        "expected_tables": [],
        "actual_tables": [],
        "wrong_tables": [],
        "severity": "none",  # low, medium, high
        "suggestion": "",
    }

    # å°‡ SQL è½‰ç‚ºå¤§å¯«é€²è¡Œåˆ†æ
    sql_upper = generated_sql.upper()

    # åµæ¸¬å¯¦éš›ä½¿ç”¨çš„è¡¨
    actual_tables = []
    if "IMA_FILE" in sql_upper:
        actual_tables.append("ima_file")
    if "IMG_FILE" in sql_upper:
        actual_tables.append("img_file")
    if "TLF_FILE" in sql_upper:
        actual_tables.append("tlf_file")
    if "PMN_FILE" in sql_upper:
        actual_tables.append("pmn_file")

    analysis["actual_tables"] = actual_tables

    # æ ¹æ“šå ´æ™¯åˆ†é¡åˆ¤æ–·é æœŸè¡¨
    expected_tables = []

    # åº«å­˜ç›¸é—œæŸ¥è©¢æ‡‰ä½¿ç”¨ img_file
    if "åº«å­˜" in category or "stock" in generated_sql.lower():
        if "æŸ¥è©¢" in category and ("WHERE" in generated_sql or "LIMIT" in generated_sql):
            expected_tables.append("img_file")

    # äº¤æ˜“è¨˜éŒ„æ‡‰ä½¿ç”¨ tlf_file
    if "äº¤æ˜“" in category or "tlf" in generated_sql.lower():
        expected_tables.append("tlf_file")

    # ç‰©æ–™æŸ¥è©¢æ‡‰ä½¿ç”¨ ima_file
    if "ç‰©æ–™" in category or "å“å" in category:
        expected_tables.append("ima_file")

    # èšåˆæŸ¥è©¢ï¼ˆSUM, AVG, MAX, MINï¼‰æ¶‰åŠåº«å­˜é‡æ‡‰ä½¿ç”¨ img_file
    if any(k in sql_upper for k in ["SUM", "AVG", "MAX", "MIN"]):
        if "img10" in generated_sql.lower():
            expected_tables.append("img_file")

    analysis["expected_tables"] = expected_tables

    # åˆ¤æ–·æ˜¯å¦é¸éŒ¯è¡¨
    if expected_tables and actual_tables:
        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†é æœŸä»¥å¤–çš„è¡¨
        for table in actual_tables:
            if table not in expected_tables:
                analysis["wrong_tables"].append(table)

        # åˆ¤æ–·åš´é‡ç¨‹åº¦
        if analysis["wrong_tables"]:
            analysis["has_wrong_table"] = True
            # å¦‚æœå®Œå…¨æ²’æœ‰ä½¿ç”¨é æœŸè¡¨ï¼Œåš´é‡ç¨‹åº¦é«˜
            if not any(t in actual_tables for t in expected_tables):
                analysis["severity"] = "high"
                analysis["suggestion"] = (
                    f"æ‡‰ä½¿ç”¨ {expected_tables} è¡¨ï¼Œä½†å¯¦éš›ä½¿ç”¨äº† {actual_tables} è¡¨"
                )
            # å¦‚æœä½¿ç”¨äº†é æœŸè¡¨ä¹‹å¤–çš„å…¶ä»–è¡¨ï¼Œåš´é‡ç¨‹åº¦ä¸­ç­‰
            else:
                analysis["severity"] = "medium"
                analysis["suggestion"] = (
                    f"å»ºè­°ä½¿ç”¨ {expected_tables} è¡¨ï¼Œé¡å¤–ä½¿ç”¨äº† {analysis['wrong_tables']} è¡¨"
                )

    # ç‰¹åˆ¥æƒ…æ³ï¼šåº«å­˜æŸ¥è©¢ä½¿ç”¨äº† pmn_fileï¼ˆæ¡è³¼å–®ï¼‰é€šå¸¸æ˜¯éŒ¯çš„
    if "åº«å­˜" in category and "pmn_file" in actual_tables and "img_file" not in actual_tables:
        analysis["has_wrong_table"] = True
        analysis["severity"] = "high"
        analysis["suggestion"] = "åº«å­˜æŸ¥è©¢æ‡‰ä½¿ç”¨ img_file è€Œé pmn_file"

    # ç‰¹åˆ¥æƒ…æ³ï¼šäº¤æ˜“è¨˜éŒ„æŸ¥è©¢ä½¿ç”¨äº† pmn_file é€šå¸¸æ˜¯éŒ¯çš„
    if "äº¤æ˜“" in category and "pmn_file" in actual_tables and "tlf_file" not in actual_tables:
        analysis["has_wrong_table"] = True
        analysis["severity"] = "high"
        analysis["suggestion"] = "äº¤æ˜“è¨˜éŒ„æŸ¥è©¢æ‡‰ä½¿ç”¨ tlf_file è€Œé pmn_file"

    return analysis


def analyze_exception_handling(result: Dict[str, Any], scenario: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ†æç•°å¸¸è™•ç†æ©Ÿåˆ¶

    Args:
        result: æ¸¬è©¦çµæœ
        scenario: å ´æ™¯å®šç¾©

    Returns:
        åŒ…å«ç•°å¸¸è™•ç†åˆ†æçš„å­—å…¸
    """
    analysis = {
        "expected_failure": not scenario.get("expected_success", True),
        "actual_failure": not result.get("actual_success", True),
        "error_message": result.get("error"),
        "error_type": None,
        "has_proper_error_handling": False,
        "exception_notes": [],
    }

    # æª¢æŸ¥éŒ¯èª¤è¨Šæ¯
    error_msg = result.get("error", "")
    if error_msg:
        # åˆ¤æ–·éŒ¯èª¤é¡å‹
        if "validation" in error_msg.lower() or "required" in error_msg.lower():
            analysis["error_type"] = "åƒæ•¸é©—è­‰éŒ¯èª¤"
        elif "unknown action" in error_msg.lower() or "invalid action" in error_msg.lower():
            analysis["error_type"] = "ç„¡æ•ˆçš„ action éŒ¯èª¤"
        elif "dangerous" in error_msg.lower() or "injection" in error_msg.lower():
            analysis["error_type"] = "å®‰å…¨è­¦å‘Š"
        elif "sql" in error_msg.lower() or "syntax" in error_msg.lower():
            analysis["error_type"] = "SQL èªæ³•éŒ¯èª¤"
        else:
            analysis["error_type"] = "å…¶ä»–éŒ¯èª¤"

    # åˆ¤æ–·æ˜¯å¦æœ‰æ­£ç¢ºçš„éŒ¯èª¤è™•ç†
    if analysis["expected_failure"]:
        if analysis["actual_failure"]:
            if error_msg and len(error_msg) > 0:
                analysis["has_proper_error_handling"] = True
                analysis["exception_notes"].append(f"âœ… æ­£ç¢ºå›å ±éŒ¯èª¤: {analysis['error_type']}")
            else:
                analysis["exception_notes"].append("âš ï¸ å¤±æ•—ä½†ç¼ºå°‘éŒ¯èª¤è¨Šæ¯")
        else:
            analysis["exception_notes"].append("âŒ é æœŸå¤±æ•—ä½†å¯¦éš›æˆåŠŸ")
    else:
        if analysis["actual_failure"]:
            analysis["exception_notes"].append(f"âŒ æœªé æœŸçš„éŒ¯èª¤: {error_msg}")
        else:
            analysis["has_proper_error_handling"] = True
            analysis["exception_notes"].append("âœ… æ­£å¸¸åŸ·è¡Œ")

    return analysis


def generate_scenario_detail(result: Dict[str, Any], scenario: Dict[str, Any]) -> str:
    """ç”Ÿæˆå–®ä¸€å ´æ™¯çš„è©³ç´°å ±å‘Š

    Args:
        result: æ¸¬è©¦çµæœ
        scenario: å ´æ™¯å®šç¾©

    Returns:
        Markdown æ ¼å¼çš„è©³ç´°å ±å‘Š
    """
    sid = result["scenario_id"]
    category = result.get("category", "")
    passed = result.get("passed", False)
    duration = result.get("duration_sec")
    generated_sql = result.get("generated_sql", "")
    notes = result.get("notes", [])

    detail_lines = [
        f"#### {sid}: {category}",
        "",
        "**åŸ·è¡Œçµæœ:**",
        f"- ç‹€æ…‹: {'âœ… é€šé' if passed else 'âŒ å¤±æ•—'}",
    ]

    if duration is not None:
        detail_lines.append(f"- åŸ·è¡Œæ™‚é–“: {duration:.3f} ç§’")
    else:
        detail_lines.append("- åŸ·è¡Œæ™‚é–“: N/A")

    detail_lines.append("")

    # SQL èªæ³•åˆ†æ
    if generated_sql:
        detail_lines.extend(
            [
                "**ç”Ÿæˆçš„ SQL:**",
                f"```sql",
                generated_sql,
                f"```",
                "",
                "**SQL èªæ³•åˆ†æ:**",
            ]
        )

        syntax_analysis = analyze_sql_syntax(generated_sql)
        detail_lines.extend(
            [
                f"- åŒ…å« SELECT: {'âœ…' if syntax_analysis['has_select'] else 'âŒ'}",
                f"- åŒ…å« FROM: {'âœ…' if syntax_analysis['has_from'] else 'âŒ'}",
                f"- åŒ…å« WHERE: {'âœ…' if syntax_analysis['has_where'] else 'âŒ'}",
                f"- åŒ…å« JOIN: {'âœ…' if syntax_analysis['has_join'] else 'âŒ'}",
                f"- åŒ…å«èšåˆå‡½æ•¸: {'âœ…' if syntax_analysis['has_aggregation'] else 'âŒ'}",
                f"- åŒ…å« GROUP BY: {'âœ…' if syntax_analysis['has_group_by'] else 'âŒ'}",
                f"- åŒ…å« ORDER BY: {'âœ…' if syntax_analysis['has_order_by'] else 'âŒ'}",
                f"- åŒ…å« LIMIT: {'âœ…' if syntax_analysis['has_limit'] else 'âŒ'}",
                f"- ä½¿ç”¨åƒæ•¸åŒ–æŸ¥è©¢: {'âœ…' if syntax_analysis['has_parameterized_query'] else 'âŒ'}",
            ]
        )

        if syntax_analysis["table_names"]:
            detail_lines.append(f"- åµæ¸¬åˆ°çš„è¡¨å: {', '.join(syntax_analysis['table_names'])}")

        if syntax_analysis["column_names"]:
            detail_lines.append(f"- åµæ¸¬åˆ°çš„æ¬„ä½å: {', '.join(syntax_analysis['column_names'])}")

        if syntax_analysis["syntax_warnings"]:
            detail_lines.extend(
                [
                    "",
                    "**èªæ³•è­¦å‘Š:**",
                ]
            )
            for warning in syntax_analysis["syntax_warnings"]:
                detail_lines.append(f"- {warning}")

        # é¸éŒ¯è¡¨æª¢æ¸¬
        wrong_table_analysis = detect_wrong_table_usage(sid, generated_sql, category)
        if wrong_table_analysis["has_wrong_table"]:
            severity_icon = "ğŸ”´" if wrong_table_analysis["severity"] == "high" else "ğŸŸ¡"
            detail_lines.extend(
                [
                    "",
                    f"{severity_icon} **é¸éŒ¯è¡¨å•é¡Œ:**",
                    f"- åš´é‡ç¨‹åº¦: {wrong_table_analysis['severity'].upper()}",
                    f"- é æœŸè¡¨: {', '.join(wrong_table_analysis['expected_tables']) if wrong_table_analysis['expected_tables'] else 'æœªæŒ‡å®š'}",
                    f"- å¯¦éš›è¡¨: {', '.join(wrong_table_analysis['actual_tables'])}",
                    f"- éŒ¯èª¤è¡¨: {', '.join(wrong_table_analysis['wrong_tables'])}",
                    f"- å»ºè­°: {wrong_table_analysis['suggestion']}",
                ]
            )

        detail_lines.append("")

    # ç•°å¸¸è™•ç†åˆ†æ
    exception_analysis = analyze_exception_handling(result, scenario)
    detail_lines.extend(
        [
            "**ç•°å¸¸è™•ç†åˆ†æ:**",
            f"- é æœŸå¤±æ•—: {'æ˜¯' if exception_analysis['expected_failure'] else 'å¦'}",
            f"- å¯¦éš›å¤±æ•—: {'æ˜¯' if exception_analysis['actual_failure'] else 'å¦'}",
        ]
    )

    if exception_analysis["error_message"]:
        detail_lines.append(f"- éŒ¯èª¤è¨Šæ¯: {exception_analysis['error_message']}")
        detail_lines.append(f"- éŒ¯èª¤é¡å‹: {exception_analysis['error_type']}")

    if exception_analysis["exception_notes"]:
        detail_lines.extend(
            [
                "",
                "**ç•°å¸¸è™•ç†è©•ä¼°:**",
            ]
        )
        for note in exception_analysis["exception_notes"]:
            detail_lines.append(f"- {note}")

    detail_lines.append("")

    # å…¶ä»–å‚™è¨»
    if notes:
        detail_lines.extend(
            [
                "**æ¸¬è©¦å‚™è¨»:**",
            ]
        )
        for note in notes:
            detail_lines.append(f"- {note}")
        detail_lines.append("")

    # åŸ·è¡Œæ­¥é©Ÿè©³æƒ…ï¼ˆå¦‚æœæœ‰ conversion_logï¼‰
    result_summary = result.get("result_summary", {})
    if isinstance(result_summary, dict):
        result_data = result_summary.get("result", {})
        if isinstance(result_data, dict):
            conversion_log = result_data.get("conversion_log")
            if conversion_log and isinstance(conversion_log, dict):
                steps = conversion_log.get("steps", [])
                if steps:
                    detail_lines.extend(
                        [
                            "**åŸ·è¡Œæ­¥é©Ÿè©³æƒ…:**",
                        ]
                    )
                    for step in steps:
                        step_name = step.get("step", "unknown")
                        step_status = step.get("status", "unknown")
                        step_duration = step.get("duration_ms")
                        status_icon = "âœ…" if step_status == "success" else "âŒ"
                        if step_duration:
                            detail_lines.append(
                                f"- {status_icon} {step_name}: {step_duration:.2f}ms"
                            )
                        else:
                            detail_lines.append(f"- {status_icon} {step_name}")
                    detail_lines.append("")

    # Confidence
    if isinstance(result_summary, dict):
        result_data = result_summary.get("result", {})
        if isinstance(result_data, dict):
            confidence = result_data.get("confidence")
            if confidence is not None:
                detail_lines.extend(
                    [
                        "**ä¿¡å¿ƒåº¦åˆ†æ:**",
                        f"- LLM ä¿¡å¿ƒåº¦: {confidence:.2f}",
                    ]
                )
                if confidence >= 0.8:
                    detail_lines.append("- è©•ä¼°: é«˜ä¿¡å¿ƒåº¦")
                elif confidence >= 0.6:
                    detail_lines.append("- è©•ä¼°: ä¸­ç­‰ä¿¡å¿ƒåº¦")
                else:
                    detail_lines.append("- è©•ä¼°: ä½ä¿¡å¿ƒåº¦")
                detail_lines.append("")

    # Warnings
    if isinstance(result_summary, dict):
        result_data = result_summary.get("result", {})
        if isinstance(result_data, dict):
            warnings = result_data.get("warnings", [])
            if warnings:
                detail_lines.extend(
                    [
                        "**ç³»çµ±è­¦å‘Š:**",
                    ]
                )
                for warning in warnings:
                    detail_lines.append(f"- âš ï¸ {warning}")
                detail_lines.append("")

    detail_lines.append("---")
    detail_lines.append("")

    return "\n".join(detail_lines)


def generate_summary_report(
    data: Dict[str, Any],
    round_number: int = 5,
    previous_round_data: Optional[Dict[str, Any]] = None,
) -> str:
    """ç”Ÿæˆæ‘˜è¦å ±å‘Š

    Args:
        data: æ¸¬è©¦çµæœæ•¸æ“š
        round_number: æ¸¬è©¦è¼ªæ¬¡ï¼ˆé»˜èªç‚ºç¬¬5è¼ªï¼‰
        previous_round_data: ä¸Šä¸€è¼ªæ¸¬è©¦çµæœï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰

    Returns:
        Markdown æ ¼å¼çš„æ‘˜è¦å ±å‘Š
    """
    # ç²å–ç•¶å‰æ—¥æœŸå’Œæ™‚é–“
    current_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    summary = data.get("summary", {})
    results = data.get("results", [])

    total = summary.get("total", 0)
    passed = summary.get("passed", 0)
    failed = summary.get("failed", 0)
    pass_rate = (passed / total * 100) if total > 0 else 0

    # çµ±è¨ˆåŸ·è¡Œæ™‚é–“
    durations = [r.get("duration_sec", 0) for r in results if r.get("duration_sec")]
    avg_duration = sum(durations) / len(durations) if durations else 0
    max_duration = max(durations) if durations else 0
    min_duration = min(durations) if durations else 0

    # çµ±è¨ˆèªæ³•å•é¡Œ
    sql_issues = 0
    sql_success = 0
    for result in results:
        sql = result.get("generated_sql", "")
        if sql:
            analysis = analyze_sql_syntax(sql)
            if analysis["syntax_warnings"]:
                sql_issues += 1
            else:
                sql_success += 1

    # åˆ†é¡çµ±è¨ˆ
    categories = {}
    for result in results:
        cat = result.get("category", "æœªåˆ†é¡")
        if cat not in categories:
            categories[cat] = {"total": 0, "passed": 0, "failed": 0, "results": []}
        categories[cat]["total"] += 1
        if result.get("passed", False):
            categories[cat]["passed"] += 1
        else:
            categories[cat]["failed"] += 1
        categories[cat]["results"].append(result)

    summary_lines = [
        "# Data-Agent 50 å ´æ™¯æ¸¬è©¦è©³ç´°å ±å‘Š",
        "",
        f"**å ±å‘Šæ—¥æœŸï¼š** {current_datetime}",
        "**æ¸¬è©¦äººå“¡ï¼š** Daniel Chung",
        "**æ¸¬è©¦ç¯„åœï¼š** Data-Agent 50 å ´æ™¯æ¸¬è©¦è¨ˆåŠƒ",
        "**æ¸¬è©¦è¨ˆåŠƒï¼š** `Data-Agent-50å ´æ™¯æ¸¬è©¦è¨ˆåŠƒ.md`",
        f"**æ¸¬è©¦è¼ªæ¬¡ï¼š** ç¬¬ {round_number} è¼ªï¼ˆé–¾å€¼ 60 ç§’ï¼‰",
        "",
        "---",
        "",
        "## æ¸¬è©¦ç¸½çµ",
        "",
        f"æœ¬æ¬¡æ¸¬è©¦åŸ·è¡Œ Data-Agent 50 å ´æ™¯æ¸¬è©¦è¨ˆåŠƒï¼Œé©—è­‰ text_to_sqlã€execute_queryã€validate_query åŠç•°å¸¸è™•ç†åŠŸèƒ½ã€‚",
        "",
        "### æ¸¬è©¦çµæœç¸½çµ",
        "",
        "| çµ±è¨ˆé … | æ•¸å€¼ |",
        "|--------|------|",
        f"| ç¸½å ´æ™¯æ•¸ | {total} |",
        f"| é€šéæ•¸ | {passed} |",
        f"| å¤±æ•—æ•¸ | {failed} |",
        f"| é€šéç‡ | {pass_rate:.1f}% |",
        "",
    ]

    # å¦‚æœæœ‰ä¸Šä¸€è¼ªæ•¸æ“šï¼Œæ·»åŠ æ¯”è¼ƒ
    if previous_round_data:
        prev_summary = previous_round_data.get("summary", {})
        prev_passed = prev_summary.get("passed", 0)
        prev_failed = prev_summary.get("failed", 0)
        prev_pass_rate = (prev_passed / total * 100) if total > 0 else 0

        passed_change = passed - prev_passed
        failed_change = failed - prev_failed
        pass_rate_change = pass_rate - prev_pass_rate

        passed_arrow = "â¬†ï¸" if passed_change > 0 else "â¬‡ï¸" if passed_change < 0 else "â–"
        failed_arrow = "â¬†ï¸" if failed_change > 0 else "â¬‡ï¸" if failed_change < 0 else "â–"
        pass_rate_arrow = "â¬†ï¸" if pass_rate_change > 0 else "â¬‡ï¸" if pass_rate_change < 0 else "â–"

        summary_lines.extend(
            [
                "### èˆ‡ä¸Šä¸€è¼ªæ¯”è¼ƒï¼ˆç¬¬ 4 è¼ªï¼‰",
                "",
                "| çµ±è¨ˆé … | ç¬¬ 4 è¼ª | ç¬¬ 5 è¼ª | è®ŠåŒ– |",
                "|--------|--------|--------|------|",
                f"| é€šéæ•¸ | {prev_passed} | {passed} | {passed_arrow} {abs(passed_change)} |",
                f"| å¤±æ•—æ•¸ | {prev_failed} | {failed} | {failed_arrow} {abs(failed_change)} |",
                f"| é€šéç‡ | {prev_pass_rate:.1f}% | {pass_rate:.1f}% | {pass_rate_arrow} {abs(pass_rate_change):.1f}% |",
                "",
            ]
        )

    summary_lines.extend(
        [
            "### åŸ·è¡Œæ™‚é–“çµ±è¨ˆ",
            "",
            "| çµ±è¨ˆé … | æ•¸å€¼ |",
            "|--------|------|",
            f"| å¹³å‡åŸ·è¡Œæ™‚é–“ | {avg_duration:.3f} ç§’ |",
            f"| æœ€å¿«åŸ·è¡Œæ™‚é–“ | {min_duration:.3f} ç§’ |",
            f"| æœ€æ…¢åŸ·è¡Œæ™‚é–“ | {max_duration:.3f} ç§’ |",
            "",
            "### SQL èªæ³•åˆ†æ",
            "",
            "| çµ±è¨ˆé … | æ•¸å€¼ |",
            "|--------|------|",
            f"| ç”Ÿæˆ SQL çš„å ´æ™¯æ•¸ | {sql_success + sql_issues} |",
            f"| SQL èªæ³•æ­£ç¢º | {sql_success} |",
            f"| SQL èªæ³•æœ‰å•é¡Œ | {sql_issues} |",
            "",
            "### åˆ†é¡çµ±è¨ˆ",
            "",
            "| åˆ†é¡ | ç¸½æ•¸ | é€šé | å¤±æ•— | é€šéç‡ |",
            "|------|------|------|------|--------|",
        ]
    )

    for cat, stats in categories.items():
        cat_pass_rate = (stats["passed"] / stats["total"] * 100) if stats["total"] > 0 else 0
        status = "âœ…" if stats["passed"] == stats["total"] else "âŒ"
        summary_lines.append(
            f"| {status} {cat} | {stats['total']} | {stats['passed']} | {stats['failed']} | {cat_pass_rate:.1f}% |"
        )

    summary_lines.extend(
        [
            "",
            "---",
            "",
            "## æ¸¬è©¦çµæœè©³æƒ…",
            "",
        ]
    )

    return "\n".join(summary_lines)


def get_expected_success_from_scenario_id(scenario_id: str) -> bool:
    """æ ¹æ“šå ´æ™¯ ID åˆ¤æ–·é æœŸæ˜¯å¦æˆåŠŸ

    Args:
        scenario_id: å ´æ™¯ ID (å¦‚ "T2S-001")

    Returns:
        True è¡¨ç¤ºé æœŸæˆåŠŸï¼ŒFalse è¡¨ç¤ºé æœŸå¤±æ•—
    """
    # ç•°å¸¸è™•ç†å ´æ™¯é æœŸå¤±æ•—
    if "T2S-031" <= scenario_id <= "T2S-047":
        return False
    return True


def analyze_failure_reasons(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ†æå¤±æ•—åŸå› ä¸¦åˆ†é¡

    Args:
        results: æ¸¬è©¦çµæœåˆ—è¡¨

    Returns:
        å¤±æ•—åŸå› åˆ†æçš„å­—å…¸
    """
    failure_analysis = {
        "keyword_mismatch": [],  # é—œéµå­—ä¸åŒ¹é…
        "wrong_table": [],  # é¸éŒ¯è¡¨
        "timeout": [],  # è¶…æ™‚
        "schema_mismatch": [],  # Schema ä¸åŒ¹é…
        "validation_error": [],  # é©—è­‰éŒ¯èª¤
        "other": [],  # å…¶ä»–
        "by_category": {},  # æŒ‰åˆ†é¡çµ±è¨ˆ
    }

    for result in results:
        if not result.get("passed", False):
            sid = result["scenario_id"]
            category = result.get("category", "")
            notes = result.get("notes", [])

            # æŒ‰åˆ†é¡çµ±è¨ˆ
            if category not in failure_analysis["by_category"]:
                failure_analysis["by_category"][category] = []
            failure_analysis["by_category"][category].append(
                {
                    "scenario_id": sid,
                    "notes": notes,
                    "error": result.get("error"),
                    "generated_sql": result.get("generated_sql"),
                    "duration_sec": result.get("duration_sec"),
                }
            )

            # åˆ†æå¤±æ•—åŸå› 
            has_keyword_issue = any("é—œéµå­—" in note for note in notes)
            has_timeout = any("è¶…é–¾å€¼" in note for note in notes)
            has_validation = any("é æœŸ" in note for note in notes)

            # æª¢æŸ¥é¸éŒ¯è¡¨
            generated_sql = result.get("generated_sql", "")
            if generated_sql:
                wrong_table_analysis = detect_wrong_table_usage(sid, generated_sql, category)
                if wrong_table_analysis["has_wrong_table"]:
                    failure_analysis["wrong_table"].append(sid)

            if has_keyword_issue:
                failure_analysis["keyword_mismatch"].append(sid)
            elif has_timeout:
                failure_analysis["timeout"].append(sid)
            elif has_validation:
                failure_analysis["validation_error"].append(sid)
            else:
                failure_analysis["other"].append(sid)

    return failure_analysis


def generate_failure_analysis_report(data: Dict[str, Any]) -> str:
    """ç”Ÿæˆå¤±æ•—åŸå› åˆ†æå ±å‘Šï¼ˆæ ¹æ“šå¯¦éš›æ¸¬è©¦çµæœï¼‰

    Args:
        data: æ¸¬è©¦çµæœæ•¸æ“š

    Returns:
        Markdown æ ¼å¼çš„å¤±æ•—åŸå› åˆ†æå ±å‘Š
    """
    results = data.get("results", [])
    failed_results = [r for r in results if not r.get("passed", False)]

    analysis = analyze_failure_reasons(results)

    report_lines = [
        "---",
        "",
        "## å¤±æ•—åŸå› åˆ†æèˆ‡æ”¹é€²æ–¹å‘",
        "",
        f"æœ¬æ¬¡æ¸¬è©¦å…±æœ‰ **{len(failed_results)}** å€‹å ´æ™¯å¤±æ•—ï¼Œä»¥ä¸‹ç‚ºè©³ç´°çš„å¤±æ•—åŸå› åˆ†æèˆ‡æ”¹é€²å»ºè­°ã€‚",
        "",
        "### å¤±æ•—åŸå› åˆ†é¡çµ±è¨ˆ",
        "",
        "| å¤±æ•—åŸå›  | å ´æ™¯æ•¸ | å ´æ™¯ ID | åš´é‡ç¨‹åº¦ |",
        "|----------|--------|---------|----------|",
    ]

    # é—œéµå­—ä¸åŒ¹é…
    if analysis["keyword_mismatch"]:
        report_lines.append(
            f"| ğŸ”´ é—œéµå­—ä¸åŒ¹é… | {len(analysis['keyword_mismatch'])} | {', '.join(analysis['keyword_mismatch'][:10])}{'...' if len(analysis['keyword_mismatch']) > 10 else ''} | é«˜ |"
        )

    # é¸éŒ¯è¡¨
    if analysis["wrong_table"]:
        report_lines.append(
            f"| ğŸ”´ é¸éŒ¯è¡¨ | {len(analysis['wrong_table'])} | {', '.join(analysis['wrong_table'][:10])}{'...' if len(analysis['wrong_table']) > 10 else ''} | é«˜ |"
        )

    # è¶…æ™‚
    if analysis["timeout"]:
        report_lines.append(
            f"| ğŸŸ¡ è¶…æ™‚ | {len(analysis['timeout'])} | {', '.join(analysis['timeout'][:10])}{'...' if len(analysis['timeout']) > 10 else ''} | ä¸­ |"
        )

    # é©—è­‰éŒ¯èª¤
    if analysis["validation_error"]:
        report_lines.append(
            f"| ğŸ”´ é©—è­‰éŒ¯èª¤ | {len(analysis['validation_error'])} | {', '.join(analysis['validation_error'][:10])}{'...' if len(analysis['validation_error']) > 10 else ''} | é«˜ |"
        )

    # å…¶ä»–
    if analysis["other"]:
        report_lines.append(
            f"| ğŸŸ¡ å…¶ä»– | {len(analysis['other'])} | {', '.join(analysis['other'][:10])}{'...' if len(analysis['other']) > 10 else ''} | ä¸­ |"
        )

    # æŒ‰åˆ†é¡çµ±è¨ˆå¤±æ•—
    report_lines.extend(
        [
            "",
            "### æŒ‰åˆ†é¡çš„å¤±æ•—è©³æƒ…",
            "",
        ]
    )

    for cat, failures in analysis["by_category"].items():
        if failures:
            report_lines.append(f"#### {cat}")
            report_lines.append("")
            report_lines.append("| å ´æ™¯ ID | å•é¡Œæè¿° | åŸ·è¡Œæ™‚é–“ | éŒ¯èª¤/å‚™è¨» |")
            report_lines.append("|---------|----------|----------|-----------|")
            for f in failures:
                sid = f["scenario_id"]
                notes_str = "; ".join(f["notes"]) if f["notes"] else "-"
                duration = f"{f['duration_sec']:.3f}s" if f["duration_sec"] else "N/A"
                report_lines.append(f"| {sid} | {notes_str} | {duration} | {f['error'] or '-'} |")
            report_lines.append("")

    # å•é¡Œå®šä½èˆ‡æ”¹é€²æ–¹å‘ï¼ˆæ ¹æ“šå¯¦éš›å¤±æ•—å ´æ™¯ï¼‰
    report_lines.extend(
        [
            "### å•é¡Œå®šä½èˆ‡æ”¹é€²æ–¹å‘ï¼ˆç¬¬ 5 è¼ªï¼‰",
            "",
        ]
    )

    # ç²å–å¤±æ•—çš„å ´æ™¯ ID
    failed_scenario_ids = [r["scenario_id"] for r in failed_results]

    # æ ¹æ“šå…·é«”å¤±æ•—å ´æ™¯ç”Ÿæˆé‡å°æ€§æ”¹é€²å»ºè­°
    report_lines.extend(
        [
            "#### 1. é—œéµå­—ä¸åŒ¹é…å•é¡Œï¼ˆç¬¬ 5 è¼ªå¯¦éš›åˆ†æï¼‰",
            "",
            "**ç¬¬ 5 è¼ªå¤±æ•—å ´æ™¯:**",
            f"- {', '.join(analysis['keyword_mismatch'])}",
            "",
            "**ç¾è±¡åˆ†æ:**",
            "- T2S-003ï¼ˆOR æ¢ä»¶ï¼‰: ç”Ÿæˆçš„ SQL æœªåŒ…å« OR é—œéµå­—",
            "- T2S-010ï¼ˆGROUP BY åˆ†çµ„ï¼‰: ç”Ÿæˆçš„ SQL æœªåŒ…å« GROUP BYã€img02ã€img10",
            "- T2S-022ï¼ˆBottom Nï¼‰: ç”Ÿæˆçš„ SQL æœªåŒ…å« ORDER BYã€ASCã€LIMITã€5",
            "- T2S-026ï¼ˆHAVING æ¢ä»¶ï¼‰: ç”Ÿæˆçš„ SQL æœªåŒ…å« GROUP BYã€HAVINGã€SUMã€img10",
            "- T2S-028ï¼ˆæ—¥æœŸç¯„åœï¼‰: ç”Ÿæˆçš„ SQL æœªåŒ…å« tlf06ã€01",
            "",
            "**å•é¡Œå®šä½:**",
            "- **OR æ¢ä»¶ç¯„ä¾‹ç¼ºå¤±**: 6 å€‹ç¯„ä¾‹ä¸­æ²’æœ‰ OR æ¢ä»¶çš„ç¯„ä¾‹",
            "- **è¤‡é›œæŸ¥è©¢ç¯„ä¾‹ä¸å¤ **: GROUP BY + HAVINGã€Bottom Nã€æ—¥æœŸç¯„åœç­‰è¤‡é›œæŸ¥è©¢çš„ç¯„ä¾‹ç¼ºå¤±",
            "- **é—œéµå­—æª¢æ¸¬é‚è¼¯**: æ¸¬è©¦è…³æœ¬çš„é—œéµå­—æª¢æŸ¥å¯èƒ½å°æŸäº› SQL èªæ³•éæ–¼åš´æ ¼",
            "",
            "**ç¬¬ 5 è¼ªå·²åŸ·è¡Œçš„æ”¹é€²:**",
            "- âœ… å·²å¢å¼· Schema æç¤ºï¼ˆæ·»åŠ æ¬„ä½èªªæ˜ã€è¡¨ç”¨é€”ã€è©å½™æ˜ å°„ï¼‰",
            "- âœ… å·²æ·»åŠ  6 å€‹æ¨™æº–ç¯„ä¾‹",
            "- âœ… å·²æ·»åŠ è¡¨é—œä¿‚èªªæ˜",
            "- âœ… é€šéç‡å¾ 48% æå‡è‡³ 84%ï¼ˆæå‡ 36%ï¼‰",
            "",
            "**é€²ä¸€æ­¥æ”¹é€²æ–¹å‘:**",
            "1. **æ·»åŠ è¤‡é›œæŸ¥è©¢ç¯„ä¾‹**",
            "   - æ·»åŠ  OR æ¢ä»¶ç¯„ä¾‹ï¼ˆT2S-003 å¤±æ•—ï¼‰",
            "   - æ·»åŠ  GROUP BY + HAVING ç¯„ä¾‹ï¼ˆT2S-026 å¤±æ•—ï¼‰",
            "   - æ·»åŠ  ORDER BY + LIMIT ç¯„ä¾‹ï¼ˆT2S-022 å¤±æ•—ï¼‰",
            "   - æ·»åŠ æ—¥æœŸç¯„åœç¯„ä¾‹ï¼ˆT2S-028 å¤±æ•—ï¼‰",
            "",
            "2. **å„ªåŒ–é—œéµå­—æª¢æŸ¥é‚è¼¯**",
            "   - è€ƒæ…® LLM å¯èƒ½ä½¿ç”¨ä¸åŒçš„ SQL èªæ³•é¢¨æ ¼",
            "   - æª¢æŸ¥é—œéµå­—çš„åŒç¾©è©ï¼ˆä¾‹å¦‚ UNION ALL å¯èƒ½æœƒå–ä»£ ORï¼‰",
            "   - å°è¤‡é›œæŸ¥è©¢ï¼Œæª¢æŸ¥å…¶ç­‰æ•ˆçš„ SQL èªæ³•",
            "",
            "3. **å¢åŠ ç¯„ä¾‹æ•¸é‡**",
            "   - å¾ 6 å€‹ç¯„ä¾‹å¢åŠ åˆ° 10-12 å€‹ç¯„ä¾‹",
            "   - è¦†è“‹æ›´å¤šæŸ¥è©¢æ¨¡å¼å’Œé—œéµå­—çµ„åˆ",
            "",
        ]
    )

    # å•é¡Œ 2: é©—è­‰éŒ¯èª¤
    if analysis["validation_error"]:
        report_lines.extend(
            [
                "",
                "#### 2. validate_query é©—è­‰é‚è¼¯å•é¡Œï¼ˆç¬¬ 5 è¼ªå¯¦éš›åˆ†æï¼‰",
                "",
                "**ç¬¬ 5 è¼ªå¤±æ•—å ´æ™¯:**",
                f"- {', '.join(analysis['validation_error'])}",
                "",
                "**ç¾è±¡åˆ†æ:**",
                "- T2S-039ï¼ˆç„¡æ•ˆ SQL èªæ³•ï¼‰: SQL ç‚º `SELECT FROM img_file`ï¼Œæ‡‰è©²è¢«é©—è­‰ç‚º invalidï¼Œä½†å¯¦éš›é€šé",
                "- T2S-041ï¼ˆvalidate_query ä¸é€šéï¼‰: é æœŸ valid=Falseï¼Œä½†å¯¦éš›è¿”å› valid=True",
                "",
                "**å•é¡Œå®šä½:**",
                "- **sqlparse å°å…¥å¤±æ•—**: sqlparse åº«å¯èƒ½æœªæ­£ç¢ºå°å…¥æˆ–ä½¿ç”¨",
                "- **é©—è­‰é‚è¼¯ä¸å®Œæ•´**: validate_query å¯èƒ½æ²’æœ‰ä½¿ç”¨ sqlparse é€²è¡Œé©—è­‰",
                "- **æ¸¬è©¦ç’°å¢ƒå·®ç•°**: sqlparse å¯èƒ½åœ¨æ¸¬è©¦ç’°å¢ƒä¸­ä¸å¯ç”¨",
                "",
                "**ç¬¬ 5 è¼ªå·²åŸ·è¡Œçš„æ”¹é€²:**",
            ]
        )

        # æª¢æŸ¥æ˜¯å¦æœ‰ sqlparse
        try:
            import sqlparse

            has_sqlparse = True
        except ImportError:
            has_sqlparse = False

        report_lines.extend(
            [
                f"- âœ… ä»£ç¢¼ä¸­å·²æ·»åŠ  sqlparse å°å…¥",
                f"- âŒ sqlparse {'å·²å°å…¥' if has_sqlparse else 'æœªå°å…¥ï¼ˆæ¸¬è©¦ç’°å¢ƒä¸å¯ç”¨ï¼‰'}",
                "",
                "**é€²ä¸€æ­¥æ”¹é€²æ–¹å‘:**",
                "1. **ç¢ºä¿ sqlparse å¯ç”¨**",
                f"   - {'åœ¨æ¸¬è©¦ç’°å¢ƒå®‰è£ sqlparse: pip3 install sqlparse' if not has_sqlparse else 'sqlparse å·²å°å…¥'}",
                "   - é©—è­‰ sqlparse æ˜¯å¦æ­£ç¢ºå°å…¥",
                "",
                "2. **å¢å¼·é©—è­‰é‚è¼¯**",
                "   - ä½¿ç”¨ sqlparse æª¢æŸ¥ SQL èªæ³•çµæ§‹",
                "   - æª¢æŸ¥ SELECT æ˜¯å¦æœ‰æ¬„ä½ã€æ˜¯å¦æœ‰ FROM å­å¥",
                "   - æª¢æŸ¥æ˜¯å¦æœ‰å±éšªé—œéµå­—",
                "",
                "3. **æ·»åŠ æ›´å¤šç„¡æ•ˆ SQL æ¸¬è©¦æ¡ˆä¾‹**",
                "   - æ·»åŠ  `SELECT FROM`ï¼ˆç„¡æ¬„ä½ï¼‰",
                "   - æ·»åŠ  `FROM table`ï¼ˆç„¡ SELECTï¼‰",
                "   - æ·»åŠ  `DROP TABLE`",
                "",
            ]
        )

    # å•é¡Œ 3: é¸éŒ¯è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
    if analysis["wrong_table"]:
        report_lines.extend(
            [
                "",
                "#### 3. é¸éŒ¯è¡¨å•é¡Œï¼ˆç¬¬ 5 è¼ªå¯¦éš›åˆ†æï¼‰",
                "",
                "**ç¬¬ 5 è¼ªå¤±æ•—å ´æ™¯:**",
                f"- {', '.join(analysis['wrong_table'][:10])}",
                "",
                "**ç¾è±¡åˆ†æ:**",
                "- T2S-010: LLM ä½¿ç”¨äº†æ‰€æœ‰è¡¨ï¼ˆima_file, img_file, tlf_file, pmn_fileï¼‰çš„ UNIONï¼Œä½†é æœŸåªä½¿ç”¨ img_file",
                "",
                "**å•é¡Œå®šä½:**",
                "- **è¡¨é¸æ“‡é‚è¼¯éæ–¼å¯¬é¬†**: LLM å¯èƒ½èªç‚ºæ‰€æœ‰è¡¨éƒ½å¯èƒ½åŒ…å«ç›¸é—œæ•¸æ“š",
                "- **UNION ALL æ¿«é€²è¡¨é¸æ“‡**: LLM ä½¿ç”¨ UNION ALL åˆä½µæ‰€æœ‰è¡¨ï¼Œå°è‡´é¸éŒ¯è¡¨",
                "",
                "**ç¬¬ 5 è¼ªå·²åŸ·è¡Œçš„æ”¹é€²:**",
                "- âœ… å·²æ·»åŠ è¡¨ç”¨é€”èªªæ˜",
                "- âœ… å·²æ·»åŠ è¡¨é—œä¿‚èªªæ˜",
                "- âœ… å·²æ·»åŠ è©å½™æ˜ å°„è¡¨",
                "",
                "**é€²ä¸€æ­¥æ”¹é€²æ–¹å‘:**",
                "1. **å¼·åŒ–è¡¨é¸æ“‡æç¤º**",
                "   - åœ¨ prompt ä¸­æ›´æ˜ç¢ºåœ°æŒ‡å®šï¼šã€Œåº«å­˜æŸ¥è©¢ã€åªä½¿ç”¨ img_file",
                "   - æ·»åŠ åä¾‹ç¯„ä¾‹ï¼Œå±•ç¤ºéŒ¯èª¤çš„ UNION ALL ç”¨æ³•",
                "",
                "2. **æ”¹é€²æŸ¥è©¢ç†è§£**",
                "   - æ·»åŠ æ›´å¤šé—œéµè©åˆ°è©å½™æ˜ å°„è¡¨",
                "   - æ˜ç¢ºå€åˆ†ä¸åŒæŸ¥è©¢é¡å‹æ‡‰ä½¿ç”¨çš„è¡¨",
                "",
            ]
        )

    # å•é¡Œ 4: å…¶ä»–ï¼ˆT2S-038 SQL æ³¨å…¥ï¼‰
    if "T2S-038" in failed_scenario_ids:
        report_lines.extend(
            [
                "",
                "#### 4. SQL æ³¨å…¥å˜—è©¦å•é¡Œï¼ˆç¬¬ 5 è¼ªå¯¦éš›åˆ†æï¼‰",
                "",
                "**ç¾è±¡:**",
                "- T2S-038: SQL æ³¨å…¥å˜—è©¦ï¼ˆ`'; DROP TABLE img_file;--`ï¼‰è¢«è­˜åˆ¥ç‚ºã€Œç”¢å‡ºå«å±éšªé—œéµå­—ã€è€Œå¤±æ•—",
                "",
                "**åˆ†æ:**",
                "- é€™æ˜¯é æœŸçš„è¡Œç‚ºï¼šSQL æ³¨å…¥æ”»æ“Šæ‡‰è©²è¢«æ‹’çµ•",
                "- ä½†æ¸¬è©¦å¤±æ•—çš„åŸå› æ˜¯ã€Œç”¢å‡ºå«å±éšªé—œéµå­—ã€ï¼Œè€Œä¸æ˜¯ã€ŒSQL è¢«æ‹’çµ•ã€",
                "- å¯èƒ½æ˜¯ validate_query æˆ– text_to_sql çš„é‚è¼¯éœ€è¦èª¿æ•´",
                "",
                "**æ”¹é€²æ–¹å‘:**",
                "1. **æ˜ç¢ºæ¸¬è©¦é æœŸ**",
                "   - é€™å€‹å ´æ™¯æ‡‰è©²æˆåŠŸï¼ˆSQL è¢«æ‹’çµ•ï¼Œä½†ç³»çµ±æ­£ç¢ºè­˜åˆ¥äº†å±éšªæ“ä½œï¼‰",
                "   - è€ƒæ…®ä¿®æ”¹æ¸¬è©¦é æœŸï¼Œå°‡æ­¤å ´æ™¯æ”¹ç‚ºé€šé",
                "",
            ]
        )

    # ç¸½çµ
    report_lines.extend(
        [
            "### æ”¹é€²å„ªå…ˆç´šå»ºè­°ï¼ˆç¬¬ 5 è¼ªå¾Œï¼‰",
            "",
            "**é«˜å„ªå…ˆç´šï¼ˆçŸ­æœŸï¼Œ1-2 é€±ï¼‰:**",
            "1. âœ… å¢å¼· Schema æç¤ºï¼Œæ·»åŠ æ¬„ä½ä¸­æ–‡èªªæ˜å’Œæ¥­å‹™å«ç¾©ï¼ˆå·²å®Œæˆï¼‰",
            "2. âœ… æ˜ç¢ºè¡¨ç”¨é€”èªªæ˜ï¼Œèªªæ˜æ¯å€‹è¡¨çš„æ¥­å‹™ç”¨é€”å’Œä½¿ç”¨å ´æ™¯ï¼ˆå·²å®Œæˆï¼‰",
            "3. âœ… å»ºç«‹ã€Œè‡ªç„¶èªè¨€è©å½™ã€åˆ°ã€ŒSQL æ¬„ä½ã€çš„æ˜ å°„è¡¨ï¼ˆå·²å®Œæˆï¼‰",
            "4. ğŸ”² æ·»åŠ  OR æ¢ä»¶ç¯„ä¾‹ï¼ˆT2S-003 å¤±æ•—ï¼‰",
            "5. ğŸ”² æ·»åŠ  GROUP BY + HAVING ç¯„ä¾‹ï¼ˆT2S-026 å¤±æ•—ï¼‰",
            "6. ğŸ”² æ·»åŠ  ORDER BY + LIMIT ç¯„ä¾‹ï¼ˆT2S-022 å¤±æ•—ï¼‰",
            "7. ğŸ”² æ·»åŠ æ—¥æœŸç¯„åœç¯„ä¾‹ï¼ˆT2S-028 å¤±æ•—ï¼‰",
            "8. ğŸ”² ç¢ºä¿ sqlparse æ­£ç¢ºå®‰è£å’Œå°å…¥ï¼ˆT2S-039, T2S-041 å¤±æ•—ï¼‰",
            "",
            "**ä¸­å„ªå…ˆç´šï¼ˆä¸­æœŸï¼Œ1-2 æœˆï¼‰:**",
            "1. âœ… åœ¨ prompt ä¸­æ·»åŠ  6 å€‹æ¨™æº–ç¯„ä¾‹ï¼ˆå·²å®Œæˆï¼‰",
            "2. âœ… æ·»åŠ è¡¨é—œä¿‚èªªæ˜ï¼ˆå·²å®Œæˆï¼‰",
            "3. âš ï¸ å„ªåŒ–é—œéµå­—æª¢æŸ¥é‚è¼¯ï¼ˆè€ƒæ…®åŒç¾©è©å’Œç­‰æ•ˆ SQL èªæ³•ï¼‰",
            "4. âš ï¸ æŸ¥è©¢å ´æ™¯åˆ†é¡ï¼Œæ˜ç¢ºæ¯é¡æŸ¥è©¢æ‡‰ä½¿ç”¨çš„ä¸»è¦è¡¨",
            "5. âš ï¸ å„ªåŒ– promptï¼Œæ¸›å°‘ token æ•¸é‡",
            "6. âš ï¸ æ·»åŠ æŸ¥è©¢å¿«å–æ©Ÿåˆ¶ï¼Œæ¸›å°‘é‡è¤‡æŸ¥è©¢çš„ LLM èª¿ç”¨",
            "",
            "**ä½å„ªå…ˆç´šï¼ˆé•·æœŸï¼Œ3 æœˆä»¥ä¸Šï¼‰:**",
            "1. ğŸ”² è€ƒæ…®ä½¿ç”¨æ›´å¿«çš„ LLM æ¨¡å‹æˆ– GPU åŠ é€Ÿ",
            "2. ğŸ”² å»ºç«‹äººæ©Ÿå›é¥‹æ©Ÿåˆ¶ï¼Œæ”¶é›†å¤±æ•—æ¡ˆä¾‹ç”¨æ–¼æ”¹é€²",
            "3. ğŸ”² å¯¦ç¾ä¸¦è¡Œè™•ç†ï¼Œæ¸›å°‘æ•´é«”ç­‰å¾…æ™‚é–“",
            "",
            "### é æœŸæ”¹é€²æ•ˆæœï¼ˆç¬¬ 6 è¼ªç›®æ¨™ï¼‰",
            "",
            "å¯¦æ–½ä¸Šè¿°æ”¹é€²æªæ–½å¾Œï¼Œé æœŸå¯ä»¥é”åˆ°ä»¥ä¸‹æ•ˆæœï¼š",
            "",
            "| æ”¹é€²é …ç›® | ç¬¬ 5 è¼ªç‹€æ…‹ | ç¬¬ 6 è¼ªç›®æ¨™ |",
            "|----------|----------|----------|",
            "| é—œéµå­—ä¸åŒ¹é…å¤±æ•— | 5 å ´æ™¯ | é™ä½è‡³ 0 å ´æ™¯ |",
            "| é¸éŒ¯è¡¨å¤±æ•— | 2 å ´æ™¯ | é™ä½è‡³ 0 å ´æ™¯ |",
            "| é©—è­‰éŒ¯èª¤ | 2 å ´æ™¯ | é™ä½è‡³ 0 å ´æ™¯ |",
            "| SQL æ³¨å…¥å˜—è©¦ | 1 å ´æ™¯ï¼ˆè¡Œç‚ºæ­£ç¢ºï¼Œéœ€èª¿æ•´æ¸¬è©¦é æœŸï¼‰ | èª¿æ•´æ¸¬è©¦é æœŸ |",
            "| å¹³å‡åŸ·è¡Œæ™‚é–“ | 1.953 ç§’ | ç¶­æŒåœ¨ 2 ç§’ä»¥å…§ |",
            "| æ•´é«”é€šéç‡ | 84.0% | æå‡è‡³ > 90% |",
            "",
            "---",
            "",
        ]
    )

    return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•¸ï¼šç”Ÿæˆå®Œæ•´å ±å‘Š"""
    data = load_results()

    # å˜—è©¦åŠ è¼‰ä¸Šä¸€è¼ªæ¸¬è©¦çµæœï¼ˆç¬¬4è¼ªï¼‰
    previous_round_data = None
    try:
        # æŸ¥æ‰¾å¯èƒ½çš„ä¸Šä¸€è¼ªçµæœæ–‡ä»¶
        import glob

        result_files = list(Path(__file__).parent.glob("run_50_scenarios_results_*.json"))
        # æ‰¾åˆ°æœ€æ–°çš„çµæœæ–‡ä»¶ï¼ˆæ’é™¤ç•¶å‰çš„ round5 æ–‡ä»¶ï¼‰
        if len(result_files) > 1:
            # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œå–ç¬¬äºŒæ–°çš„
            result_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            # æ’é™¤ round5 æ–‡ä»¶ï¼ˆåç¨±åŒ…å« round5ï¼‰
            filtered_files = [f for f in result_files if "round5" not in f.name]

            for result_file in filtered_files[1:2]:  # å˜—è©¦æœ€å¤š 2 å€‹æ–‡ä»¶
                try:
                    with open(result_file, "r", encoding="utf-8") as f:
                        previous_round_data = json.load(f)
                    print(f"æ‰¾åˆ°ä¸Šä¸€è¼ªæ¸¬è©¦çµæœ: {result_file}")
                    break
                except Exception as e:
                    print(f"ç„¡æ³•è®€å– {result_file}: {e}")
                    continue

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°éæ¿¾å¾Œçš„æ–‡ä»¶ï¼Œå˜—è©¦æ‰‹å‹•æŸ¥æ‰¾ round4 æ–‡ä»¶
        if previous_round_data is None:
            round4_file = (
                Path(__file__).parent / "run_50_scenarios_results_round4_20260130_090349.json"
            )
            if round4_file.exists():
                with open(round4_file, "r", encoding="utf-8") as f:
                    previous_round_data = json.load(f)
                print(f"æ‰¾åˆ°ç¬¬4è¼ªæ¸¬è©¦çµæœ: {round4_file}")
    except Exception as e:
        print(f"ç„¡æ³•åŠ è¼‰ä¸Šä¸€è¼ªæ¸¬è©¦çµæœ: {e}")

    # ç”Ÿæˆå ±å‘Šï¼ˆç¬¬5è¼ªï¼‰
    report_lines = [
        generate_summary_report(data, round_number=5, previous_round_data=previous_round_data),
    ]

    # æ·»åŠ æ¯å€‹å ´æ™¯çš„è©³ç´°å ±å‘Š
    results = data.get("results", [])

    for result in results:
        sid = result["scenario_id"]
        # æ§‹å»ºç°¡å–®çš„å ´æ™¯å®šç¾©ï¼ˆåªéœ€è¦ expected_successï¼‰
        scenario = {
            "scenario_id": sid,
            "expected_success": get_expected_success_from_scenario_id(sid),
        }
        detail = generate_scenario_detail(result, scenario)
        report_lines.append(detail)

    # æ·»åŠ å¤±æ•—åŸå› åˆ†æå ±å‘Š
    failure_report = generate_failure_analysis_report(data)
    report_lines.append(failure_report)

    # å¯«å…¥å ±å‘Šæ–‡ä»¶
    output_path = Path(__file__).parent / "Data-Agent-50å ´æ™¯æ¸¬è©¦å ±å‘Š.md"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))

    print(f"è©³ç´°æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {output_path}")


if __name__ == "__main__":
    main()
