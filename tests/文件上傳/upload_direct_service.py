#!/usr/bin/env python3
# ä»£ç¢¼åŠŸèƒ½èªªæ˜: ç›´æ¥ä½¿ç”¨ç³»çµ±æœå‹™ä¸Šå‚³æ–‡ä»¶ï¼ˆç¹é HTTP APIï¼‰
# å‰µå»ºæ—¥æœŸ: 2026-01-04
# å‰µå»ºäºº: Daniel Chung
# æœ€å¾Œä¿®æ”¹æ—¥æœŸ: 2026-01-04

"""ç›´æ¥ä½¿ç”¨ç³»çµ±æœå‹™ä¸Šå‚³æ–‡ä»¶ä¸¦è§¸ç™¼è™•ç†"""

import sys
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
env_file = project_root / ".env"
if env_file.exists():
    load_dotenv(env_file, override=True)

from storage.file_storage import get_storage
from services.api.services.file_metadata_service import get_metadata_service
from services.api.services.upload_status_service import get_upload_status_service
from services.api.processors.parser_factory import get_parser_factory
from services.api.processors.chunk_processor import ChunkProcessor
from system.security.auth import get_system_user_token
from database.arangodb.client import get_arangodb_client

import uuid
from datetime import datetime


async def upload_and_process_file():
    """ç›´æ¥ä½¿ç”¨ç³»çµ±æœå‹™ä¸Šå‚³ä¸¦è™•ç†æ–‡ä»¶"""
    
    # æ¸¬è©¦æ–‡ä»¶
    test_file = project_root / 'docs' / 'ä¸œæ–¹ä¼Šå¨-é¢„åˆ¶èœå‘å±•ç­–ç•¥æŠ¥å‘Š20250902.pdf'
    
    if not test_file.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    file_size_mb = test_file.stat().st_size / (1024 * 1024)
    print("=" * 80)
    print("ğŸ“¤ ç›´æ¥ä½¿ç”¨ç³»çµ±æœå‹™ä¸Šå‚³æ–‡ä»¶")
    print("=" * 80)
    print(f"æ–‡ä»¶: {test_file.name}")
    print(f"å¤§å°: {file_size_mb:.2f} MB")
    print()
    
    # ç²å–ç³»çµ±ç”¨æˆ¶ IDï¼ˆä½¿ç”¨é»˜èªç³»çµ±ç”¨æˆ¶ï¼‰
    system_user_id = "system"
    
    try:
        # 1. è®€å–æ–‡ä»¶å…§å®¹
        print("ğŸ“– è®€å–æ–‡ä»¶å…§å®¹...")
        with open(test_file, 'rb') as f:
            file_content = f.read()
        print(f"  âœ… æ–‡ä»¶è®€å–å®Œæˆ ({len(file_content) / (1024*1024):.2f} MB)")
        print()
        
        # 2. ä¿å­˜æ–‡ä»¶åˆ°å­˜å„²
        print("ğŸ’¾ ä¿å­˜æ–‡ä»¶åˆ°å­˜å„²...")
        storage = get_storage()
        file_id = str(uuid.uuid4())
        file_path = storage.save_file(file_content, test_file.name, file_id=file_id)
        print(f"  âœ… æ–‡ä»¶ä¿å­˜å®Œæˆ")
        print(f"     æ–‡ä»¶ ID: {file_id}")
        print(f"     å­˜å„²è·¯å¾‘: {file_path}")
        print()
        
        # 3. å‰µå»ºæ–‡ä»¶å…ƒæ•¸æ“š
        print("ğŸ“ å‰µå»ºæ–‡ä»¶å…ƒæ•¸æ“š...")
        metadata_service = get_metadata_service()
        
        from services.api.models.file_metadata import FileMetadataCreate
        
        file_metadata = FileMetadataCreate(
            file_id=file_id,
            filename=test_file.name,
            file_type="application/pdf",
            file_size=len(file_content),
            user_id=system_user_id,
            storage_path=file_path,
            status="uploaded"
        )
        
        created_metadata = metadata_service.create(file_metadata)
        print(f"  âœ… å…ƒæ•¸æ“šå‰µå»ºå®Œæˆ")
        print()
        
        # 4. åˆå§‹åŒ–è™•ç†ç‹€æ…‹
        print("âš™ï¸  åˆå§‹åŒ–è™•ç†ç‹€æ…‹...")
        upload_status_service = get_upload_status_service()
        upload_status_service.update_upload_progress(
            file_id=file_id,
            progress=0,
            status="uploading",
            message="æ–‡ä»¶ä¸Šå‚³å®Œæˆï¼Œé–‹å§‹è™•ç†..."
        )
        print(f"  âœ… è™•ç†ç‹€æ…‹åˆå§‹åŒ–å®Œæˆ")
        print()
        
        # 5. è§¸ç™¼ç•°æ­¥è™•ç†ï¼ˆé€™è£¡éœ€è¦é€šé RQ éšŠåˆ—ï¼‰
        print("ğŸš€ è§¸ç™¼ç•°æ­¥è™•ç†...")
        print("   æ³¨æ„ï¼šéœ€è¦é€šé RQ Worker è™•ç†")
        print()
        
        # å˜—è©¦ç›´æ¥èª¿ç”¨è™•ç†å‡½æ•¸ï¼ˆå¦‚æœæ˜¯åŒæ­¥çš„ï¼‰
        # å¦å‰‡éœ€è¦ä½¿ç”¨ RQ éšŠåˆ—
        
        from workers.service import get_rq_queue
        from api.routers.file_upload import process_file_chunking_and_vectorization
        
        queue = get_rq_queue('file_processing')
        
        # ç²å–æ–‡ä»¶é¡å‹
        parser_factory = get_parser_factory()
        file_type = "application/pdf"
        
        # æäº¤ä»»å‹™åˆ°éšŠåˆ—
        job = queue.enqueue(
            process_file_chunking_and_vectorization,
            file_id=file_id,
            file_path=file_path,
            file_type=file_type,
            user_id=system_user_id,
            job_timeout='1h'
        )
        
        print(f"  âœ… ä»»å‹™å·²æäº¤åˆ°éšŠåˆ—")
        print(f"     ä»»å‹™ ID: {job.id}")
        print()
        
        print("=" * 80)
        print("âœ… æ–‡ä»¶ä¸Šå‚³å’Œä»»å‹™æäº¤å®Œæˆï¼")
        print("=" * 80)
        print(f"æ–‡ä»¶ ID: {file_id}")
        print()
        print("â³ æ–‡ä»¶æ­£åœ¨å¾Œå°è™•ç†ï¼ˆåˆ†å¡Šã€å‘é‡åŒ–ï¼‰...")
        print("   å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥è©¢ç‹€æ…‹ï¼š")
        print(f"   GET /api/v1/files/{file_id}/processing-status")
        print()
        print(f"FILE_ID={file_id}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(upload_and_process_file())

