# WBS-階段二A：文件分塊與向量化

**文檔功能說明**: 文件分塊與向量化處理實施工作分解結構
**創建日期**: 2025-01-27
**創建人**: Daniel Chung
**最後修改日期**: 2025-01-27

## 1. 工作分解結構

### 1.1 文件分塊處理 (2-3天)

#### 1.1.1 文件解析增強 (1天)

- [x] **任務 1.1.1.1**: 檢查現有文件解析器
  - 位置: `services/api/processors/parsers/`
  - 功能: 確認支持的格式和功能
  - 預計時間: 2小時

- [ ] **任務 1.1.1.2**: 增強PDF解析器
  - 支持表格提取
  - 支持圖片OCR（可選）
  - 預計時間: 4小時

- [ ] **任務 1.1.1.3**: 增強DOCX解析器
  - 支持樣式保留
  - 支持表格提取
  - 預計時間: 3小時

- [ ] **任務 1.1.1.4**: 實現Markdown解析器優化
  - 支持代碼塊保留
  - 支持數學公式（可選）
  - 預計時間: 2小時

#### 1.1.2 智能分塊策略 (1-2天)

- [ ] **任務 1.1.2.1**: 實現語義分塊
  - 位置: `genai/api/routers/chunk_processing.py`
  - 功能: 根據文意進行分塊，而非簡單按字符數
  - 預計時間: 6小時

- [ ] **任務 1.1.2.2**: 整合Ollama模型進行文意分析
  - 使用 本地 Ollama 服務 (localhost:11434) 的模型分析段落語義
  - 確定最佳分塊點
  - 預計時間: 8小時

- [ ] **任務 1.1.2.3**: 實現分塊重疊策略
  - 功能: 相鄰chunk之間保留一定重疊，確保語義連續性
  - 重疊比例: 10-20%
  - 預計時間: 4小時

- [ ] **任務 1.1.2.4**: 實現分塊元數據提取
  - 提取: 標題、段落序號、頁碼等
  - 預計時間: 3小時

### 1.2 文意分析集成 (2-3天)

#### 1.2.1 Ollama模型集成 (1天)

- [ ] **任務 1.2.1.1**: 配置Ollama客戶端
  - 位置: `llm/clients/ollama.py`
  - 功能: 連接到本地 Ollama 服務 (localhost:11434，使用環境變數 OLLAMA_REMOTE_HOST 和 OLLAMA_REMOTE_PORT 配置)
  - 預計時間: 2小時

- [ ] **任務 1.2.1.2**: 實現文意分析Prompt
  - 功能: 設計Prompt讓模型分析段落語義邊界
  - 預計時間: 4小時

- [ ] **任務 1.2.1.3**: 實現批量文意分析
  - 功能: 批量處理多個段落，提高效率
  - 預計時間: 4小時

- [ ] **任務 1.2.1.4**: 實現錯誤處理和重試
  - 處理網絡錯誤、超時等
  - 預計時間: 2小時

#### 1.2.2 分塊質量評估 (1天)

- [ ] **任務 1.2.2.1**: 實現分塊質量評分
  - 評估指標: 語義完整性、長度適中、信息密度
  - 預計時間: 4小時

- [ ] **任務 1.2.2.2**: 實現分塊優化
  - 功能: 根據評分自動調整分塊策略
  - 預計時間: 4小時

### 1.3 向量化處理 (2-3天)

#### 1.3.1 Embedding模型集成 (1天)

- [x] **任務 1.3.1.1**: 配置Embedding模型
  - 使用 本地 Ollama 服務 (localhost:11434) 的embedding模型
  - 模型選擇: nomic-embed-text 或類似
  - 預計時間: 2小時

- [x] **任務 1.3.1.2**: 實現批量向量化
  - 功能: 批量處理chunk，提高效率
  - 預計時間: 4小時

- [x] **任務 1.3.1.3**: 實現向量緩存
  - 功能: 避免重複計算相同文本的向量
  - 預計時間: 3小時

- [x] **任務 1.3.1.4**: 實現向量質量驗證
  - 檢查向量維度、NaN值等
  - 預計時間: 2小時

#### 1.3.2 ChromaDB存儲 (1-2天)

- [x] **任務 1.3.2.1**: 設計ChromaDB集合結構
  - 集合命名: 按文件ID或用戶ID組織
  - 元數據設計: file_id, chunk_index, metadata等
  - 預計時間: 2小時

- [x] **任務 1.3.2.2**: 實現批量存儲到ChromaDB
  - 位置: `api/routers/chromadb.py`
  - 功能: 批量添加文檔和向量
  - 預計時間: 4小時

- [x] **任務 1.3.2.3**: 實現存儲錯誤處理
  - 處理連接失敗、數據格式錯誤等
  - 預計時間: 3小時

- [x] **任務 1.3.2.4**: 實現存儲進度追蹤
  - 追蹤存儲進度，更新處理狀態
  - 預計時間: 3小時

### 1.4 LangChain/LangGraph集成 (2-3天)

#### 1.4.1 LangChain集成 (1-2天)

- [ ] **任務 1.4.1.1**: 檢查現有LangChain集成
  - 位置: `genai/workflows/langchain/`
  - 功能: 確認現有功能
  - 預計時間: 2小時

- [ ] **任務 1.4.1.2**: 實現RAG Chain
  - 功能: 使用LangChain構建RAG檢索鏈
  - 預計時間: 6小時

- [ ] **任務 1.4.1.3**: 實現向量檢索器
  - 功能: 使用ChromaDB作為向量存儲的檢索器
  - 預計時間: 4小時

- [ ] **任務 1.4.1.4**: 實現檢索結果重排序
  - 功能: 使用重排序模型優化檢索結果
  - 預計時間: 4小時

#### 1.4.2 LangGraph集成 (1-2天)

- [ ] **任務 1.4.2.1**: 設計RAG處理流程圖
  - 節點: 文件解析、分塊、向量化、存儲
  - 邊: 數據流和控制流
  - 預計時間: 4小時

- [ ] **任務 1.4.2.2**: 實現LangGraph工作流
  - 位置: `genai/workflows/rag/`
  - 功能: 使用LangGraph編排RAG處理流程
  - 預計時間: 8小時

- [ ] **任務 1.4.2.3**: 實現錯誤恢復機制
  - 功能: 處理節點失敗，實現重試和回滾
  - 預計時間: 4小時

### 1.5 模型使用追蹤集成 (1天)

#### 1.5.1 模型調用記錄 (0.5天)

- [ ] **任務 1.5.1.1**: 在向量化處理中添加模型使用記錄
  - 位置: `api/routers/chromadb.py`
  - 功能: 記錄embedding模型調用
  - 預計時間: 2小時

- [ ] **任務 1.5.1.2**: 在文意分析中添加模型使用記錄
  - 功能: 記錄用於文意分析的模型調用
  - 預計時間: 2小時

#### 1.5.2 使用統計 (0.5天)

- [ ] **任務 1.5.2.1**: 實現模型使用統計
  - 功能: 統計模型調用次數、輸入輸出長度
  - 預計時間: 3小時

- [ ] **任務 1.5.2.2**: 實現使用報告
  - 功能: 生成模型使用報告
  - 預計時間: 1小時

## 2. 技術規格

### 2.1 分塊參數

- **最小chunk大小**: 100字符
- **最大chunk大小**: 2000字符
- **理想chunk大小**: 500-1000字符
- **重疊比例**: 10-20%

### 2.2 向量化參數

- **Embedding模型**: nomic-embed-text (或類似)
- **向量維度**: 768 (根據模型)
- **批量大小**: 32-64 chunks/批次

### 2.3 ChromaDB配置

- **集合命名規則**: `rag_{file_id}` 或 `rag_{user_id}`
- **元數據字段**:
  - `file_id`: 文件ID
  - `chunk_index`: Chunk序號
  - `chunk_text`: 原始文本
  - `metadata`: 其他元數據（標題、頁碼等）

### 2.4 API端點

#### 觸發文件處理

```
POST /api/files/{file_id}/process

Response:
{
  "success": true,
  "data": {
    "file_id": "uuid",
    "task_id": "task_uuid",
    "status": "processing"
  }
}
```

#### 查詢處理狀態

```
GET /api/files/{file_id}/process/status

Response:
{
  "success": true,
  "data": {
    "file_id": "uuid",
    "status": "processing|completed|failed",
    "progress": {
      "chunking": 100,
      "vectorization": 75,
      "storage": 50
    },
    "chunks_count": 25,
    "vectors_count": 19
  }
}
```

## 3. 驗收標準

### 3.1 功能驗收

- ✅ 文件成功解析為文本
- ✅ 文本按語義正確分塊
- ✅ 每個chunk包含完整的語義單元
- ✅ Chunk之間有適當重疊
- ✅ 向量成功生成
- ✅ 向量成功存儲到ChromaDB
- ✅ 可以從ChromaDB檢索相關chunk

### 3.2 性能驗收

- 文件解析時間 < 文件大小 × 0.05秒/MB
- 分塊處理時間 < 文本長度 × 0.01秒/1000字符
- 向量化時間 < chunk數量 × 0.1秒/chunk
- 存儲時間 < chunk數量 × 0.05秒/chunk

### 3.3 質量驗收

- Chunk語義完整性 > 90%
- 向量相似度檢索準確率 > 85%
- 處理錯誤率 < 1%

## 4. 測試計劃

### 4.1 單元測試

- 文件解析器測試
- 分塊策略測試
- 向量化測試
- ChromaDB存儲測試

### 4.2 集成測試

- 端到端處理流程測試
- LangChain集成測試
- LangGraph工作流測試

### 4.3 性能測試

- 大文件處理測試（> 10MB）
- 批量文件處理測試
- 並發處理測試

## 5. 依賴關係

### 5.1 前置任務

- 階段一：文件上傳功能完成

### 5.2 後續任務

- 階段二B：可以並行進行，但需要文件ID

### 5.3 外部依賴

- Ollama服務 (本地 Ollama 服務 (localhost:11434)) 可用
- ChromaDB數據庫運行正常
- LangChain/LangGraph庫安裝

## 6. 風險與緩解

### 6.1 技術風險

- **風險**: Ollama服務不穩定或響應慢
- **緩解**: 實現重試機制、超時處理、降級策略

- **風險**: 大文件向量化耗時過長
- **緩解**: 實現異步處理、進度追蹤、批量優化

- **風險**: ChromaDB存儲失敗
- **緩解**: 實現重試、數據驗證、備份機制

### 6.2 性能風險

- **風險**: 並發處理導致資源耗盡
- **緩解**: 實現任務隊列、資源限制、優先級管理

## 7. 交付物

- [ ] 增強的文件解析器
- [ ] 智能分塊處理邏輯
- [ ] Ollama文意分析集成
- [x] 向量化處理服務
- [x] ChromaDB存儲集成
- [ ] LangChain RAG Chain
- [ ] LangGraph工作流
- [x] 處理狀態API
- [ ] 單元測試代碼
- [ ] 集成測試代碼
- [ ] 技術文檔

## 8. 時間估算

| 任務 | 預計時間 | 實際時間 | 狀態 |
|------|---------|---------|------|
| 文件分塊處理 | 2-3天 | - | ⏸️ 待開始 |
| 文意分析集成 | 2-3天 | - | ⏸️ 待開始 |
| 向量化處理 | 2-3天 | - | ⏸️ 待開始 |
| LangChain/LangGraph集成 | 2-3天 |
| 模型使用追蹤集成 | 1天 | - | ⏸️ 待開始 |
| **總計** | **9-13天** | | - | - |

## 9. 更新日誌

| 日期 | 版本 | 更新內容 | 更新人 |
|------|------|---------|--------|
| 2025-01-27 | 1.0 | 初始版本創建 | Daniel Chung |

## 10. Ollama 服務配置說明

### 10.1 本機服務配置（推薦）

為了確保數據安全和性能，**強烈建議使用本機 Ollama 服務**：

**環境變數配置**（`.env` 文件）：

```bash
# Ollama 本機服務配置
OLLAMA_REMOTE_HOST=localhost
OLLAMA_REMOTE_PORT=11434
OLLAMA_SCHEME=http
OLLAMA_DEFAULT_MODEL=gpt-oss:20b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

**配置文件方式**（`config/config.json`）：

```json
{
  "services": {
    "ollama": {
      "scheme": "http",
      "host": "localhost",
      "port": 11434,
      "default_model": "gpt-oss:20b",
      "embedding_model": "nomic-embed-text",
      "request_timeout": 60
    }
  }
}
```

### 10.2 安全優勢

使用本機服務的優勢：

- ✅ **數據安全**: 敏感文件內容不經過外部網路，避免中間人攻擊
- ✅ **性能優異**: 無網路延遲，響應更快（localhost 通信）
- ✅ **配置簡單**: 無需 SSL 證書和複雜網路配置
- ✅ **可靠性高**: 不受外部網路波動影響，不依賴 DNS 解析

### 10.3 驗證配置

配置完成後，可以通過以下方式驗證：

```bash
# 檢查 Ollama 服務是否運行
curl http://localhost:11434/api/tags

# 檢查模型列表
ollama list
```

預期看到您本機安裝的模型：

- mistral-nemo:12b
- llama3.1:8b
- gpt-oss:20b
- qwen3-coder:30b
