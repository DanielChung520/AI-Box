# GenAI-llm-MoE配置說明

**文檔版本**: v2.0  
**創建日期**: 2026-01-24  
**最後更新**: 2026-01-24  
**適用場景**: 前端 GenAI 聊天 LLM 配置

---

## 📋 目錄

- [1. 概述](#1-概述)
- [2. LLM MoE 配置場景](#2-llm-moe-配置場景)
- [3. 前端模型選擇清單](#3-前端模型選擇清單)
- [4. 後端路由策略](#4-後端路由策略)
- [5. 配置維護方式](#5-配置維護方式)
- [6. 技術架構](#6-技術架構)
- [7. 代碼審閱和改進計劃](#7-代碼審閱和改進計劃)
- [8. 故障排除](#8-故障排除)

---

## 1. 概述

### 1.1 目的

本配置說明文檔針對 **AI-Box 系統前端 GenAI 聊天功能** 的 LLM 模型配置，實現以下目標：

- **簡化用戶體驗**：提供簡潔的模型選擇清單，避免用戶選擇困擾
- **品牌統一形象**：使用 SmartQ 品牌替代敏感性模型名稱
- **兩岸友好策略**：避免涉及地區敏感的模型提供商名稱
- **性能最優化**：通過 MoE (Mixture of Experts) 技術智能路由到最佳模型
- **成本控制**：根據場景和負載動態選擇最經濟的模型

### 1.2 核心特點

| 特點 | 說明 | 實現方式 |
|------|------|----------|
| **簡化選擇** | 前端只顯示 6 個模型選項 | 前端配置 + 後端路由 |
| **智能路由** | SmartQ-HCI 自動選擇最佳後端模型 | MoE 策略配置 |
| **品牌統一** | 使用 SmartQ-HCI 替代具體模型 | JSON 配置映射 |
| **國際大廠** | 支持主流國際模型 | ArangoDB 配置管理 |
| **本地優化** | 支持本地 Ollama 模型 | Ollama API 集成 |

---

## 2. LLM MoE 配置場景

### 2.1 主要使用場景

AI-Box 系統將 LLM 使用分為以下場景，每個場景有不同的模型優化策略：

#### 🎯 **Chat (聊天對話)**

**用途**: 前端 AI 聊天、用戶對話交互

**特點**:
- 實時性要求高，延遲需 < 3 秒
- 對話上下文連續性重要
- 支持多輪對話記憶

**優化策略**:
- 優先選擇響應速度快的模型
- 溫度參數較高 (0.7) 增加自然感
- 支持 Function Calling 增強功能

**推薦模型順序**:
1. `gpt-oss:120b-cloud` (高性能)
2. `glm-4.7:cloud` (高質量)
3. `qwen3-next:latest` (平衡性)

#### 🧠 **Semantic Understanding (語義理解)**

**用途**: 文檔理解、知識提取、語義分析

**特點**:
- 精確性要求高
- 批次處理能力重要
- 複雜推理能力需要

**優化策略**:
- 優先選擇理解能力強的模型
- 溫度參數較低 (0.3) 提高精確度
- 支持長文本處理

**推薦模型順序**:
1. `glm-4.7:cloud` (128K 上下文)
2. `gpt-oss:120b-cloud` (高理解)
3. `llama3.2:latest` (本地優化)

#### 📝 **Task Analysis (任務分析)**

**用途**: 任務分解、流程規劃、複雜任務處理

**特點**:
- 邏輯推理能力重要
- 結構化輸出需求
- 代碼生成能力支持

**優化策略**:
- 優先選擇編程和邏輯能力強的模型
- 中等溫度 (0.3-0.4) 平衡創造性和精確度
- 支持結構化輸出

**推薦模型順序**:
1. `qwen3-coder:30b` (代碼專優化)
2. `gpt-oss:120b-cloud` (通用能力)
3. `glm-4.7:cloud` (推理能力)

#### 🎬 **Orchestrator (協調器)**

**用途**: Agent 任務協調、資源分配、流程控制

**特點**:
- 極低延遲要求 (< 1 秒)
- 簡潔指令輸出
- 高可靠性要求

**優化策略**:
- 使用最快速的模型
- 最低溫度 (0.2) 確保穩定性
- 最大並發支持

**推薦模型順序**:
1. `gpt-oss:120b-cloud` (最快響應)
2. `llama3.2:latest` (本地最低延遲)
3. `glm-4.6:cloud` (雲端備選)

#### 🔤 **Embedding (向量化)**

**用途**: 文本向量化、語義搜索、知識圖譜

**特點**:
- 批次處理效率
- 向量維度一致性
- 資源佔用低

**優化策略**:
- 使用專用嵌入模型
- 標準化向量維度
- 批次處理優化

**推薦模型**:
1. `nomic-embed-text:latest` (Ollama 本地)
2. `text-embedding-v1` (Qwen 雲端)

#### 🌐 **Knowledge Graph Extraction (知識圖譜提取)**

**用途**: 實體識別、關係抽取、知識圖譜構建

**特點**:
- 結構化提取能力
- 高精確度要求
- 支持複雜關係類型

**優化策略**:
- 使用高推理能力模型
- 低溫度 (0.2) 確保精確
- 長時間處理支持

**推薦模型順序**:
1. `gpt-oss:120b-cloud` (高推理)
2. `glm-4.7:cloud` (長上下文)
3. `qwen3-coder:30b` (結構化能力)

### 2.2 MoE 路由策略

#### 🧪 **Weighted Scoring (加權評分)**

**應用場景**: Chat 場景 (SmartQ-HCI 默認)

**評分公式**:
```
Score = (Performance × 0.4) + (Cost × 0.3) + (Availability × 0.3)
```

**評分指標**:
- **Performance** (性能): 響應時間、輸出質量
- **Cost** (成本): 每 1K tokens 花費
- **Availability** (可用性): 過去 1 小時成功率

**選擇邏輯**:
1. 計算所有候選模型的加權得分
2. 選擇得分最高的模型
3. 錯誤時自動故障轉移到次優模型

#### 🚀 **Latency Based (延遲優先)**

**應用場景**: Orchestrator、高實時性需求

**選擇邏輯**:
1. 監控各模型的平均響應時間
2. 優先選擇延遲最低的模型
3. 設置延遲閾值 (5 秒)，超過自動切換

#### 💰 **Cost Based (成本優化)**

**應用場景**: 批次處理、長文本生成

**選擇邏輯**:
1. 按每 1K tokens 成本排序
2. 優先選擇最便宜的模型
3. 質量低於閾值時自動切換到高級模型

---

## 3. 前端模型選擇清單

### 3.1 預期模型清單

前端 AI 聊天介面的模型選擇器將顯示以下 6 個選項：

| # | 模型 ID | 顯示名稱 | 圖標 | 顏色 | 說明 | 類別 |
|---|----------|----------|------|------|------|------|
| 1 | `auto` | `Auto` | `fa-magic` | 紫色 | 自動選擇最佳模型 | 自動 |
| 2 | `chatgpt-latest` | `ChatGPT` | `fa-robot` | 綠色 | OpenAI 最新 ChatGPT 模型 | 國際大廠 |
| 3 | `gemini-latest` | `Gemini` | `fa-gem` | 藍色 | Google 最新 Gemini 模型 | 國際大廠 |
| 4 | `grok-latest` | `Grok` | `fa-bolt` | 黃色 | xAI Grok 模型 | 國際大廠 |
| 5 | `smartq-hci` | `SmartQ-HCI` | `fa-microchip` | 橘色 | 智能融合模型 (Human-Computer Interface) | SmartQ 品牌 |
| 6 | `ollama-local` | `Ollama` | `fa-server` | 紫色 | 本地部署模型 | 本地模型 |

### 3.2 模型對照表

#### 🔮 **Auto (自動選擇)**

- **用途**: 系統自動根據場景選擇最佳模型
- **實現**: 使用 MoE 路由策略
- **優點**: 零配置、最優性能
- **推薦**: 默認選擇，適合大多數用戶

#### 🤖 **ChatGPT (OpenAI)**

- **前端模型 ID**: `chatgpt-latest`
- **後端實際模型**: 根據配置動態選擇最新 GPT 模型
- **推薦場景**: 聊天對話、創意生成
- **API 配置**: 在系統設置頁面維護

**後端候選模型**:
- `gpt-4-turbo` (高性能)
- `gpt-4o` (平衡性)
- `gpt-4o-mini` (成本優化)

#### 💎 **Gemini (Google)**

- **前端模型 ID**: `gemini-latest`
- **後端實際模型**: 根據配置動態選擇最新 Gemini 模型
- **推薦場景**: 多模態處理、長文本理解
- **API 配置**: 在系統設置頁面維護

**後端候選模型**:
- `gemini-2.0-flash` (快速響應)
- `gemini-2.5-pro` (高質量)
- `gemini-2.5-flash-lite` (成本優化)

#### ⚡ **Grok (xAI)**

- **前端模型 ID**: `grok-latest`
- **後端實際模型**: 根據配置動態選擇最新 Grok 模型
- **推薦場景**: 實時信息、新聞摘要
- **API 配置**: 在系統設置頁面維護

**後端候選模型**:
- `grok-2` (最新版本)
- `grok-beta` (測試版本)

#### 🧠 **SmartQ-HCI (智能融合模型)**

- **前端模型 ID**: `smartq-hci`
- **後端實現**: MoE 智能路由
- **核心特點**: 自動選擇最佳後端模型，用戶無感知
- **推薦場景**: 通用聊天、智能對話

**後端路由策略**:
```
SmartQ-HCI 選擇邏輯:
├── 場景: Chat
│   ├── 主要: gpt-oss:120b-cloud (40% 權重)
│   ├── 主要: glm-4.7:cloud (30% 權重)
│   ├── 主要: qwen3-next:latest (30% 權重)
│   └── 備用: llama3.2:latest, glm-4.6:cloud
│
├── 場景: Semantic Understanding
│   ├── 主要: glm-4.7:cloud (50% 權重)
│   ├── 主要: gpt-oss:120b-cloud (50% 權重)
│   └── 備用: qwen3-coder:30b, llama3.2:latest
│
└── 故障轉移: 自動切換到可用模型
```

#### 🏠 **Ollama (本地模型)**

- **前端模型 ID**: `ollama-local`
- **後端實現**: 動態獲取本地 Ollama 模型
- **核心特點**: 零網絡延遲、數據隱私
- **推薦場景**: 敏感數據處理、離線環境

**推薦本地模型**:
- `gpt-oss:120b-cloud` (本地高質量)
- `llama3.2:latest` (開源優質)
- `mistral-nemo:12b` (輕量高效)

### 3.3 移除的功能

#### ❌ **收藏功能**

**移除原因**:
- 簡化用戶介面，避免選擇困擾
- SmartQ-HCI 自動選擇最佳模型，收藏意義不大
- 系統自動根據使用模式優化

**替代方案**:
- `Auto` 選項提供自動最優選擇
- SmartQ-HCI 自學習用戶偏好
- 系統級別的模型優化

---

## 4. 後端路由策略

### 4.1 MoE 管理器架構

```
┌─────────────────────────────────────────────────────────────┐
│                   AI-Box MoE 系統                      │
└─────────────────────────────────────────────────────────────┘
                         │
        ┌────────────────┼────────────────┐
        │                │                │
   ┌────▼────┐    ┌─────▼──────┐   ┌────▼─────┐
   │ SmartQ- │    │ International│   │  Ollama  │
   │  HCI     │    │  Providers  │   │  Models   │
   └────┬────┘    └─────┬──────┘   └────┬─────┘
        │                │                │
        └────────────────┼────────────────┘
                         │
                   ┌─────▼─────┐
                   │ MoE Router │
                   └─────┬─────┘
                         │
         ┌───────────────┼───────────────┐
         │               │               │
    ┌────▼────┐   ┌─────▼──────┐  ┌────▼────┐
    │ ChatGPT  │   │  Gemini    │  │  Grok   │
    └─────────┘   └────────────┘  └─────────┘
```

### 4.2 路由決策樹

```
前端選擇 SmartQ-HCI
        │
        ├─ 用戶場景: Chat
        │  ├─ 高性能需求 → gpt-oss:120b-cloud
        │  ├─ 高質量需求 → glm-4.7:cloud
        │  ├─ 成本敏感 → qwen3-next:latest
        │  └─ 模型故障 → 自動切換到備用模型
        │
        ├─ 用戶場景: Semantic Understanding
        │  ├─ 長文本 → glm-4.7:cloud (128K 上下文)
        │  ├─ 短文本 → gpt-oss:120b-cloud
        │  └─ 模型故障 → qwen3-coder:30b
        │
        └─ 其他場景
           ├─ 任務分析 → qwen3-coder:30b
           ├─ 協調器 → gpt-oss:120b-cloud
           └─ 向量化 → nomic-embed-text:latest
```

### 4.3 故障轉移機制

#### 🔄 **自動故障轉移**

**觸發條件**:
- API 連接超時 (> 30 秒)
- API 返回錯誤 (4xx, 5xx)
- 模型無響應或質量異常
- 速率限制或配額不足

**轉移策略**:
```
主要模型 (Primary)
    ↓ 失敗
故障檢測 (錯誤 > 2 次)
    ↓
切換到備用模型 (Fallback)
    ↓ 成功
記錄成功模型，下次優先使用
```

**重試邏輯**:
1. 首次失敗: 立即重試
2. 第二次失敗: 切換到次優模型
3. 連續失敗 3 次: 標記模型為不可用
4. 10 分鐘後: 重新檢查模型可用性

### 4.4 性能監控

#### 📊 **監控指標**

| 指標 | 說明 | 閾值 | 警告級別 |
|------|------|--------|----------|
| 響應時間 | 平均 API 響應時間 | > 5 秒 | ⚠️ 警告 |
| 錯誤率 | 請求失敗率 | > 10% | 🔴 嚴重 |
| 可用性 | 模型可用時間比例 | < 95% | 🔴 嚴重 |
| 成本 | 每 1K tokens 花費 | 變動 > 50% | ⚠️ 警告 |

#### 🔄 **自動優化**

系統將根據監控數據自動調整路由策略：

1. **性能學習**: 記錄各模型在不同場景的表現
2. **成本優化**: 在質量可接受範圍內選擇更便宜的模型
3. **負載均衡**: 根據 API 限額動態分配請求

---

## 5. 配置維護方式

### 5.1 國際大廠配置維護

#### 🖥️ **系統設置頁面**

**訪問路徑**: `https://iee.k84.org/#/admin/settings`

**配置內容**:
- API Key 管理和驗證
- 模型列表和版本更新
- 基礎 URL 和端點配置
- 限額和成本監控

**數據存儲**: ArangoDB (加密存儲)

#### 🔑 **API Key 管理**

**存儲方式**:
```
ArangoDB 集合: llm_provider_configs
文檔結構:
{
  "_key": "openai_config",
  "provider": "openai",
  "api_key": "<加密後的 key>",
  "base_url": "https://api.openai.com/v1",
  "last_updated": "2026-01-24T10:00:00Z",
  "health_status": "healthy",
  "models": [
    {
      "model_id": "gpt-4",
      "is_available": true,
      "context_size": 128000
    }
  ]
}
```

**加密方式**: AES-256 加密
**訪問權限**: 僅系統管理員可查看和修改

#### 📝 **模型版本管理**

**自動更新**:
- 系統定期檢查模型版本更新
- 新版本自動加入可用列表
- 舊版本自動標記為 deprecated

**手動更新**:
1. 在系統設置頁面查看當前模型版本
2. 手動添加/刪除模型
3. 保存後立即生效

### 5.2 SmartQ-HCI 配置維護

#### 📁 **配置文件**

**文件路徑**: `/config/smartq_models_schema.json`

**配置結構**:
```json
{
  "smartq_models_schema": {
    "chat_models": {
      "smartq_hci": {
        "backend_config": {
          "moe_config": {
            "primary_models": [...],
            "fallback_models": [...],
            "selection_criteria": {...}
          }
        }
      }
    }
  }
}
```

**修改方式**:
1. 編輯 JSON 配置文件
2. 重啟服務生效
3. 支持熱重載 (開發模式)

#### ⚙️ **路由策略調整**

**權重調整示例**:
```json
{
  "selection_criteria": {
    "performance_weight": 0.5,    // 提高性能權重
    "cost_weight": 0.2,           // 降低成本權重
    "availability_weight": 0.3      // 維持可用性權重
  }
}
```

**場景映射調整**:
```json
{
  "scene_mapping": {
    "chat": {
      "primary_models": ["新增模型ID"],
      "fallback_models": ["移除模型ID"]
    }
  }
}
```

### 5.3 Ollama 模型管理

#### 🔄 **動態發現**

**發現機制**:
1. 系統定期掃描 Ollama API (`/api/tags`)
2. 新模型自動加入可用列表
3. 模型狀態實時更新

**掃描頻率**: 每 5 分鐘
**故障重試**: 連接失敗後 30 秒重試

#### 🏷️ **模型分類**

**顯示控制**:
- 在配置中指定哪些 Ollama 模型在前端顯示
- 支持按類型分類 (本地/雲端)
- 自動檢測模型能力 (chat, embedding, vision)

---

## 6. 技術架構

### 6.1 系統組件

```
┌─────────────────────────────────────────────────────────────┐
│                   前端架構                             │
├─────────────────────────────────────────────────────────────┤
│  AI 聊天介面                                          │
│  ┌────────────────────────────────────────────────────┐   │
│  │ 模型選擇器                                   │   │
│  │ - Auto                                       │   │
│  │ - ChatGPT                                    │   │
│  │ - Gemini                                     │   │
│  │ - Grok                                       │   │
│  │ - SmartQ-HCI                                 │   │
│  │ - Ollama                                     │   │
│  └────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                         │
                    API 請求
                         │
┌─────────────────────────────────────────────────────────────┐
│                   後端架構                             │
├─────────────────────────────────────────────────────────────┤
│                                                     │
│  ┌──────────────────────────────────────────────────┐    │
│  │  API Gateway (FastAPI)                        │    │
│  │  ┌─────────────────────────────────────────┐   │    │
│  │  │ /api/v1/models (模型列表 API)          │   │    │
│  │  │ /api/v1/chat (聊天 API)                │   │    │
│  │  └─────────────────────────────────────────┘   │    │
│  └──────────────────────────────────────────────────┘    │
│                         │                                  │
│  ┌──────────────────────────────────────────────────┐    │
│  │  SimplifiedModelService                      │    │
│  │  - 前端模型到後端模型的映射                │    │
│  │  - SmartQ-HCI 路由配置                    │    │
│  └──────────────────────────────────────────────────┘    │
│                         │                                  │
│  ┌──────────────────────────────────────────────────┐    │
│  │  LLMMoEManager                             │    │
│  │  - 場景路由                                │    │
│  │  - 故障轉移                                │    │
│  │  - 性能監控                                │    │
│  └──────────────────────────────────────────────────┘    │
│                         │                                  │
│  ┌─────────────────┬────────────────┬──────────────┐    │
│  │                 │                │              │    │
│  ▼                 ▼                ▼              ▼    │
│ ┌──────┐      ┌──────┐       ┌──────┐     ┌──────┐│
│ │ OpenAI │      │Google │       │ xAI   │     │Ollama││
│ │Client │      │Client │       │Client │     │Client ││
│ └───┬──┘      └───┬──┘       └───┬──┘     └───┬──┘│
│     │              │                │              │    │
│  API             API             API           API  │
└────────────────────────────────────────────────────┘
```

### 6.2 數據流程

#### 💬 **聊天流程**

1. **前端選擇模型**
   ```
   用戶選擇: SmartQ-HCI
   ↓
   前端發送請求: POST /api/v1/chat
   {
     "model": "smartq-hci",
     "messages": [...]
   }
   ```

2. **後端路由決策**
   ```
   API Gateway 接收請求
   ↓
   SimplifiedModelService 映射模型
   smartq-hci → gpt-oss:120b-cloud (當前最優)
   ↓
   LLMMoEManager 選擇實際執行模型
   ```

3. **執行和響應**
   ```
   調用模型 API 請求
   ↓
   返回結果給前端
   {
     "content": "模型生成的回應",
     "model_used": "gpt-oss:120b-cloud",
     "routing_info": {
       "selected_model": "gpt-oss:120b-cloud",
       "fallback_used": false,
       "latency_ms": 1250
     }
   }
   ```

### 6.3 配置優先級

```
最高優先級: 用戶明確選擇 (ChatGPT, Gemini, Grok)
    ↓
次優先級: SmartQ-HCI 智能路由
    ↓
最低優先級: Auto 自動選擇
```

---

## 7. 代碼審閱和改進計劃

### 7.1 現有架構分析

#### 📊 **前端架構 (ChatInput.tsx)**

**優點**:
- ✅ 已經實現從 API 獲取模型列表的完整邏輯
- ✅ 支持 Auto 模型選擇
- ✅ 支持收藏功能
- ✅ 支持模型排序 (按 order 字段)
- ✅ 已有完整的錯誤處理和 fallback 邏輯

**缺點**:
- ❌ 缺少場景感知能力 (不知道當前是 SmartQ-HCI 還是通用場景)
- ❌ 無法根據場景動態過濾/排序模型
- ❌ 模型列表過於龐大 (limit=1000)，前端渲染性能問題
- ❌ 缺少場景特定的 UI 提示和引導

#### 🔧 **後端 API 層**

**優點**:
- ✅ 統一的 API 請求封裝
- ✅ 支持多種查詢參數
- ✅ 完整的錯誤處理

**缺點**:
- ❌ 缺少場景參數傳遞機制
- ❌ 無法獲取場景特定的模型列表

#### 🌐 **後端 API 路由 (llm_models.py)**

**優點**:
- ✅ 支持動態發現 Ollama 模型
- ✅ 支持收藏狀態標記
- ✅ 支持多種篩選條件
- ✅ 完整的 CRUD 操作

**缺點**:
- ❌ 缺少場景特定模型列表端點
- ❌ 無法根據場景動態過濾模型
- ❌ 模型優先級未與場景關聯

#### 🧠 **MoE 管理器**

**優點**:
- ✅ 完善的場景路由機制
- ✅ 支持模型優先級列表
- ✅ 支持用戶偏好
- ✅ 支持環境變量覆蓋

**缺點**:
- ❌ SmartQ-HCI 場景未在配置中定義
- ❌ 前端無法直接訪問場景優先級列表

#### 🎯 **場景路由**

**優點**:
- ✅ 清晰的場景配置結構
- ✅ 支持模型優先級配置
- ✅ 支持用戶默認模型

**缺點**:
- ❌ 缺少 SmartQ-HCI 場景定義
- ❌ 缺少場景的前端可編輯標記
- ❌ 無法向 API 暴露場景優先級列表

#### 💾 **模型服務**

**優點**:
- ✅ 完整的 CRUD 操作
- ✅ 動態發現 Ollama 模型
- ✅ 支持收藏和 Active 狀態標記

**缺點**:
- ❌ 缺少場景特定查詢方法
- ❌ order 字段未與場景關聯
- ❌ 性能優化不足 (返回所有模型)

### 7.2 改進建議

#### 🎯 **配置層改進**

**1. 修改 config.json 添加 SmartQ-HCI 場景配置**

```json
{
  "services": {
    "moe": {
      "model_priority": {
        "chat": {
          "frontend_editable": true,
          "user_default": "gpt-4",
          "priority": [
            {
              "model": "gpt-4",
              "order": 1,
              "context_size": 8192,
              "max_tokens": 4096,
              "temperature": 0.7,
              "timeout": 60,
              "retries": 3,
              "rpm": 30,
              "concurrency": 5
            }
          ]
        },
        "smartq_hci": {
          "frontend_editable": false,
          "user_default": "gpt-4o-mini",
          "priority": [
            {
              "model": "gpt-4o-mini",
              "order": 1,
              "context_size": 128000,
              "max_tokens": 4096,
              "temperature": 0.7,
              "timeout": 30,
              "retries": 3,
              "rpm": 100,
              "concurrency": 10,
              "cost_per_1k_input": 0.00015,
              "cost_per_1k_output": 0.0006,
              "icon": "fa-robot",
              "color": "text-green-500"
            },
            {
              "model": "gpt-4-turbo",
              "order": 2,
              "context_size": 128000,
              "max_tokens": 4096,
              "temperature": 0.7,
              "timeout": 30,
              "retries": 3,
              "rpm": 30,
              "concurrency": 5
            }
          ]
        }
      },
      "features": {
        "user_preference_enabled": true,
        "auto_fallback_enabled": true,
        "scene_aware_model_list_enabled": true
      }
    }
  }
}
```

#### 🔌 **後端改進**

**1. 新增 API 端點：`/api/v1/models/scene/{scene_name}`**

在 `api/routers/llm_models.py` 中新增場景特定模型列表端點，根據場景返回優先級排序的模型列表。

**2. 新增場景列表端點：`/api/v1/models/scenes`**

獲取所有可用場景列表，包括場景的配置信息。

#### 💻 **前端改進**

**1. 修改 api.ts 添加場景 API**

添加場景特定模型列表的 API 調用函數。

**2. 修改 ChatInput.tsx 支持場景感知**

添加場景狀態管理和場景特定的模型獲取邏輯。

**3. 添加場景提示 UI**

為 SmartQ-HCI 模式添加友好的 UI 提示，說明已為用戶優化模型選擇。

#### 🗄️ **數據庫設計建議**

**1. 優化 ArangoDB 索引**

添加場景相關的索引以提高查詢性能。

**2. 可選：創建模擬場景映射集合**

為場景和模型的映射創建專用集合，支持複雜的場景配置。

### 7.3 實現步驟總結

#### Phase 1: 配置和後端 (優先級：高)

1. ✅ 修改 `config/config.json` 添加 SmartQ-HCI 場景配置
2. ✅ 在 `api/routers/llm_models.py` 新增場景模型列表端點
3. ✅ 在 `api/routers/llm_models.py` 新增場景列表端點
4. ✅ 測試後端 API

#### Phase 2: 前端 (優先級：高)

5. ✅ 在 `ai-bot/src/lib/api.ts` 添加場景 API 調用函數
6. ✅ 修改 `ai-bot/src/components/ChatInput.tsx` 支持場景感知
7. ✅ 添加場景提示 UI
8. ✅ 測試前端功能

#### Phase 3: 優化和文檔 (優先級：中)

9. ✅ 性能優化：添加緩存機制
10. ✅ 添加測試覆蓋
11. ✅ 更新 API 文檔
12. ✅ 添加用戶指南

### 7.4 風險評估

| 風險 | 影響 | 概率 | 緩解措施 |
|------|------|------|----------|
| 配置文件格式錯誤 | 高 | 中 | 添加配置驗證和默認值 fallback |
| API 性能問題 | 中 | 中 | 添加緩存機制和查詢優化 |
| 前端兼容性問題 | 中 | 低 | 添加向後兼容的 fallback 邏輯 |
| 數據庫查詢性能 | 中 | 低 | 優化索引和查詢語句 |

### 7.5 回滾計劃

如果新功能出現問題：
1. 保留原有的通用模型列表 API
2. 前端可以通過配置禁用場景感知功能
3. 使用 A/B 測試逐步推出新功能

---

## 8. 故障排除

### 8.1 常見問題

#### ❌ **問題: SmartQ-HCI 無響應**

**可能原因**:
- 所有後端模型都不可用
- MoE 配置錯誤
- 網絡連接問題

**解決方案**:
1. 檢查 `/api/v1/moe/scenes` API 狀態
2. 驗證 `smartq_models_schema.json` 配置
3. 查看日誌: `tail -f logs/moe_manager.log`

#### ⚠️ **問題: 模型切換頻繁**

**可能原因**:
- 主要模型不穩定
- 閾值設置過低
- 負載過高

**解決方案**:
1. 提高故障切換閾值 (改為 3 次失敗才切換)
2. 檢查模型 API 狀態和限額
3. 優化 `selection_criteria` 權重

#### 🐛 **問題: 響應時間過長**

**可能原因**:
- 選擇了慢速模型
- 網絡延遲
- 模型限流

**解決方案**:
1. 使用 `Latency Based` 路由策略
2. 優先選擇本地 Ollama 模型
3. 調整超時設置

### 8.2 日誌監控

#### 📊 **關鍵日誌**

**MoE 路由日誌**:
```bash
# 查看路由決策
grep "moe_routing_decision" logs/moe_manager.log

# 查看故障轉移
grep "model_fallback" logs/moe_manager.log

# 查看性能數據
grep "model_performance" logs/moe_manager.log
```

**API 日誌**:
```bash
# 查看模型 API 調用
grep "model_api_call" logs/api.log

# 查看錯誤
grep "model_api_error" logs/api.log
```

#### 🔍 **調試模式**

**啟用詳細日誌**:
```bash
# 設置環境變數
export MOE_LOG_LEVEL=DEBUG
export API_LOG_LEVEL=DEBUG

# 重啟服務
python -m uvicorn api.main:app --reload
```

### 8.3 性能優化

#### 🚀 **優化建議**

1. **前端優化**:
   - 緩存模型列表 (減少 API 調用)
   - 懶步載入模型狀態
   - 使用 Web Worker 處理路由決策

2. **後端優化**:
   - 使用連接池減少建立開銷
   - 實施請求批處理
   - 緩存路由決策結果

3. **模型優化**:
   - 優先使用本地 Ollama 模型
   - 實施預熱機制
   - 動態調整並發數

---

## 9. 總結

### 🎯 **設計原則**

1. **用戶優先**: 簡化選擇，智能路由
2. **品牌統一**: SmartQ 替代敏感性名稱
3. **性能最優**: MoE 技術確保最佳性能
4. **成本控制**: 智能選擇經濟模型
5. **兩岸友好**: 避免地區敏感問題

### ✅ **實現效果**

- ✅ **簡化用戶體驗**: 從數十個模型減少到 6 個選項
- ✅ **統一品牌形象**: SmartQ-HCI 成為核心品牌
- ✅ **智能路由**: 系統自動選擇最佳模型
- ✅ **高可用性**: 自動故障轉移確保穩定
- ✅ **成本優化**: 根據場景動態選擇最經濟方案

### 🚀 **未來擴展**

- [ ] 支持自定義 SmartQ 模型配置
- [ ] 機器學習優化路由決策
- [ ] 用戶行為分析和個性化路由
- [ ] 多租戶模型隔離
- [ ] 成本預算和告警

---

**文檔維護**: AI-Box 開發團隊  
**技術支持**: support@ai-box.com  
**更新頻率**: 每季度更新或重大變更時更新
