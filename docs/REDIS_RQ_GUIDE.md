# Redis/RQ ä»»å‹™éšŠåˆ—ç³»çµ±é–‹ç™¼æŒ‡å—

**å‰µå»ºæ—¥æœŸ**: 2025-12-10  
**å‰µå»ºäºº**: Daniel Chung  
**æœ€å¾Œä¿®æ”¹æ—¥æœŸ**: 2025-12-10

## ğŸ“‹ ç›®éŒ„

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [Redis æœå‹™åŠŸèƒ½](#redis-æœå‹™åŠŸèƒ½)
3. [RQ ä»»å‹™éšŠåˆ—æ¶æ§‹](#rq-ä»»å‹™éšŠåˆ—æ¶æ§‹)
4. [ç¾æœ‰ä»£ç¢¼åŠŸèƒ½](#ç¾æœ‰ä»£ç¢¼åŠŸèƒ½)
5. [ä»»å‹™æäº¤æŒ‡å—](#ä»»å‹™æäº¤æŒ‡å—)
6. [ä»»å‹™æŸ¥è©¢æŒ‡å—](#ä»»å‹™æŸ¥è©¢æŒ‡å—)
7. [é–‹ç™¼æŒ‡å—](#é–‹ç™¼æŒ‡å—)
8. [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)
9. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## æ¦‚è¿°

### Redis èˆ‡ RQ çš„é—œä¿‚

**é‡è¦èªªæ˜**ï¼š
- **Redis** æ˜¯ç¨ç«‹çš„æœå‹™ï¼Œç”¨æ–¼æ•¸æ“šå­˜å„²å’Œç·©å­˜
- **RQ (Redis Queue)** æ˜¯ Python åº«ï¼Œ**ä½¿ç”¨ç¾æœ‰çš„ Redis æœå‹™**ä¾†å­˜å„²ä»»å‹™éšŠåˆ—
- **RQ ä¸æ˜¯ç¨ç«‹æœå‹™**ï¼Œå®ƒé€šé Redis ä¾†ç®¡ç†ä»»å‹™éšŠåˆ—
- **RQ Worker** æ˜¯ç¨ç«‹é€²ç¨‹ï¼Œå¾ Redis æ‹‰å–ä»»å‹™ä¸¦åŸ·è¡Œ

### æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI API    â”‚
â”‚     Server      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (æäº¤ä»»å‹™)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RQ Queue      â”‚  â† Python åº«
â”‚   (å®¢æˆ¶ç«¯)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (å­˜å„²ä»»å‹™)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Redis       â”‚  â† ç¾æœ‰æœå‹™
â”‚   (å­˜å„²å±¤)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (Worker æ‹‰å–)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RQ Worker      â”‚  â† ç¨ç«‹é€²ç¨‹
â”‚   (åŸ·è¡Œä»»å‹™)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Redis æœå‹™åŠŸèƒ½

### 1. æ–‡ä»¶è™•ç†ç‹€æ…‹è¿½è¹¤

**ç”¨é€”**: è¿½è¹¤æ–‡ä»¶ä¸Šå‚³å’Œè™•ç†çš„å¯¦æ™‚é€²åº¦

**Key æ ¼å¼**:
- `upload:progress:{file_id}` - æ–‡ä»¶ä¸Šå‚³é€²åº¦ï¼ˆTTL: 1å°æ™‚ï¼‰
- `processing:status:{file_id}` - æ–‡ä»¶è™•ç†ç‹€æ…‹ï¼ˆTTL: 2å°æ™‚ï¼‰

**æ•¸æ“šçµæ§‹**:
```json
{
  "file_id": "xxx",
  "status": "processing",
  "progress": 50,
  "chunking": {"status": "completed", "progress": 100},
  "vectorization": {"status": "processing", "progress": 50},
  "storage": {"status": "pending", "progress": 0},
  "kg_extraction": {"status": "pending", "progress": 0},
  "message": "æ­£åœ¨è™•ç†..."
}
```

**ä½¿ç”¨ä½ç½®**:
- `api/routers/file_upload.py` - `_update_upload_progress()`, `_update_processing_status()`

### 2. JWT Token é»‘åå–®ç®¡ç†

**ç”¨é€”**: ç®¡ç†å·²ç™»å‡ºæˆ–å¤±æ•ˆçš„ JWT Token

**Key æ ¼å¼**:
- `jwt:blacklist:{token_hash}` - Token é»‘åå–®ï¼ˆTTL: èˆ‡ Token éæœŸæ™‚é–“ä¸€è‡´ï¼‰

**ä½¿ç”¨ä½ç½®**:
- `system/security/jwt_service.py` - `add_to_blacklist()`, `is_blacklisted()`

### 3. Agent è¨˜æ†¶ç®¡ç† (AAM)

**ç”¨é€”**: ç‚º Agent æä¾›çŸ­æœŸè¨˜æ†¶å­˜å„²

**Key æ ¼å¼**:
- `aam:memory:{key}` - Agent è¨˜æ†¶æ•¸æ“šï¼ˆTTL: 3600ç§’ï¼Œ1å°æ™‚ï¼‰

**ä½¿ç”¨ä½ç½®**:
- `agents/infra/memory/aam/storage_adapter.py`

### 4. RQ ä»»å‹™éšŠåˆ—å­˜å„²

**ç”¨é€”**: RQ ä½¿ç”¨ Redis å­˜å„²ä»»å‹™éšŠåˆ—æ•¸æ“š

**Key æ ¼å¼**:
- `rq:queue:{queue_name}` - ä»»å‹™éšŠåˆ—
- `rq:job:{job_id}` - ä»»å‹™æ•¸æ“š
- `rq:worker:{worker_name}` - Worker è¨»å†Šä¿¡æ¯

**ä½¿ç”¨ä½ç½®**:
- `database/rq/queue.py` - RQ éšŠåˆ—å®¢æˆ¶ç«¯
- `database/rq/monitor.py` - éšŠåˆ—ç›£æ§å·¥å…·

---

## RQ ä»»å‹™éšŠåˆ—æ¶æ§‹

### éšŠåˆ—å®šç¾©

ç³»çµ±ä¸­å®šç¾©äº†ä¸‰å€‹ä¸»è¦éšŠåˆ—ï¼š

| éšŠåˆ—åç¨± | ç”¨é€” | è™•ç†ä»»å‹™ |
|---------|------|---------|
| `file_processing` | æ–‡ä»¶è™•ç†éšŠåˆ— | åˆ†å¡Š + å‘é‡åŒ– + åœ–è­œæå– |
| `vectorization` | å‘é‡åŒ–å°ˆç”¨éšŠåˆ— | åƒ…å‘é‡åŒ–è™•ç† |
| `kg_extraction` | çŸ¥è­˜åœ–è­œæå–å°ˆç”¨éšŠåˆ— | åƒ…åœ–è­œæå– |

### çµ„ä»¶èªªæ˜

#### 1. éšŠåˆ—å®¢æˆ¶ç«¯ (`database/rq/queue.py`)

**åŠŸèƒ½**:
- æä¾› `get_task_queue()` å‡½æ•¸ç²å–éšŠåˆ—å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
- å°è£ Redis é€£æ¥ç®¡ç†
- æ”¯æŒå¤šå€‹éšŠåˆ—å¯¦ä¾‹

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from database.rq.queue import get_task_queue, FILE_PROCESSING_QUEUE

queue = get_task_queue(FILE_PROCESSING_QUEUE)
```

#### 2. Worker ä»»å‹™å‡½æ•¸ (`workers/tasks.py`)

**åŠŸèƒ½**:
- å®šç¾©æ‰€æœ‰éœ€è¦åœ¨ Worker ä¸­åŸ·è¡Œçš„ä»»å‹™å‡½æ•¸
- è™•ç†ç•°æ­¥å‡½æ•¸çš„åŸ·è¡Œï¼ˆä½¿ç”¨ `asyncio.run()`ï¼‰
- æä¾›éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„

**ç¾æœ‰ä»»å‹™å‡½æ•¸**:
- `process_file_chunking_and_vectorization_task()` - æ–‡ä»¶è™•ç†ä»»å‹™
- `process_vectorization_only_task()` - å‘é‡åŒ–ä»»å‹™
- `process_kg_extraction_only_task()` - åœ–è­œæå–ä»»å‹™

#### 3. ç›£æ§å·¥å…· (`database/rq/monitor.py`)

**åŠŸèƒ½**:
- æŸ¥è©¢æ‰€æœ‰éšŠåˆ—åˆ—è¡¨
- æŸ¥è©¢éšŠåˆ—çµ±è¨ˆä¿¡æ¯
- æŸ¥è©¢ Worker ä¿¡æ¯
- æŸ¥è©¢ä»»å‹™åˆ—è¡¨

#### 4. ç›£æ§ API (`api/routers/rq_monitor.py`)

**åŠŸèƒ½**:
- æä¾› RESTful API æ¥å£æŸ¥è©¢éšŠåˆ—ç‹€æ…‹
- æ”¯æŒèªè­‰å’Œæ¬Šé™æ§åˆ¶

---

## ç¾æœ‰ä»£ç¢¼åŠŸèƒ½

### 1. Redis å®¢æˆ¶ç«¯ (`database/redis/client.py`)

**åŠŸèƒ½**:
- å–®ä¾‹æ¨¡å¼çš„ Redis å®¢æˆ¶ç«¯ç®¡ç†
- è‡ªå‹•é€£æ¥ç®¡ç†å’Œé‡é€£
- æ”¯æŒç’°å¢ƒè®Šæ•¸é…ç½®

**é…ç½®æ–¹å¼**:
```bash
# .env æ–‡ä»¶
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_URL=redis://localhost:6379/0
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from database.redis import get_redis_client

redis_client = get_redis_client()
redis_client.setex("key", 3600, "value")
value = redis_client.get("key")
```

### 2. RQ éšŠåˆ—å®¢æˆ¶ç«¯ (`database/rq/queue.py`)

**åŠŸèƒ½**:
- éšŠåˆ—å¯¦ä¾‹ç®¡ç†ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
- Redis é€£æ¥å°è£
- éšŠåˆ—åç¨±å¸¸é‡å®šç¾©

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from database.rq.queue import get_task_queue, FILE_PROCESSING_QUEUE

queue = get_task_queue(FILE_PROCESSING_QUEUE)
```

### 3. RQ ç›£æ§å·¥å…· (`database/rq/monitor.py`)

**åŠŸèƒ½å‡½æ•¸**:
- `get_all_queues()` - ç²å–æ‰€æœ‰éšŠåˆ—åˆ—è¡¨
- `get_queue_stats(queue_name)` - ç²å–éšŠåˆ—çµ±è¨ˆ
- `get_all_queues_stats()` - ç²å–æ‰€æœ‰éšŠåˆ—çµ±è¨ˆ
- `get_workers_info()` - ç²å– Worker ä¿¡æ¯
- `get_queue_jobs(queue_name, status, limit)` - ç²å–ä»»å‹™åˆ—è¡¨

### 4. Worker å•Ÿå‹•è…³æœ¬ (`scripts/start_rq_worker.sh`)

**åŠŸèƒ½**:
- è‡ªå‹•æª¢æ¸¬è™›æ“¬ç’°å¢ƒ
- æª¢æŸ¥ Redis é€£æ¥
- å•Ÿå‹• RQ Worker é€²ç¨‹
- æ—¥èªŒè¨˜éŒ„

**ä½¿ç”¨æ–¹æ³•**:
```bash
./scripts/start_rq_worker.sh file_processing
```

### 5. ç‹€æ…‹æŸ¥è©¢è…³æœ¬ (`scripts/rq_status.sh`)

**åŠŸèƒ½**:
- æŸ¥è©¢æ‰€æœ‰éšŠåˆ—ç‹€æ…‹
- é¡¯ç¤ºéšŠåˆ—çµ±è¨ˆä¿¡æ¯
- é¡¯ç¤º Worker ä¿¡æ¯

**ä½¿ç”¨æ–¹æ³•**:
```bash
./scripts/rq_status.sh
```

---

## ä»»å‹™æäº¤æŒ‡å—

### åŸºæœ¬ç”¨æ³•

#### 1. å°å…¥å¿…è¦çš„æ¨¡çµ„

```python
from database.rq.queue import get_task_queue, FILE_PROCESSING_QUEUE
from workers.tasks import process_file_chunking_and_vectorization_task
```

#### 2. ç²å–éšŠåˆ—å¯¦ä¾‹

```python
queue = get_task_queue(FILE_PROCESSING_QUEUE)
```

#### 3. æäº¤ä»»å‹™

```python
job = queue.enqueue(
    process_file_chunking_and_vectorization_task,
    file_id=file_id,
    file_path=file_path,
    file_type=file_type,
    user_id=user_id,
)
```

### å®Œæ•´ç¤ºä¾‹

#### æ–‡ä»¶ä¸Šå‚³è™•ç†

```python
from database.rq.queue import get_task_queue, FILE_PROCESSING_QUEUE
from workers.tasks import process_file_chunking_and_vectorization_task

# åœ¨æ–‡ä»¶ä¸Šå‚³è·¯ç”±ä¸­
@router.post("/upload")
async def upload_file(...):
    # ... æ–‡ä»¶ä¸Šå‚³é‚è¼¯ ...
    
    # æäº¤è™•ç†ä»»å‹™åˆ° RQ
    queue = get_task_queue(FILE_PROCESSING_QUEUE)
    job = queue.enqueue(
        process_file_chunking_and_vectorization_task,
        file_id=file_id,
        file_path=file_path,
        file_type=file_type,
        user_id=current_user.user_id,
    )
    
    return APIResponse.success(
        data={
            "file_id": file_id,
            "job_id": job.id,
            "status": "queued",
        },
        message="æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼Œè™•ç†ä»»å‹™å·²æäº¤",
    )
```

#### å‘é‡é‡æ–°ç”Ÿæˆ

```python
from database.rq.queue import get_task_queue, VECTORIZATION_QUEUE
from workers.tasks import process_vectorization_only_task

@router.post("/{file_id}/regenerate")
async def regenerate_vector(...):
    # ... é©—è­‰é‚è¼¯ ...
    
    queue = get_task_queue(VECTORIZATION_QUEUE)
    job = queue.enqueue(
        process_vectorization_only_task,
        file_id=file_id,
        file_path=file_path,
        file_type=file_metadata.file_type,
        user_id=current_user.user_id,
    )
    
    return APIResponse.success(
        data={"job_id": job.id, "status": "queued"},
        message="å‘é‡é‡æ–°ç”Ÿæˆä»»å‹™å·²æäº¤",
    )
```

#### åœ–è­œé‡æ–°ç”Ÿæˆ

```python
from database.rq.queue import get_task_queue, KG_EXTRACTION_QUEUE
from workers.tasks import process_kg_extraction_only_task

@router.post("/{file_id}/regenerate")
async def regenerate_graph(...):
    # ... é©—è­‰é‚è¼¯ ...
    
    queue = get_task_queue(KG_EXTRACTION_QUEUE)
    job = queue.enqueue(
        process_kg_extraction_only_task,
        file_id=file_id,
        file_path=file_path,
        file_type=file_metadata.file_type,
        user_id=current_user.user_id,
        force_rechunk=False,
    )
    
    return APIResponse.success(
        data={"job_id": job.id, "status": "queued"},
        message="åœ–è­œé‡æ–°ç”Ÿæˆä»»å‹™å·²æäº¤",
    )
```

### ä»»å‹™é¸é …

#### ä»»å‹™å„ªå…ˆç´š

```python
from rq import Queue

queue = get_task_queue(FILE_PROCESSING_QUEUE)
job = queue.enqueue(
    task_function,
    arg1, arg2,
    job_timeout=3600,  # ä»»å‹™è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    result_ttl=86400,  # çµæœä¿ç•™æ™‚é–“ï¼ˆç§’ï¼‰
    failure_ttl=86400,  # å¤±æ•—ä»»å‹™ä¿ç•™æ™‚é–“ï¼ˆç§’ï¼‰
)
```

#### å»¶é²åŸ·è¡Œ

```python
from datetime import datetime, timedelta

# å»¶é² 5 åˆ†é˜åŸ·è¡Œ
job = queue.enqueue_in(
    timedelta(minutes=5),
    task_function,
    arg1, arg2,
)
```

#### å®šæ™‚åŸ·è¡Œ

```python
from rq import Queue
from datetime import datetime

# åœ¨æŒ‡å®šæ™‚é–“åŸ·è¡Œ
job = queue.enqueue_at(
    datetime(2025, 12, 10, 22, 0, 0),
    task_function,
    arg1, arg2,
)
```

### ä»»å‹™ç‹€æ…‹è¿½è¹¤

```python
# ç²å–ä»»å‹™ç‹€æ…‹
job = queue.fetch_job(job_id)
status = job.get_status()  # 'queued', 'started', 'finished', 'failed'

# ç²å–ä»»å‹™çµæœ
if job.is_finished:
    result = job.result

# ç²å–ä»»å‹™éŒ¯èª¤ä¿¡æ¯
if job.is_failed:
    error = job.exc_info
```

---

## ä»»å‹™æŸ¥è©¢æŒ‡å—

### 1. å‘½ä»¤è¡ŒæŸ¥è©¢

#### ä½¿ç”¨ç‹€æ…‹æŸ¥è©¢è…³æœ¬

```bash
./scripts/rq_status.sh
```

**è¼¸å‡ºç¤ºä¾‹**:
```
======================================================================
RQ éšŠåˆ—ç‹€æ…‹
======================================================================

ğŸ“‹ æ‰€æœ‰éšŠåˆ—:
----------------------------------------------------------------------
æ‰¾åˆ° 3 å€‹éšŠåˆ—:
  âœ… file_processing
  âœ… vectorization
  âœ… kg_extraction

ğŸ“Š éšŠåˆ—çµ±è¨ˆ:
----------------------------------------------------------------------

  file_processing:
    ç­‰å¾…ä¸­: 5
    åŸ·è¡Œä¸­: 2
    å·²å®Œæˆ: 100
    å¤±æ•—: 1
    ç¸½è¨ˆ: 108

ğŸ‘· Worker ä¿¡æ¯:
----------------------------------------------------------------------
æ‰¾åˆ° 2 å€‹ Worker:
  âœ… rq_worker_file_processing_12345
    ç‹€æ…‹: busy
    éšŠåˆ—: file_processing
    ç•¶å‰ä»»å‹™: abc123-def456-...
```

### 2. API æŸ¥è©¢

#### ç²å–æ‰€æœ‰éšŠåˆ—åˆ—è¡¨

```bash
GET /api/v1/rq/queues
```

**éŸ¿æ‡‰**:
```json
{
  "success": true,
  "data": {
    "queues": ["file_processing", "vectorization", "kg_extraction"],
    "count": 3
  },
  "message": "éšŠåˆ—åˆ—è¡¨ç²å–æˆåŠŸ"
}
```

#### ç²å–æ‰€æœ‰éšŠåˆ—çµ±è¨ˆ

```bash
GET /api/v1/rq/queues/stats
```

**éŸ¿æ‡‰**:
```json
{
  "success": true,
  "data": {
    "queues": {
      "file_processing": {
        "queue_name": "file_processing",
        "queued": 5,
        "started": 2,
        "finished": 100,
        "failed": 1,
        "total": 108
      },
      "vectorization": {
        "queue_name": "vectorization",
        "queued": 0,
        "started": 0,
        "finished": 50,
        "failed": 0,
        "total": 50
      }
    }
  }
}
```

#### ç²å–æŒ‡å®šéšŠåˆ—çµ±è¨ˆ

```bash
GET /api/v1/rq/queues/file_processing/stats
```

#### ç²å–éšŠåˆ—ä»»å‹™åˆ—è¡¨

```bash
GET /api/v1/rq/queues/file_processing/jobs?status=queued&limit=10
```

**åƒæ•¸**:
- `status` (å¯é¸): `queued`, `started`, `finished`, `failed`
- `limit` (å¯é¸): è¿”å›ä»»å‹™æ•¸é‡é™åˆ¶ï¼ˆ1-100ï¼Œé»˜èª 10ï¼‰

#### ç²å– Worker ä¿¡æ¯

```bash
GET /api/v1/rq/workers
```

**éŸ¿æ‡‰**:
```json
{
  "success": true,
  "data": {
    "workers": [
      {
        "name": "rq_worker_file_processing_12345",
        "state": "busy",
        "queues": ["file_processing"],
        "current_job_id": "abc123-def456-...",
        "birth_date": "2025-12-10T20:00:00"
      }
    ],
    "count": 1
  }
}
```

### 3. Python ä»£ç¢¼æŸ¥è©¢

#### æŸ¥è©¢æ‰€æœ‰éšŠåˆ—

```python
from database.rq.monitor import get_all_queues

queues = get_all_queues()
print(f"æ‰¾åˆ° {len(queues)} å€‹éšŠåˆ—: {queues}")
```

#### æŸ¥è©¢éšŠåˆ—çµ±è¨ˆ

```python
from database.rq.monitor import get_queue_stats

stats = get_queue_stats("file_processing")
print(f"ç­‰å¾…ä¸­: {stats['queued']}")
print(f"åŸ·è¡Œä¸­: {stats['started']}")
print(f"å·²å®Œæˆ: {stats['finished']}")
print(f"å¤±æ•—: {stats['failed']}")
```

#### æŸ¥è©¢æ‰€æœ‰éšŠåˆ—çµ±è¨ˆ

```python
from database.rq.monitor import get_all_queues_stats

all_stats = get_all_queues_stats()
for queue_name, stats in all_stats.items():
    print(f"{queue_name}: {stats['total']} å€‹ä»»å‹™")
```

#### æŸ¥è©¢ Worker ä¿¡æ¯

```python
from database.rq.monitor import get_workers_info

workers = get_workers_info()
for worker in workers:
    print(f"{worker['name']}: {worker['state']}")
```

#### æŸ¥è©¢ä»»å‹™åˆ—è¡¨

```python
from database.rq.monitor import get_queue_jobs

# æŸ¥è©¢ç­‰å¾…ä¸­çš„ä»»å‹™
queued_jobs = get_queue_jobs("file_processing", status="queued", limit=10)

# æŸ¥è©¢åŸ·è¡Œä¸­çš„ä»»å‹™
started_jobs = get_queue_jobs("file_processing", status="started", limit=10)

# æŸ¥è©¢å¤±æ•—çš„ä»»å‹™
failed_jobs = get_queue_jobs("file_processing", status="failed", limit=10)
```

---

## é–‹ç™¼æŒ‡å—

### æ·»åŠ æ–°ä»»å‹™é¡å‹

#### æ­¥é©Ÿ 1: åœ¨ `workers/tasks.py` ä¸­æ·»åŠ ä»»å‹™å‡½æ•¸

```python
def my_new_task(
    param1: str,
    param2: int,
) -> dict:
    """
    æ–°ä»»å‹™è™•ç†å‡½æ•¸

    Args:
        param1: åƒæ•¸1
        param2: åƒæ•¸2

    Returns:
        è™•ç†çµæœå­—å…¸
    """
    try:
        # å¦‚æœæ˜¯ç•°æ­¥å‡½æ•¸ï¼Œä½¿ç”¨ asyncio.run()
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                async_task_function(param1, param2)
            )
            return {"success": True, "result": result}
        finally:
            loop.close()
    except Exception as e:
        logger.error("Task failed", error=str(e))
        return {"success": False, "error": str(e)}
```

#### æ­¥é©Ÿ 2: åœ¨è·¯ç”±ä¸­æäº¤ä»»å‹™

```python
from database.rq.queue import get_task_queue
from workers.tasks import my_new_task

@router.post("/my-endpoint")
async def my_endpoint(...):
    queue = get_task_queue("my_queue")
    job = queue.enqueue(
        my_new_task,
        param1=value1,
        param2=value2,
    )
    return APIResponse.success(data={"job_id": job.id})
```

#### æ­¥é©Ÿ 3: å•Ÿå‹•å°æ‡‰çš„ Worker

```bash
./scripts/start_rq_worker.sh my_queue
```

### å‰µå»ºæ–°éšŠåˆ—

#### æ­¥é©Ÿ 1: åœ¨ `database/rq/queue.py` ä¸­å®šç¾©éšŠåˆ—å¸¸é‡

```python
MY_NEW_QUEUE = "my_new_queue"  # æ–°éšŠåˆ—åç¨±
```

#### æ­¥é©Ÿ 2: åœ¨ `database/rq/__init__.py` ä¸­å°å‡º

```python
from database.rq.queue import MY_NEW_QUEUE

__all__ = [
    # ... å…¶ä»–å°å‡º ...
    "MY_NEW_QUEUE",
]
```

#### æ­¥é©Ÿ 3: ä½¿ç”¨æ–°éšŠåˆ—

```python
from database.rq.queue import get_task_queue, MY_NEW_QUEUE

queue = get_task_queue(MY_NEW_QUEUE)
```

### è™•ç†ç•°æ­¥ä»»å‹™

**é‡è¦**: RQ Worker æ˜¯åŒæ­¥çš„ï¼Œå¦‚æœä»»å‹™å‡½æ•¸æ˜¯ç•°æ­¥çš„ï¼Œéœ€è¦åœ¨ Worker ä»»å‹™å‡½æ•¸ä¸­ä½¿ç”¨ `asyncio.run()`:

```python
def async_task_wrapper(param1: str) -> dict:
    """ç•°æ­¥ä»»å‹™åŒ…è£å‡½æ•¸"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                async_function(param1)
            )
            return {"success": True, "result": result}
        finally:
            loop.close()
    except Exception as e:
        return {"success": False, "error": str(e)}
```

### ä»»å‹™éŒ¯èª¤è™•ç†

#### åœ¨ä»»å‹™å‡½æ•¸ä¸­è™•ç†éŒ¯èª¤

```python
def my_task(param: str) -> dict:
    try:
        # ä»»å‹™é‚è¼¯
        result = do_something(param)
        return {"success": True, "result": result}
    except ValueError as e:
        logger.error("Validation error", error=str(e))
        return {"success": False, "error": f"Validation failed: {e}"}
    except Exception as e:
        logger.error("Task failed", error=str(e))
        raise  # é‡æ–°æ‹‹å‡ºç•°å¸¸ï¼Œè®“ RQ è¨˜éŒ„ç‚ºå¤±æ•—ä»»å‹™
```

#### æŸ¥è©¢å¤±æ•—ä»»å‹™

```python
from database.rq.monitor import get_queue_jobs

failed_jobs = get_queue_jobs("file_processing", status="failed", limit=10)
for job_info in failed_jobs:
    print(f"ä»»å‹™ {job_info['job_id']} å¤±æ•—: {job_info.get('exc_info')}")
```

### ä»»å‹™é‡è©¦

#### ä½¿ç”¨ RQ çš„é‡è©¦æ©Ÿåˆ¶

```python
from rq import Retry

job = queue.enqueue(
    task_function,
    arg1, arg2,
    retry=Retry(max=3, interval=60),  # æœ€å¤šé‡è©¦3æ¬¡ï¼Œé–“éš”60ç§’
)
```

#### æ‰‹å‹•é‡è©¦å¤±æ•—ä»»å‹™

```python
from database.rq.queue import get_task_queue

queue = get_task_queue("file_processing")
failed_jobs = get_queue_jobs("file_processing", status="failed")

for job_info in failed_jobs:
    job = queue.fetch_job(job_info['job_id'])
    if job:
        job.requeue()  # é‡æ–°åŠ å…¥éšŠåˆ—
```

---

## æœ€ä½³å¯¦è¸

### 1. ä»»å‹™è¨­è¨ˆåŸå‰‡

#### âœ… æ¨è–¦åšæ³•

- **ä»»å‹™å‡½æ•¸æ‡‰è©²æ˜¯ç´”å‡½æ•¸**: ç›¡é‡é¿å…å‰¯ä½œç”¨ï¼Œæ˜“æ–¼æ¸¬è©¦å’Œèª¿è©¦
- **åƒæ•¸æ‡‰è©²æ˜¯å¯åºåˆ—åŒ–çš„**: ä½¿ç”¨åŸºæœ¬é¡å‹ï¼ˆstr, int, dict, listï¼‰
- **ä»»å‹™æ‡‰è©²æœ‰æ˜ç¢ºçš„è¼¸å…¥å’Œè¼¸å‡º**: è¿”å›çµæ§‹åŒ–çš„çµæœå­—å…¸
- **è™•ç†ç•°æ­¥å‡½æ•¸**: åœ¨ Worker ä»»å‹™å‡½æ•¸ä¸­ä½¿ç”¨ `asyncio.run()`

#### âŒ é¿å…åšæ³•

- **ä¸è¦å‚³éä¸å¯åºåˆ—åŒ–çš„å°è±¡**: å¦‚æ–‡ä»¶å¥æŸ„ã€æ•¸æ“šåº«é€£æ¥ç­‰
- **ä¸è¦ä½¿ç”¨å…¨å±€ç‹€æ…‹**: ä»»å‹™æ‡‰è©²æ˜¯ç„¡ç‹€æ…‹çš„
- **ä¸è¦åœ¨ä»»å‹™ä¸­é€²è¡Œé•·æ™‚é–“é˜»å¡**: ä½¿ç”¨ç•°æ­¥æ“ä½œ

### 2. éšŠåˆ—é¸æ“‡ç­–ç•¥

| ä»»å‹™é¡å‹ | æ¨è–¦éšŠåˆ— | åŸå›  |
|---------|---------|------|
| æ–‡ä»¶ä¸Šå‚³å¾Œçš„å®Œæ•´è™•ç† | `file_processing` | åŒ…å«åˆ†å¡Šã€å‘é‡åŒ–ã€åœ–è­œæå– |
| åƒ…å‘é‡åŒ–è™•ç† | `vectorization` | å°ˆç”¨éšŠåˆ—ï¼Œè³‡æºéš”é›¢ |
| åƒ…åœ–è­œæå– | `kg_extraction` | å°ˆç”¨éšŠåˆ—ï¼Œè³‡æºéš”é›¢ |
| æ‰¹é‡è™•ç† | å‰µå»ºå°ˆç”¨éšŠåˆ— | é¿å…å½±éŸ¿å¯¦æ™‚ä»»å‹™ |

### 3. éŒ¯èª¤è™•ç†ç­–ç•¥

#### ä»»å‹™ç´šåˆ¥éŒ¯èª¤è™•ç†

```python
def robust_task(param: str) -> dict:
    """å¥å£¯çš„ä»»å‹™å‡½æ•¸"""
    try:
        # ä¸»è¦é‚è¼¯
        result = process(param)
        return {"success": True, "result": result}
    except RecoverableError as e:
        # å¯æ¢å¾©çš„éŒ¯èª¤ï¼Œè¨˜éŒ„ä½†ä¸æ‹‹å‡º
        logger.warning("Recoverable error", error=str(e))
        return {"success": False, "error": str(e), "recoverable": True}
    except Exception as e:
        # ä¸å¯æ¢å¾©çš„éŒ¯èª¤ï¼Œæ‹‹å‡ºè®“ RQ è¨˜éŒ„
        logger.error("Fatal error", error=str(e))
        raise
```

#### ç›£æ§å¤±æ•—ä»»å‹™

```python
# å®šæœŸæª¢æŸ¥å¤±æ•—ä»»å‹™
from database.rq.monitor import get_queue_jobs

failed_jobs = get_queue_jobs("file_processing", status="failed", limit=100)
if failed_jobs:
    logger.warning(f"ç™¼ç¾ {len(failed_jobs)} å€‹å¤±æ•—ä»»å‹™")
    # ç™¼é€å‘Šè­¦æˆ–è‡ªå‹•é‡è©¦
```

### 4. æ€§èƒ½å„ªåŒ–

#### Worker é€²ç¨‹æ•¸é‡

```bash
# å•Ÿå‹•å¤šå€‹ Workerï¼ˆä¸åŒçµ‚ç«¯ï¼‰
./scripts/start_rq_worker.sh file_processing
./scripts/start_rq_worker.sh file_processing
./scripts/start_rq_worker.sh vectorization
```

#### ä»»å‹™å„ªå…ˆç´š

```python
# ä½¿ç”¨ä¸åŒçš„éšŠåˆ—å¯¦ç¾å„ªå…ˆç´š
high_priority_queue = get_task_queue("high_priority")
normal_queue = get_task_queue("file_processing")

# é«˜å„ªå…ˆç´šä»»å‹™
high_priority_queue.enqueue(urgent_task, ...)

# æ™®é€šä»»å‹™
normal_queue.enqueue(normal_task, ...)
```

### 5. ç›£æ§å’Œæ—¥èªŒ

#### ä»»å‹™æ—¥èªŒ

```python
import structlog

logger = structlog.get_logger(__name__)

def my_task(param: str) -> dict:
    logger.info("Task started", param=param)
    try:
        result = process(param)
        logger.info("Task completed", result=result)
        return {"success": True, "result": result}
    except Exception as e:
        logger.error("Task failed", error=str(e))
        raise
```

#### ç›£æ§æŒ‡æ¨™

- **éšŠåˆ—é•·åº¦**: ç›£æ§ç­‰å¾…ä¸­çš„ä»»å‹™æ•¸é‡
- **Worker ç‹€æ…‹**: ç›£æ§ Worker æ˜¯å¦é‹è¡Œ
- **å¤±æ•—ç‡**: ç›£æ§ä»»å‹™å¤±æ•—æ¯”ä¾‹
- **è™•ç†æ™‚é–“**: ç›£æ§ä»»å‹™å¹³å‡è™•ç†æ™‚é–“

---

## æ•…éšœæ’æŸ¥

### å¸¸è¦‹å•é¡Œ

#### 1. Redis é€£æ¥å¤±æ•—

**ç—‡ç‹€**:
```
RuntimeError: Failed to connect to Redis: Connection refused
```

**è§£æ±ºæ–¹æ³•**:
1. æª¢æŸ¥ Redis æœå‹™æ˜¯å¦é‹è¡Œ: `docker ps | grep redis`
2. æª¢æŸ¥ Redis é…ç½®: `.env` æ–‡ä»¶ä¸­çš„ `REDIS_HOST` å’Œ `REDIS_PORT`
3. æ¸¬è©¦é€£æ¥: `redis-cli -h localhost -p 6379 ping`

#### 2. Worker ç„¡æ³•å•Ÿå‹•

**ç—‡ç‹€**:
```
éŒ¯èª¤: ç„¡æ³•é€£æ¥åˆ° Redis
```

**è§£æ±ºæ–¹æ³•**:
1. ç¢ºä¿ Redis æœå‹™æ­£åœ¨é‹è¡Œ
2. æª¢æŸ¥ `.env` æ–‡ä»¶é…ç½®
3. æŸ¥çœ‹ Worker æ—¥èªŒ: `tail -f logs/rq_worker_*.log`

#### 3. ä»»å‹™ä¸€ç›´è™•æ–¼ queued ç‹€æ…‹

**ç—‡ç‹€**:
- ä»»å‹™æäº¤æˆåŠŸï¼Œä½†ä¸€ç›´ä¸åŸ·è¡Œ

**è§£æ±ºæ–¹æ³•**:
1. æª¢æŸ¥æ˜¯å¦æœ‰ Worker é‹è¡Œ: `./scripts/rq_status.sh`
2. æª¢æŸ¥ Worker æ˜¯å¦ç›£è½æ­£ç¢ºçš„éšŠåˆ—
3. æŸ¥çœ‹ Worker æ—¥èªŒæ˜¯å¦æœ‰éŒ¯èª¤

#### 4. ä»»å‹™åŸ·è¡Œå¤±æ•—

**ç—‡ç‹€**:
- ä»»å‹™ç‹€æ…‹ç‚º `failed`

**è§£æ±ºæ–¹æ³•**:
1. æŸ¥è©¢å¤±æ•—ä»»å‹™: `GET /api/v1/rq/queues/{queue_name}/jobs?status=failed`
2. æŸ¥çœ‹ä»»å‹™éŒ¯èª¤ä¿¡æ¯: `job.exc_info`
3. æª¢æŸ¥ä»»å‹™å‡½æ•¸é‚è¼¯å’Œåƒæ•¸

#### 5. ç•°æ­¥å‡½æ•¸åŸ·è¡Œå•é¡Œ

**ç—‡ç‹€**:
```
RuntimeError: This event loop is already running
```

**è§£æ±ºæ–¹æ³•**:
- ç¢ºä¿åœ¨ Worker ä»»å‹™å‡½æ•¸ä¸­ä½¿ç”¨ `asyncio.new_event_loop()` å’Œ `loop.run_until_complete()`

### èª¿è©¦æŠ€å·§

#### 1. æŸ¥çœ‹éšŠåˆ—ç‹€æ…‹

```bash
# å‘½ä»¤è¡ŒæŸ¥è©¢
./scripts/rq_status.sh

# æˆ–ä½¿ç”¨ Python
python3 -c "
from database.rq.monitor import get_all_queues_stats
import json
print(json.dumps(get_all_queues_stats(), indent=2))
"
```

#### 2. æŸ¥çœ‹ Worker æ—¥èªŒ

```bash
tail -f logs/rq_worker_file_processing.log
```

#### 3. æŸ¥çœ‹ä»»å‹™è©³æƒ…

```python
from database.rq.queue import get_task_queue

queue = get_task_queue("file_processing")
job = queue.fetch_job("job_id")
print(f"ç‹€æ…‹: {job.get_status()}")
print(f"çµæœ: {job.result}")
print(f"éŒ¯èª¤: {job.exc_info}")
```

#### 4. æ¸¬è©¦ä»»å‹™å‡½æ•¸

```python
# ç›´æ¥èª¿ç”¨ä»»å‹™å‡½æ•¸æ¸¬è©¦
from workers.tasks import process_file_chunking_and_vectorization_task

result = process_file_chunking_and_vectorization_task(
    file_id="test",
    file_path="/path/to/file",
    file_type="text/plain",
    user_id="test_user",
)
print(result)
```

---

## å¾ŒçºŒé–‹ç™¼å»ºè­°

### çŸ­æœŸæ”¹é€²ï¼ˆ1-2å‘¨ï¼‰

1. **æ›¿æ› BackgroundTasks ç‚º RQ**
   - ä¿®æ”¹ `api/routers/file_upload.py`
   - ä¿®æ”¹ `api/routers/file_management.py`
   - æ¸¬è©¦ä»»å‹™æäº¤å’ŒåŸ·è¡Œ

2. **å¯¦ç¾ä»»å‹™å„ªå…ˆç´š**
   - æ·»åŠ å„ªå…ˆç´šéšŠåˆ—
   - å¯¦ç¾ç”¨æˆ¶é…é¡ç®¡ç†

3. **å¢å¼·ç›£æ§**
   - æ·»åŠ ä»»å‹™è™•ç†æ™‚é–“çµ±è¨ˆ
   - å¯¦ç¾å¤±æ•—ä»»å‹™è‡ªå‹•å‘Šè­¦

### ä¸­æœŸæ”¹é€²ï¼ˆ1-2æœˆï¼‰

1. **è³‡æºç®¡ç†**
   - å¯¦ç¾ Worker è³‡æºé™åˆ¶ï¼ˆCPUã€å…§å­˜ï¼‰
   - æ·»åŠ ä»»å‹™è¶…æ™‚ç®¡ç†

2. **ä»»å‹™èª¿åº¦å„ªåŒ–**
   - å¯¦ç¾å…¬å¹³èª¿åº¦ç®—æ³•
   - æ·»åŠ ä»»å‹™å»é‡æ©Ÿåˆ¶

3. **å¯è¦–åŒ–ç›£æ§**
   - é›†æˆ RQ Dashboard
   - å‰µå»ºè‡ªå®šç¾©ç›£æ§é¢æ¿

### é•·æœŸè¦åŠƒï¼ˆ3-6æœˆï¼‰

1. **æ··åˆæ¶æ§‹**
   - æ”¯æŒç”¨æˆ¶æœ¬åœ° Workerï¼ˆå¯é¸ï¼‰
   - å¯¦ç¾ä»»å‹™è·¯ç”±ç­–ç•¥

2. **é«˜ç´šåŠŸèƒ½**
   - ä»»å‹™ä¾è³´ç®¡ç†
   - ä»»å‹™éˆï¼ˆChainï¼‰
   - ä»»å‹™çµ„ï¼ˆGroupï¼‰

3. **æ“´å±•æ€§**
   - æ”¯æŒå¤š Redis å¯¦ä¾‹
   - å¯¦ç¾ä»»å‹™åˆ†ç‰‡
   - æ”¯æŒä»»å‹™é·ç§»

---

## åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡æª”

- **RQ å®˜æ–¹æ–‡æª”**: https://python-rq.org/
- **RQ Dashboard**: https://github.com/nvie/rq-dashboard
- **Redis å®˜æ–¹æ–‡æª”**: https://redis.io/docs/

### é …ç›®ç›¸é—œæ–‡ä»¶

- `database/redis/client.py` - Redis å®¢æˆ¶ç«¯å¯¦ç¾
- `database/rq/queue.py` - RQ éšŠåˆ—å®¢æˆ¶ç«¯
- `database/rq/monitor.py` - éšŠåˆ—ç›£æ§å·¥å…·
- `workers/tasks.py` - Worker ä»»å‹™å‡½æ•¸
- `api/routers/rq_monitor.py` - ç›£æ§ API
- `scripts/start_rq_worker.sh` - Worker å•Ÿå‹•è…³æœ¬
- `scripts/rq_status.sh` - ç‹€æ…‹æŸ¥è©¢è…³æœ¬

### ç›¸é—œé…ç½®

- `.env` - Redis é€£æ¥é…ç½®
- `docker-compose.yml` - Redis æœå‹™é…ç½®

---

## é™„éŒ„

### A. Redis Key å‘½åè¦ç¯„

| ç”¨é€” | Key æ ¼å¼ | TTL | èªªæ˜ |
|------|---------|-----|------|
| ä¸Šå‚³é€²åº¦ | `upload:progress:{file_id}` | 1å°æ™‚ | æ–‡ä»¶ä¸Šå‚³é€²åº¦ |
| è™•ç†ç‹€æ…‹ | `processing:status:{file_id}` | 2å°æ™‚ | æ–‡ä»¶è™•ç†ç‹€æ…‹ |
| JWT é»‘åå–® | `jwt:blacklist:{token_hash}` | èˆ‡ Token ä¸€è‡´ | Token é»‘åå–® |
| Agent è¨˜æ†¶ | `aam:memory:{key}` | 1å°æ™‚ | Agent çŸ­æœŸè¨˜æ†¶ |
| RQ éšŠåˆ— | `rq:queue:{queue_name}` | æ°¸ä¹… | RQ ä»»å‹™éšŠåˆ— |
| RQ ä»»å‹™ | `rq:job:{job_id}` | æ ¹æ“šé…ç½® | RQ ä»»å‹™æ•¸æ“š |
| RQ Worker | `rq:worker:{worker_name}` | æ°¸ä¹… | Worker è¨»å†Šä¿¡æ¯ |

### B. ä»»å‹™ç‹€æ…‹èªªæ˜

| ç‹€æ…‹ | èªªæ˜ | å¯æ“ä½œ |
|------|------|--------|
| `queued` | ä»»å‹™å·²åŠ å…¥éšŠåˆ—ï¼Œç­‰å¾…åŸ·è¡Œ | å¯å–æ¶ˆ |
| `started` | ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­ | ç„¡ |
| `finished` | ä»»å‹™åŸ·è¡ŒæˆåŠŸ | å¯æŸ¥çœ‹çµæœ |
| `failed` | ä»»å‹™åŸ·è¡Œå¤±æ•— | å¯é‡è©¦ |
| `deferred` | ä»»å‹™å»¶é²åŸ·è¡Œ | å¯å–æ¶ˆ |
| `scheduled` | ä»»å‹™å·²å®‰æ’åŸ·è¡Œ | å¯å–æ¶ˆ |

### C. ç’°å¢ƒè®Šæ•¸é…ç½®

```bash
# Redis é…ç½®
REDIS_HOST=localhost          # Redis ä¸»æ©Ÿ
REDIS_PORT=6379               # Redis ç«¯å£
REDIS_DB=0                    # Redis æ•¸æ“šåº«ç·¨è™Ÿ
REDIS_PASSWORD=               # Redis å¯†ç¢¼ï¼ˆå¯é¸ï¼‰
REDIS_URL=redis://localhost:6379/0  # Redis é€£æ¥ URLï¼ˆå„ªå…ˆï¼‰
```

---

**æ–‡æª”ç‰ˆæœ¬**: 1.0  
**æœ€å¾Œæ›´æ–°**: 2025-12-10  
**ç¶­è­·è€…**: Daniel Chung
