"""
ä»£ç¢¼åŠŸèƒ½èªªæ˜: ç”¢å“ç´š Chat API è·¯ç”±ï¼ˆ/api/v1/chatï¼‰ï¼Œä¸²æ¥ MoE Auto/Manual/Favorite èˆ‡æœ€å°è§€æ¸¬æ¬„ä½
å‰µå»ºæ—¥æœŸ: 2025-12-13 17:28:02 (UTC+8)
å‰µå»ºäºº: Daniel Chung
æœ€å¾Œä¿®æ”¹æ—¥æœŸ: 2025-01-27
"""

from __future__ import annotations

import asyncio
import json
import re
import time
import uuid
from datetime import datetime
from pathlib import Path
from threading import Lock
from typing import Any, AsyncGenerator, Dict, List, Optional

import structlog
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Request, status
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field

from agents.task_analyzer.classifier import TaskClassifier
from agents.task_analyzer.models import LLMProvider
from api.core.response import APIResponse
from database.arangodb import ArangoDBClient
from genai.workflows.context.manager import ContextManager
from llm.moe.moe_manager import LLMMoEManager
from services.api.models.chat import ChatRequest, ChatResponse, ObservabilityInfo, RoutingInfo
from services.api.models.doc_edit_request import DocEditRequestRecord, DocEditStatus
from services.api.models.file_metadata import FileMetadataCreate
from services.api.models.genai_request import (
    GenAIChatRequestCreateResponse,
    GenAIChatRequestRecord,
    GenAIChatRequestStateResponse,
    GenAIRequestStatus,
)
from services.api.services.chat_memory_service import get_chat_memory_service
from services.api.services.data_consent_service import get_consent_service
from services.api.services.doc_edit_request_store_service import get_doc_edit_request_store_service
from services.api.services.doc_patch_service import detect_doc_format
from services.api.services.file_metadata_service import FileMetadataService
from services.api.services.file_permission_service import FilePermissionService
from services.api.services.genai_chat_request_store_service import (
    get_genai_chat_request_store_service,
)
from services.api.services.genai_config_resolver_service import get_genai_config_resolver_service
from services.api.services.genai_metrics_service import get_genai_metrics_service
from services.api.services.genai_model_registry_service import get_genai_model_registry_service
from services.api.services.genai_policy_gate_service import get_genai_policy_gate_service
from services.api.services.genai_trace_store_service import (
    GenAITraceEvent,
    get_genai_trace_store_service,
)
from storage.file_storage import FileStorage, create_storage_from_config
from system.infra.config.config import get_config_section
from system.security.dependencies import get_current_tenant_id, get_current_user
from system.security.models import Permission, User

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/chat", tags=["Chat"])

_moe_manager: Optional[LLMMoEManager] = None
_task_classifier: Optional[TaskClassifier] = None
_context_manager: Optional[ContextManager] = None
_file_permission_service: Optional[FilePermissionService] = None
_storage: Optional[FileStorage] = None
_metadata_service: Optional[FileMetadataService] = None
_arango_client: Optional[ArangoDBClient] = None
_request_tasks: Dict[str, asyncio.Task[None]] = {}
_request_tasks_lock = Lock()

# hybrid MVPï¼šæ”¶è—æ¨¡å‹å…ˆä»¥ localStorage å¯ç”¨ç‚ºä¸»ï¼›å¾Œç«¯æä¾› Redis å„ªå…ˆã€fallback memory çš„åŒæ­¥æ¥å£
_favorite_models_by_user: Dict[str, List[str]] = {}


def get_moe_manager() -> LLMMoEManager:
    global _moe_manager
    if _moe_manager is None:
        _moe_manager = LLMMoEManager()
    return _moe_manager


def get_task_classifier() -> TaskClassifier:
    global _task_classifier
    if _task_classifier is None:
        _task_classifier = TaskClassifier()
    return _task_classifier


def get_context_manager() -> ContextManager:
    """
    Context å–®ä¾‹å…¥å£ï¼ˆG3ï¼‰ã€‚

    - recorder: Redis å„ªå…ˆã€memory fallbackï¼ˆç”± ContextRecorder å…§éƒ¨è™•ç†ï¼‰
    - window: ContextWindowï¼ˆç”± ContextManager å…§éƒ¨è™•ç†ï¼‰
    """
    global _context_manager
    if _context_manager is None:
        _context_manager = ContextManager()
    return _context_manager


def get_storage() -> FileStorage:
    global _storage
    if _storage is None:
        config = get_config_section("file_upload", default={}) or {}
        _storage = create_storage_from_config(config)
    return _storage


def get_metadata_service() -> FileMetadataService:
    global _metadata_service
    if _metadata_service is None:
        _metadata_service = FileMetadataService()
    return _metadata_service


def get_arango_client() -> ArangoDBClient:
    global _arango_client
    if _arango_client is None:
        _arango_client = ArangoDBClient()
    return _arango_client


def get_file_permission_service() -> FilePermissionService:
    global _file_permission_service
    if _file_permission_service is None:
        _file_permission_service = FilePermissionService()
    return _file_permission_service


_FOLDER_COLLECTION_NAME = "folder_metadata"


def _looks_like_create_file_intent(text: str) -> bool:
    """
    Chat è¼¸å…¥æ¡†ï¼šåˆ¤æ–·æ˜¯å¦æœ‰ã€Œæ–°å¢/è¼¸å‡ºæˆæª”æ¡ˆã€æ„åœ–ï¼ˆMVP ä»¥ heuristic ç‚ºä¸»ï¼‰ã€‚
    """
    t = (text or "").strip()
    if not t:
        return False

    # æ˜ç¢ºè¦æ±‚ï¼šæ–°å¢æª”æ¡ˆ / å»ºç«‹æª”æ¡ˆ / ç”¢ç”Ÿæª”æ¡ˆ / å¦å­˜ç­‰
    keywords = [
        "æ–°å¢æª”æ¡ˆ",
        "å»ºç«‹æª”æ¡ˆ",
        "ç”¢ç”Ÿæª”æ¡ˆ",
        "ç”Ÿæˆæª”æ¡ˆ",
        "è¼¸å‡ºæˆæª”æ¡ˆ",
        "è¼¸å‡ºæˆæ–‡ä»¶",
        "å¯«æˆæª”æ¡ˆ",
        "å¯«æˆæ–‡ä»¶",
        "ä¿å­˜æˆ",
        "å­˜æˆ",
        "å¦å­˜",
    ]
    if any(k in t for k in keywords):
        return True

    # éš±å«æ„åœ–ï¼šæ•´ç†ä»¥ä¸Šå°è©±ï¼ˆé€šå¸¸æ˜¯è¦ç”Ÿæˆæ–‡ä»¶ï¼‰
    implicit = [
        "å¹«æˆ‘æ•´ç†ä»¥ä¸Šå°è©±",
        "æ•´ç†ä»¥ä¸Šå°è©±",
        "æ•´ç†é€™æ®µå°è©±",
        "æ•´ç†å°è©±",
        "æ•´ç†æˆæ–‡ä»¶",
        "æ•´ç†æˆæª”æ¡ˆ",
    ]
    if any(k in t for k in implicit):
        return True

    # å‡ºç¾æ˜ç¢ºæª”åï¼ˆå«å‰¯æª”åï¼‰ä¹Ÿè¦–ç‚ºå»ºæª”æ„åœ–
    if re.search(r"[A-Za-z0-9_\-\u4e00-\u9fff/]+\.(md|txt|json)\b", t):
        return True

    return False


def _parse_target_path(text: str) -> tuple[Optional[str], Optional[str]]:
    """
    å¾ user text å˜—è©¦è§£æå‡º "dir/file.ext"ï¼›åªæ”¯æ´ md/txt/jsonã€‚
    å›å‚³ (folder_path, filename)ï¼›folder_path ä»¥ "a/b" å½¢å¼ï¼ˆä¸å«æœ«å°¾ /ï¼‰ã€‚
    """
    t = (text or "").strip()
    matches = re.findall(
        r"([A-Za-z0-9_\-\u4e00-\u9fff/]+\.(?:md|txt|json))\b",
        t,
    )
    if matches:
        raw_path = matches[-1].lstrip("/").strip()
        parts = [p for p in raw_path.split("/") if p]
        if not parts:
            return None, None
        if len(parts) == 1:
            return None, parts[0]
        return "/".join(parts[:-1]), parts[-1]

    # åªæœ‰æŒ‡å®šç›®éŒ„ä½†æœªæŒ‡å®šæª”åï¼ˆä¾‹å¦‚ï¼šæ”¾åˆ° docs ç›®éŒ„ï¼‰
    m = re.search(r"åœ¨\s*([A-Za-z0-9_\-\u4e00-\u9fff/]+)\s*ç›®éŒ„", t)
    if m:
        folder = m.group(1).strip().strip("/").strip()
        return folder or None, None

    return None, None


def _default_filename_for_intent(text: str) -> str:
    t = (text or "").strip()
    if "æ•´ç†" in t and "å°è©±" in t:
        return "conversation-summary.md"
    return "ai-output.md"


def _file_type_for_filename(filename: str) -> str:
    ext = Path(filename).suffix.lower()
    if ext == ".md":
        return "text/markdown"
    if ext == ".txt":
        return "text/plain"
    if ext == ".json":
        return "application/json"
    return "text/plain"


def _looks_like_edit_file_intent(text: str) -> bool:
    """
    æª¢æ¸¬æ˜¯å¦ç‚ºç·¨è¼¯æª”æ¡ˆæ„åœ–ã€‚
    æ¨¡å¼ï¼š
    - "å¹«æˆ‘ä¿®æ”¹ @xxx.md"
    - "åœ¨ @xxx.md ä¸­å¢åŠ "
    - "ç·¨è¼¯ @xxx.md"
    - "æ›´æ–° @xxx.md"
    - "å¹«æˆ‘åœ¨ @xxx.md å¢åŠ ä¸€äº›æ–‡å­—"
    """
    t = (text or "").strip()
    if not t:
        return False

    # æª¢æ¸¬ @filename æ¨¡å¼
    if re.search(r"@[A-Za-z0-9_\-\u4e00-\u9fff/]+\.(md|txt|json)\b", t):
        # é…åˆç·¨è¼¯é—œéµè©
        edit_keywords = [
            "ä¿®æ”¹",
            "ç·¨è¼¯",
            "æ›´æ–°",
            "å¢åŠ ",
            "æ·»åŠ ",
            "åˆªé™¤",
            "ç§»é™¤",
            "å¹«æˆ‘",
            "è«‹",
            "åœ¨",
            "ä¸­",
            "è£¡",
        ]
        if any(k in t for k in edit_keywords):
            return True

    return False


def _parse_file_reference(text: str) -> Optional[str]:
    """
    å¾æ–‡æœ¬ä¸­æå–æª”æ¡ˆå¼•ç”¨ï¼ˆ@filenameï¼‰ã€‚
    è¿”å›æª”åï¼ˆä¸å« @ ç¬¦è™Ÿï¼‰ã€‚
    """
    t = (text or "").strip()
    # åŒ¹é… @filename.ext æ¨¡å¼
    match = re.search(r"@([A-Za-z0-9_\-\u4e00-\u9fff/]+\.(?:md|txt|json))\b", t)
    if match:
        return match.group(1)
    return None


def _find_file_by_name(
    *,
    filename: str,
    task_id: str,
    user_id: str,
) -> Optional[Dict[str, Any]]:
    """
    æ ¹æ“šæª”åæŸ¥æ‰¾æª”æ¡ˆï¼ˆå¾Œç«¯æ­£å¼æª”æ¡ˆï¼‰ã€‚
    è¿”å›æª”æ¡ˆå…ƒæ•¸æ“šï¼ŒåŒ…å« file_idã€‚
    """
    metadata_service = get_metadata_service()
    # æŸ¥è©¢è©² task ä¸‹çš„æª”æ¡ˆ
    files = metadata_service.list(
        task_id=task_id,
        user_id=user_id,
        limit=100,
    )
    # æŸ¥æ‰¾åŒ¹é…çš„æª”åï¼ˆç²¾ç¢ºåŒ¹é…ï¼‰
    for file_meta in files:
        if file_meta.filename == filename:
            return {
                "file_id": file_meta.file_id,
                "filename": file_meta.filename,
                "is_draft": False,
            }
    return None


def _try_edit_file_from_chat_output(
    *,
    user_text: str,
    assistant_text: str,
    task_id: Optional[str],
    current_user: User,
    tenant_id: str,
) -> Optional[Dict[str, Any]]:
    """
    è‹¥ user_text å‘ˆç¾ç·¨è¼¯æª”æ¡ˆæ„åœ–ï¼Œå‰µå»ºç·¨è¼¯è«‹æ±‚ä¸¦è¿”å›é è¦½ã€‚

    æµç¨‹ï¼š
    1. æª¢æ¸¬ç·¨è¼¯æ„åœ–
    2. è§£ææª”æ¡ˆå¼•ç”¨
    3. æŸ¥æ‰¾æª”æ¡ˆï¼ˆå¾Œç«¯ï¼‰
    4. å‰µå»ºç·¨è¼¯è«‹æ±‚ï¼ˆèª¿ç”¨ docs_editing APIï¼‰
    5. è¿”å›é è¦½å’Œ request_id
    """
    if not task_id:
        return None
    if not _looks_like_edit_file_intent(user_text):
        return None

    filename = _parse_file_reference(user_text)
    if not filename:
        return None

    # æŸ¥æ‰¾æª”æ¡ˆï¼ˆåªæŸ¥æ‰¾å¾Œç«¯æ­£å¼æª”æ¡ˆï¼Œè‰ç¨¿æª”ç”±å‰ç«¯è™•ç†ï¼‰
    file_info = _find_file_by_name(
        filename=filename,
        task_id=task_id,
        user_id=current_user.user_id,
    )
    if not file_info:
        # æª”æ¡ˆä¸å­˜åœ¨ï¼Œè¿”å› Noneï¼ˆå‰ç«¯å¯ä»¥è™•ç†è‰ç¨¿æª”ï¼‰
        return None

    # æ¬Šé™æª¢æŸ¥
    perm = get_file_permission_service()
    try:
        perm.check_file_access(
            user=current_user,
            file_id=file_info["file_id"],
            required_permission=Permission.FILE_UPDATE.value,
        )
    except Exception as exc:  # noqa: BLE001
        logger.warning(
            "genai_chat_file_edit_permission_denied",
            error=str(exc),
            file_id=file_info["file_id"],
            user_id=current_user.user_id,
        )
        return None

    # ç²å–æª”æ¡ˆå…ƒæ•¸æ“š
    metadata_service = get_metadata_service()
    file_meta = metadata_service.get(file_info["file_id"])
    if file_meta is None:
        return None

    # æª¢æ¸¬æ–‡ä»¶æ ¼å¼
    doc_format = detect_doc_format(filename=file_meta.filename, file_type=file_meta.file_type)
    if doc_format not in {"md", "txt", "json"}:
        return None

    # ç²å–ç•¶å‰ç‰ˆæœ¬
    from api.routers.docs_editing import _get_doc_version

    current_version = _get_doc_version(file_meta.custom_metadata)

    # å‰µå»ºç·¨è¼¯è«‹æ±‚
    # ä½¿ç”¨ assistant_text ä½œç‚ºç·¨è¼¯æŒ‡ä»¤ï¼ˆç°¡åŒ–è™•ç†ï¼šå°‡ AI å›å¾©ä½œç‚ºã€Œæ›¿æ›æ•´å€‹æª”æ¡ˆã€çš„æŒ‡ä»¤ï¼‰
    # å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œå¯ä»¥è®“ LLM ç”Ÿæˆæ›´ç²¾ç¢ºçš„ç·¨è¼¯æŒ‡ä»¤
    instruction = f"æ ¹æ“šä»¥ä¸‹å…§å®¹æ›´æ–°æª”æ¡ˆï¼š\n\n{assistant_text}"

    request_id = str(uuid.uuid4())
    record = DocEditRequestRecord(
        request_id=request_id,
        tenant_id=tenant_id,
        user_id=current_user.user_id,
        file_id=file_info["file_id"],
        task_id=task_id,
        doc_format=doc_format,  # type: ignore[arg-type]
        instruction=instruction,
        base_version=int(current_version),
        status=DocEditStatus.queued,
    )

    store = get_doc_edit_request_store_service()
    store.create(record)

    # å•Ÿå‹•é è¦½ä»»å‹™ï¼ˆä½¿ç”¨ asyncio.create_taskï¼‰
    try:
        from api.routers.docs_editing import _run_preview_request

        loop = asyncio.get_event_loop()
        if loop.is_running():
            task = asyncio.create_task(_run_preview_request(request_id=request_id))
            # è¨»å†Šä»»å‹™ï¼ˆå¦‚æœéœ€è¦è¿½è¹¤ï¼‰
            from api.routers.docs_editing import _register_request_task

            _register_request_task(request_id=request_id, task=task)
        else:
            # å¦‚æœæ²’æœ‰é‹è¡Œä¸­çš„ loopï¼Œä½¿ç”¨ run_until_completeï¼ˆä¸æ‡‰è©²ç™¼ç”Ÿï¼‰
            logger.warning(
                "genai_chat_file_edit_no_event_loop",
                request_id=request_id,
                file_id=file_info["file_id"],
            )
    except Exception as exc:  # noqa: BLE001
        logger.warning(
            "genai_chat_file_edit_start_preview_failed",
            error=str(exc),
            request_id=request_id,
            file_id=file_info["file_id"],
        )

    return {
        "type": "file_edited",
        "file_id": file_info["file_id"],
        "filename": filename,
        "request_id": request_id,
        "task_id": task_id,
        "is_draft": False,
    }


def _ensure_folder_path(
    *,
    task_id: str,
    user_id: str,
    folder_path: str,
) -> Optional[str]:
    """
    ç¢ºä¿ folder_pathï¼ˆä¾‹å¦‚ a/b/cï¼‰å­˜åœ¨æ–¼ folder_metadataã€‚
    å›å‚³æœ€æ·±å±¤ folder_idï¼ˆç”¨æ–¼ file_metadata.folder_idï¼‰ã€‚

    æ³¨æ„ï¼šfolder_metadata æ˜¯ UI/æŸ¥è©¢ç”¨çš„ã€Œé‚è¼¯è³‡æ–™å¤¾ã€ï¼Œæª”æ¡ˆå¯¦é«”ä»åœ¨ task workspace æ ¹ç›®éŒ„ã€‚
    """
    folder_path = (folder_path or "").strip().strip("/")
    if not folder_path:
        return None

    client = get_arango_client()
    if client.db is None or client.db.aql is None:
        return None

    # ç¢ºä¿é›†åˆå­˜åœ¨
    if not client.db.has_collection(_FOLDER_COLLECTION_NAME):
        client.db.create_collection(_FOLDER_COLLECTION_NAME)
        col = client.db.collection(_FOLDER_COLLECTION_NAME)
        col.add_index({"type": "persistent", "fields": ["task_id"]})
        col.add_index({"type": "persistent", "fields": ["user_id"]})

    parent_task_id: str = f"{task_id}_workspace"
    folder_id: Optional[str] = None

    for seg in [p for p in folder_path.split("/") if p]:
        # æŸ¥è©¢æ˜¯å¦å·²å­˜åœ¨åŒåè³‡æ–™å¤¾ï¼ˆåŒä¸€ parent ä¸‹ï¼‰
        query = f"""
            FOR folder IN {_FOLDER_COLLECTION_NAME}
                FILTER folder.task_id == @task_id
                FILTER folder.user_id == @user_id
                FILTER folder.parent_task_id == @parent_task_id
                FILTER folder.folder_name == @folder_name
                LIMIT 1
                RETURN folder
        """
        cursor = client.db.aql.execute(
            query,
            bind_vars={
                "task_id": task_id,
                "user_id": user_id,
                "parent_task_id": parent_task_id,
                "folder_name": seg,
            },
        )
        existing = next(cursor, None) if cursor else None  # type: ignore[arg-type]  # cursor å¯èƒ½ç‚º None
        if existing and isinstance(existing, dict) and existing.get("_key"):
            folder_id = str(existing["_key"])
            parent_task_id = folder_id
            continue

        # å»ºç«‹æ–°è³‡æ–™å¤¾
        new_id = str(uuid.uuid4())
        now_iso = datetime.utcnow().isoformat()
        doc = {
            "_key": new_id,
            "task_id": task_id,
            "folder_name": seg,
            "user_id": user_id,
            "parent_task_id": parent_task_id,
            "created_at": now_iso,
            "updated_at": now_iso,
        }
        client.db.collection(_FOLDER_COLLECTION_NAME).insert(doc)
        folder_id = new_id
        parent_task_id = new_id

    return folder_id


def _try_create_file_from_chat_output(
    *,
    user_text: str,
    assistant_text: str,
    task_id: Optional[str],
    current_user: User,
) -> Optional[Dict[str, Any]]:
    """
    è‹¥ user_text å‘ˆç¾å»ºæª”æ„åœ–ï¼Œå°‡ assistant_text å¯«å…¥ task workspaceï¼ˆé è¨­æ ¹ç›®éŒ„ï¼‰ã€‚
    è‹¥æŒ‡å®šç›®éŒ„ï¼ˆå¦‚ docs/a.mdï¼‰ï¼Œå‰‡å»ºç«‹å°æ‡‰é‚è¼¯è³‡æ–™å¤¾ï¼ˆfolder_metadataï¼‰ä¸¦å°‡ file_metadata.folder_id æŒ‡å‘è©²è³‡æ–™å¤¾ã€‚
    """
    if not task_id:
        return None
    if not _looks_like_create_file_intent(user_text):
        return None

    folder_path, filename = _parse_target_path(user_text)
    if not filename:
        filename = _default_filename_for_intent(user_text)

    # åªå…è¨± md/txt/json
    ext = Path(filename).suffix.lower()
    if ext not in (".md", ".txt", ".json"):
        return None

    # æ¬Šé™ï¼šéœ€è¦èƒ½åœ¨ task ä¸‹æ–°å¢/æ›´æ–°æª”æ¡ˆ
    perm = get_file_permission_service()
    perm.check_task_file_access(
        user=current_user,
        task_id=task_id,
        required_permission=Permission.FILE_UPDATE.value,
    )
    perm.check_upload_permission(user=current_user)

    folder_id = None
    if folder_path:
        folder_id = _ensure_folder_path(
            task_id=task_id,
            user_id=current_user.user_id,
            folder_path=folder_path,
        )

    content_bytes = (assistant_text or "").rstrip("\n").encode("utf-8") + b"\n"
    storage = get_storage()
    file_id, storage_path = storage.save_file(
        file_content=content_bytes,
        filename=filename,
        task_id=task_id,
    )

    metadata_service = get_metadata_service()
    metadata_service.create(
        FileMetadataCreate(
            file_id=file_id,
            filename=filename,
            file_type=_file_type_for_filename(filename),
            file_size=len(content_bytes),
            processing_status=None,  # type: ignore[call-arg]  # æ‰€æœ‰åƒæ•¸éƒ½æ˜¯ Optional
            chunk_count=None,  # type: ignore[call-arg]
            vector_count=None,  # type: ignore[call-arg]
            kg_status=None,  # type: ignore[call-arg]
            user_id=current_user.user_id,
            task_id=task_id,
            folder_id=folder_id,
            storage_path=storage_path,
            tags=["genai", "chat"],
            description="Created from chat intent",
            status="generated",
        )
    )

    return {
        "type": "file_created",
        "file_id": file_id,
        "filename": filename,
        "task_id": task_id,
        "folder_id": folder_id,
        "folder_path": folder_path,
    }


def _record_if_changed(
    *,
    context_manager: ContextManager,
    session_id: str,
    role: str,
    content: str,
    agent_name: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> bool:
    """
    é˜²æ­¢å‰ç«¯é‡é€ history é€ æˆé‡è¤‡è¨˜éŒ„ï¼š
    è‹¥æœ€å¾Œä¸€ç­†ï¼ˆrole, contentï¼‰ç›¸åŒï¼Œå‰‡è·³éå¯«å…¥ã€‚
    """
    normalized_content = str(content or "").strip()
    if not normalized_content:
        return False

    try:
        last_messages = context_manager.get_messages(session_id=session_id, limit=1)
        if last_messages:
            last = last_messages[-1]
            if str(last.role) == role and str(last.content).strip() == normalized_content:
                return False
    except Exception:  # noqa: BLE001
        # å–æœ€å¾Œæ¶ˆæ¯å¤±æ•—æ™‚ä¸é˜»æ“‹å¯«å…¥
        pass

    return context_manager.record_message(
        session_id=session_id,
        role=role,
        content=normalized_content,
        agent_name=agent_name,
        metadata=metadata,
    )


def _infer_provider_from_model_id(model_id: str) -> LLMProvider:
    """ä»¥ç´„å®šå„ªå…ˆçš„ heuristic æ¨å° providerï¼ˆMVPï¼‰ã€‚"""
    m = model_id.lower()

    # å…·å‚™ ollama å¸¸è¦‹æ¨™è¨˜ï¼ˆæœ¬åœ°æ¨¡å‹æˆ–å¸¶ tagï¼‰
    if ":" in m or m in {"llama2", "gpt-oss:20b", "qwen3-coder:30b"}:
        return LLMProvider.OLLAMA

    if m.startswith("gpt-") or m.startswith("openai") or "gpt" in m:
        return LLMProvider.CHATGPT
    if m.startswith("gemini"):
        return LLMProvider.GEMINI
    if m.startswith("grok"):
        return LLMProvider.GROK
    if m.startswith("qwen"):
        # qwen-turbo / qwen-plus ç­‰é›²ç«¯ provider
        return LLMProvider.QWEN

    # fallbackï¼šç›¡é‡ä¸é˜»æ“‹ï¼ˆé è¨­æœ¬åœ°ï¼‰
    return LLMProvider.OLLAMA


def _extract_content(result: Dict[str, Any]) -> str:
    return str(result.get("content") or result.get("message") or result.get("text") or "")


def _register_request_task(*, request_id: str, task: asyncio.Task[None]) -> None:
    with _request_tasks_lock:
        _request_tasks[request_id] = task


def _pop_request_task(*, request_id: str) -> Optional[asyncio.Task[None]]:
    with _request_tasks_lock:
        return _request_tasks.pop(request_id, None)


def _get_request_task(*, request_id: str) -> Optional[asyncio.Task[None]]:
    with _request_tasks_lock:
        return _request_tasks.get(request_id)


async def _process_chat_request(
    *,
    request_body: ChatRequest,
    request_id: str,
    tenant_id: str,
    current_user: User,
) -> ChatResponse:
    """
    å¯é‡ç”¨çš„ chat pipelineï¼ˆåŒæ­¥å…¥å£ / éåŒæ­¥ request å…¥å£å…±ç”¨ï¼‰ã€‚

    æ³¨æ„ï¼šæ­¤å‡½æ•¸æœƒæ²¿ç”¨ç¾æœ‰çš„å¯è§€æ¸¬æ€§ï¼ˆtrace/metricsï¼‰èˆ‡è¨˜æ†¶/ä¸Šä¸‹æ–‡æµç¨‹ã€‚
    """
    moe = get_moe_manager()
    classifier = get_task_classifier()
    context_manager = get_context_manager()
    memory_service = get_chat_memory_service()
    trace_store = get_genai_trace_store_service()
    metrics = get_genai_metrics_service()
    config_resolver = get_genai_config_resolver_service()
    policy_gate = config_resolver.get_effective_policy_gate(
        tenant_id=tenant_id,
        user_id=current_user.user_id,
    )
    file_permission_service = get_file_permission_service()

    session_id = request_body.session_id or str(uuid.uuid4())
    task_id = request_body.task_id

    messages = [m.model_dump() for m in request_body.messages]
    model_selector = request_body.model_selector

    routing: Dict[str, Any] = {}
    observability = ObservabilityInfo(
        request_id=request_id,
        session_id=session_id,
        task_id=task_id,
        token_input=None,  # type: ignore[call-arg]  # token_input æœ‰é»˜èªå€¼
    )

    start_time = time.perf_counter()

    # G5ï¼šå…¥å£äº‹ä»¶ï¼ˆlog + traceï¼‰
    logger.info(
        "genai_chat_request_received",
        request_id=request_id,
        session_id=session_id,
        task_id=task_id,
        user_id=current_user.user_id,
        model_selector_mode=model_selector.mode,
        model_id=model_selector.model_id,
    )
    trace_store.add_event(
        GenAITraceEvent(
            event="chat.request_received",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            status="ok",
        )
    )

    # å–æœ€å¾Œä¸€å‰‡ user messageï¼ˆè‹¥æ‰¾ä¸åˆ°å°±å–æœ€å¾Œä¸€å‰‡ï¼‰
    user_messages = [m for m in messages if m.get("role") == "user"]
    last_user_text = (str(user_messages[-1].get("content", "")) if user_messages else "").strip()
    if not last_user_text and messages:
        last_user_text = str(messages[-1].get("content", "")).strip()

    # G3ï¼šè¨˜éŒ„ user messageï¼ˆé¿å…é‡è¤‡å¯«å…¥ï¼‰
    _record_if_changed(
        context_manager=context_manager,
        session_id=session_id,
        role="user",
        content=last_user_text,
        metadata={
            "user_id": current_user.user_id,
            "session_id": session_id,
            "task_id": task_id,
            "request_id": request_id,
        },
    )

    # G3ï¼šç”¨ windowed history ä½œç‚º MoE çš„ messagesï¼ˆä¸¦ä¿ç•™å‰ç«¯æä¾›çš„ system messageï¼‰
    system_messages = [m for m in messages if m.get("role") == "system"]
    windowed_history = context_manager.get_context_with_window(session_id=session_id)
    observability.context_message_count = len(windowed_history)

    # G6ï¼šé™„ä»¶ file_id æ¬Šé™æª¢æŸ¥ï¼ˆé¿å…é€é RAG è®€åˆ°ä¸å±¬æ–¼è‡ªå·±çš„æ–‡ä»¶ï¼‰
    if request_body.attachments:
        for att in request_body.attachments:
            file_permission_service.check_file_access(
                user=current_user,
                file_id=att.file_id,
                required_permission=Permission.FILE_READ.value,
            )

    # G6ï¼šData consent gateï¼ˆAI_PROCESSINGï¼‰- æœªåŒæ„å‰‡ä¸æª¢ç´¢/ä¸æ³¨å…¥/ä¸å¯«å…¥
    has_ai_consent = False
    try:
        from services.api.models.data_consent import ConsentType

        consent_service = get_consent_service()
        has_ai_consent = consent_service.check_consent(
            current_user.user_id, ConsentType.AI_PROCESSING
        )
    except Exception as exc:  # noqa: BLE001
        logger.warning(
            "genai_consent_check_failed",
            error=str(exc),
            request_id=request_id,
            user_id=current_user.user_id,
        )
        has_ai_consent = False

    if has_ai_consent:
        memory_result = await memory_service.retrieve_for_prompt(
            user_id=current_user.user_id,
            session_id=session_id,
            task_id=task_id,
            request_id=request_id,
            query=last_user_text,
            attachments=request_body.attachments,
        )
        observability.memory_hit_count = memory_result.memory_hit_count
        observability.memory_sources = memory_result.memory_sources
        observability.retrieval_latency_ms = memory_result.retrieval_latency_ms
    else:
        from services.api.services.chat_memory_service import MemoryRetrievalResult

        memory_result = MemoryRetrievalResult(
            injection_messages=[],
            memory_hit_count=0,
            memory_sources=[],
            retrieval_latency_ms=0.0,
        )
        observability.memory_hit_count = 0
        observability.memory_sources = []
        observability.retrieval_latency_ms = 0.0

    trace_store.add_event(
        GenAITraceEvent(
            event="chat.memory_retrieved",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            memory_hit_count=observability.memory_hit_count,
            memory_sources=observability.memory_sources,
            retrieval_latency_ms=observability.retrieval_latency_ms,
            status="ok",
        )
    )

    base_system = system_messages[:1] if system_messages else []
    messages_for_llm = base_system + memory_result.injection_messages + windowed_history

    # å‘¼å« MoE
    llm_call_start = time.perf_counter()
    if model_selector.mode == "auto":
        allowed_providers = policy_gate.get_allowed_providers()

        # ç²å–ç”¨æˆ¶çš„æ”¶è—æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨æ–¼ Auto æ¨¡å¼å„ªå…ˆé¸æ“‡ï¼‰
        favorite_model_ids: List[str] = []
        try:
            from services.api.services.user_preference_service import get_user_preference_service

            preference_service = get_user_preference_service()
            favorite_model_ids = preference_service.get_favorite_models(
                user_id=current_user.user_id
            )
        except Exception as exc:  # noqa: BLE001
            logger.debug(f"Failed to get favorite models for user {current_user.user_id}: {exc}")
            # fallback åˆ°å…§å­˜ç·©å­˜
            favorite_model_ids = _favorite_models_by_user.get(current_user.user_id, [])

        llm_api_keys = config_resolver.resolve_api_keys_map(
            tenant_id=tenant_id,
            user_id=current_user.user_id,
            providers=allowed_providers,
        )
        task_classification = classifier.classify(
            last_user_text,
            context={
                "user_id": current_user.user_id,
                "session_id": session_id,
                "task_id": task_id,
            },
        )

        result = await moe.chat(
            messages_for_llm,
            task_classification=task_classification,
            context={
                "user_id": current_user.user_id,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "task_id": task_id,
                "allowed_providers": allowed_providers,
                "llm_api_keys": llm_api_keys,
                "favorite_models": favorite_model_ids,  # å‚³éæ”¶è—æ¨¡å‹åˆ—è¡¨
            },
        )
    else:
        selected_model_id = model_selector.model_id or ""
        provider = _infer_provider_from_model_id(selected_model_id)
        if not policy_gate.is_model_allowed(provider.value, selected_model_id):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail={
                    "message": "Model is not allowed by policy",
                    "error_code": "MODEL_NOT_ALLOWED",
                    "provider": provider.value,
                    "model_id": selected_model_id,
                },
            )
        llm_api_keys = config_resolver.resolve_api_keys_map(
            tenant_id=tenant_id,
            user_id=current_user.user_id,
            providers=[provider.value],
        )
        result = await moe.chat(
            messages_for_llm,
            provider=provider,
            model=selected_model_id,
            context={
                "user_id": current_user.user_id,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "task_id": task_id,
                "llm_api_keys": llm_api_keys,
            },
        )

    llm_latency_ms = (time.perf_counter() - llm_call_start) * 1000.0
    total_latency_ms = (time.perf_counter() - start_time) * 1000.0

    content = _extract_content(result)
    routing = (result.get("_routing") if isinstance(result, dict) else None) or {}
    routing_info = RoutingInfo(
        provider=str(routing.get("provider") or "unknown"),
        model=routing.get("model"),
        strategy=str(routing.get("strategy") or "unknown"),
        latency_ms=routing.get("latency_ms"),
        failover_used=bool(routing.get("failover_used") or False),
        fallback_provider=routing.get("fallback_provider"),
    )

    trace_store.add_event(
        GenAITraceEvent(
            event="chat.llm_completed",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            provider=routing_info.provider,
            model=routing_info.model,
            strategy=routing_info.strategy,
            failover_used=routing_info.failover_used,
            fallback_provider=routing_info.fallback_provider,
            llm_latency_ms=llm_latency_ms,
            status="ok",
        )
    )

    # G3ï¼šè¨˜éŒ„ assistant messageï¼ˆé¿å…é‡è¤‡å¯«å…¥ï¼‰
    _record_if_changed(
        context_manager=context_manager,
        session_id=session_id,
        role="assistant",
        content=content,
        metadata={
            "user_id": current_user.user_id,
            "session_id": session_id,
            "task_id": task_id,
            "request_id": request_id,
            "routing": routing,
        },
    )

    if has_ai_consent:
        await memory_service.write_from_turn(
            user_id=current_user.user_id,
            session_id=session_id,
            task_id=task_id,
            request_id=request_id,
            user_text=last_user_text,
            assistant_text=content,
        )

    actions: Optional[List[Dict[str, Any]]] = None
    try:
        # å„ªå…ˆå˜—è©¦ç·¨è¼¯æª”æ¡ˆï¼ˆå¦‚æœæª¢æ¸¬åˆ°ç·¨è¼¯æ„åœ–ï¼‰
        edit_action = _try_edit_file_from_chat_output(
            user_text=last_user_text,
            assistant_text=content,
            task_id=task_id,
            current_user=current_user,
            tenant_id=tenant_id,
        )
        if edit_action:
            actions = [edit_action]
        else:
            # å¦‚æœæ²’æœ‰ç·¨è¼¯æ„åœ–ï¼Œå˜—è©¦å‰µå»ºæª”æ¡ˆ
            create_action = _try_create_file_from_chat_output(
                user_text=last_user_text,
                assistant_text=content,
                task_id=task_id,
                current_user=current_user,
            )
            if create_action:
                actions = [create_action]
    except Exception as exc:  # noqa: BLE001
        logger.warning(
            "genai_chat_file_action_failed",
            error=str(exc),
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
        )

    response = ChatResponse(
        content=content,
        session_id=session_id,
        task_id=task_id,
        routing=routing_info,
        observability=observability,
        actions=actions,
    )

    final_event = GenAITraceEvent(
        event="chat.response_sent",
        request_id=request_id,
        session_id=session_id,
        task_id=task_id,
        user_id=current_user.user_id,
        provider=routing_info.provider,
        model=routing_info.model,
        strategy=routing_info.strategy,
        failover_used=routing_info.failover_used,
        fallback_provider=routing_info.fallback_provider,
        memory_hit_count=observability.memory_hit_count,
        memory_sources=observability.memory_sources,
        retrieval_latency_ms=observability.retrieval_latency_ms,
        context_message_count=observability.context_message_count,
        total_latency_ms=total_latency_ms,
        llm_latency_ms=llm_latency_ms,
        status="ok",
    )
    trace_store.add_event(final_event)
    metrics.record_final_event(final_event)

    logger.info(
        "genai_chat_response_sent",
        request_id=request_id,
        session_id=session_id,
        task_id=task_id,
        user_id=current_user.user_id,
        provider=routing_info.provider,
        model=routing_info.model,
        strategy=routing_info.strategy,
        failover_used=routing_info.failover_used,
        fallback_provider=routing_info.fallback_provider,
        memory_hit_count=observability.memory_hit_count,
        memory_sources=observability.memory_sources,
        retrieval_latency_ms=observability.retrieval_latency_ms,
        context_message_count=observability.context_message_count,
        total_latency_ms=total_latency_ms,
        llm_latency_ms=llm_latency_ms,
    )

    return response


@router.post("/stream", status_code=status.HTTP_200_OK)
async def chat_product_stream(
    request_body: ChatRequest,
    request: Request,
    tenant_id: str = Depends(get_current_tenant_id),
    current_user: User = Depends(get_current_user),
) -> StreamingResponse:
    """
    ç”¢å“ç´š Chat æµå¼å…¥å£ï¼šè¿”å› SSE æ ¼å¼çš„æµå¼éŸ¿æ‡‰ã€‚

    - Autoï¼šTaskClassifier â†’ task_classification â†’ é¸æ“‡ provider â†’ èª¿ç”¨å®¢æˆ¶ç«¯ stream
    - Manual/Favoriteï¼šä»¥ model_id æ¨å° providerï¼Œä¸¦åš provider/model override
    """
    moe = get_moe_manager()
    classifier = get_task_classifier()
    context_manager = get_context_manager()
    memory_service = get_chat_memory_service()
    policy_gate = get_genai_policy_gate_service()
    config_resolver = get_genai_config_resolver_service()

    session_id = request_body.session_id or str(uuid.uuid4())
    task_id = request_body.task_id
    request_id = getattr(request.state, "request_id", None) or str(uuid.uuid4())

    messages = [m.model_dump() for m in request_body.messages]
    model_selector = request_body.model_selector

    # è¨˜éŒ„å·¥å…·ä¿¡æ¯
    allowed_tools = request_body.allowed_tools or []

    # æ·»åŠ è¯¦ç»†çš„å·¥å…·æ—¥å¿—
    logger.info(
        "chat_request_tools_received",
        request_id=request_id,
        allowed_tools=allowed_tools,
        has_web_search="web_search" in allowed_tools,
        allowed_tools_count=len(allowed_tools),
    )
    logger.info(
        "chat_request_tools",
        request_id=request_id,
        session_id=session_id,
        task_id=task_id,
        allowed_tools=allowed_tools,
        has_web_search="web_search" in allowed_tools,
    )

    async def generate_stream() -> AsyncGenerator[str, None]:
        """ç”Ÿæˆ SSE æ ¼å¼çš„æµå¼æ•¸æ“š"""
        try:
            # å–æœ€å¾Œä¸€å‰‡ user message
            user_messages = [m for m in messages if m.get("role") == "user"]
            last_user_text = str(user_messages[-1].get("content", "")) if user_messages else ""
            if not last_user_text and messages:
                last_user_text = str(messages[-1].get("content", ""))

            # è¨˜éŒ„ user message
            _record_if_changed(
                context_manager=context_manager,
                session_id=session_id,
                role="user",
                content=last_user_text,
                metadata={
                    "user_id": current_user.user_id,
                    "session_id": session_id,
                    "task_id": task_id,
                    "request_id": request_id,
                },
            )

            # G3ï¼šç”¨ windowed history ä½œç‚º MoE çš„ messagesï¼ˆä¸¦ä¿ç•™å‰ç«¯æä¾›çš„ system messageï¼‰
            system_messages = [m for m in messages if m.get("role") == "system"]
            windowed_history = context_manager.get_context_with_window(session_id=session_id)

            # å·¥å…·è°ƒç”¨ï¼šå¦‚æœå¯ç”¨äº† web_search ä¸”æ¶ˆæ¯ä¸­åŒ…å«æœç´¢æ„å›¾ï¼Œç›´æ¥è°ƒç”¨å·¥å…·
            logger.info(
                "web_search_check",
                request_id=request_id,
                allowed_tools=allowed_tools,
                has_web_search="web_search" in (allowed_tools or []),
                user_text=last_user_text[:200],  # è®°å½•å‰200ä¸ªå­—ç¬¦
            )

            # æ·»åŠ  print è°ƒè¯•è¾“å‡º
            print(
                f"\n[DEBUG] web_search_check: allowed_tools={allowed_tools}, has_web_search={'web_search' in (allowed_tools or [])}"
            )

            if allowed_tools and "web_search" in allowed_tools:
                # æ£€æµ‹æ˜¯å¦éœ€è¦æœç´¢ï¼ˆç®€å•çš„å…³é”®è¯æ£€æµ‹ï¼‰
                # æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼ŒåŒ…æ‹¬æ›´å¤šæœç´¢æ„å›¾
                search_keywords = [
                    "ä¸Šç¶²",
                    "æŸ¥è©¢",
                    "æœç´¢",
                    "æœå°‹",
                    "ç¾åœ¨",
                    "æ™‚é–“",
                    "æ™‚åˆ»",
                    "æœ€æ–°",
                    "ç•¶å‰",
                    "http",
                    "https",
                    "www.",
                    ".com",
                    ".net",
                    ".org",  # URL å…³é”®è¯
                    "ç¶²ç«™",
                    "ç¶²é ",
                    "é€£çµ",
                    "éˆæ¥",
                    "ç¶²å€",  # ç½‘ç«™ç›¸å…³
                    "æŸ¥æ‰¾",
                    "æ‰¾",
                    "æœ",
                    "æŸ¥",  # æœç´¢ç›¸å…³
                ]
                needs_search = any(keyword in last_user_text for keyword in search_keywords)
                matched_keywords = [kw for kw in search_keywords if kw in last_user_text]

                logger.info(
                    "web_search_intent_check",
                    request_id=request_id,
                    needs_search=needs_search,
                    matched_keywords=matched_keywords,
                    user_text=last_user_text[:200],
                    search_keywords_count=len(search_keywords),
                )

                # æ·»åŠ  print è°ƒè¯•è¾“å‡º
                print(
                    f"[DEBUG] web_search_intent_check: needs_search={needs_search}, matched_keywords={matched_keywords}"
                )

                if needs_search:
                    try:
                        # ç›´æ¥å¯¼å…¥ web_search æ¨¡å—ï¼Œé¿å…è§¦å‘ tools/__init__.py ä¸­çš„å…¶ä»–å¯¼å…¥
                        from tools.web_search.web_search_tool import WebSearchInput, WebSearchTool

                        logger.info(
                            "web_search_triggered",
                            request_id=request_id,
                            query=last_user_text,
                        )

                        # æ·»åŠ  print è°ƒè¯•è¾“å‡º
                        print(f"[DEBUG] web_search_triggered: query={last_user_text[:100]}")

                        # è°ƒç”¨ web_search å·¥å…·
                        search_tool = WebSearchTool()
                        search_input = WebSearchInput(query=last_user_text, num=5)
                        search_result = await search_tool.execute(search_input)

                        # æ·»åŠ  print è°ƒè¯•è¾“å‡º
                        print(
                            f"[DEBUG] web_search_result: status={search_result.status}, results_count={len(search_result.results) if search_result.results else 0}"
                        )

                        # å°†æœç´¢ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
                        logger.info(
                            "web_search_result",
                            request_id=request_id,
                            status=search_result.status,
                            results_count=(
                                len(search_result.results) if search_result.results else 0
                            ),
                            provider=search_result.provider,
                        )

                        if search_result.status == "success" and search_result.results:
                            search_summary = "\n\n=== ğŸ” ç¶²çµ¡æœç´¢çµæœï¼ˆä¾†è‡ªçœŸå¯¦æœç´¢ï¼‰ ===\n"
                            search_summary += f"æœç´¢æä¾›å•†: {search_result.provider}\n"
                            search_summary += f"çµæœæ•¸é‡: {len(search_result.results)}\n"
                            search_summary += "---\n"
                            logger.debug(
                                "web_search_formatting_results",
                                request_id=request_id,
                                results_type=type(search_result.results).__name__,
                                results_count=len(search_result.results),
                            )

                            for i, result in enumerate(search_result.results[:3], 1):
                                # å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼ï¼ˆå¯èƒ½æ˜¯ dict æˆ–å¯¹è±¡ï¼‰
                                try:
                                    if isinstance(result, dict):
                                        title = result.get("title", "ç„¡æ¨™é¡Œ")
                                        snippet = result.get("snippet", "ç„¡æ‘˜è¦")
                                        link = result.get("link", "ç„¡éˆæ¥")
                                    elif hasattr(result, "model_dump"):
                                        # å¦‚æœæ˜¯ Pydantic æ¨¡å‹ï¼ˆSearchResultï¼‰ï¼Œä½¿ç”¨ model_dump
                                        result_dict = result.model_dump()
                                        title = result_dict.get("title", "ç„¡æ¨™é¡Œ")
                                        snippet = result_dict.get("snippet", "ç„¡æ‘˜è¦")
                                        link = result_dict.get("link", "ç„¡éˆæ¥")
                                    elif hasattr(result, "to_dict"):
                                        # å¦‚æœæ˜¯ SearchResultItem å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
                                        result_dict = result.to_dict()
                                        title = result_dict.get("title", "ç„¡æ¨™é¡Œ")
                                        snippet = result_dict.get("snippet", "ç„¡æ‘˜è¦")
                                        link = result_dict.get("link", "ç„¡éˆæ¥")
                                    else:
                                        # å¦‚æœæ˜¯å¯¹è±¡ï¼Œå°è¯•ç›´æ¥è®¿é—®å±æ€§ï¼ˆPydantic æ¨¡å‹æ”¯æŒå±æ€§è®¿é—®ï¼‰
                                        title = getattr(result, "title", "ç„¡æ¨™é¡Œ") or "ç„¡æ¨™é¡Œ"
                                        snippet = getattr(result, "snippet", "ç„¡æ‘˜è¦") or "ç„¡æ‘˜è¦"
                                        link = getattr(result, "link", "ç„¡éˆæ¥") or "ç„¡éˆæ¥"

                                    search_summary += f"\nã€æœç´¢çµæœ {i}ã€‘\n"
                                    search_summary += f"æ¨™é¡Œ: {title}\n"
                                    search_summary += f"æ‘˜è¦: {snippet}\n"
                                    search_summary += f"ä¾†æºéˆæ¥: {link}\n"
                                except Exception as format_error:
                                    logger.warning(
                                        "web_search_result_format_error",
                                        request_id=request_id,
                                        result_index=i,
                                        error=str(format_error),
                                        result_type=type(result).__name__,
                                        result_repr=str(result)[:200],
                                    )
                                    # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œè‡³å°‘æ·»åŠ åŸºæœ¬ä¿¡æ¯
                                    search_summary += (
                                        f"{i}. æœç´¢çµæœ {i} (æ ¼å¼åŒ–å¤±æ•—: {str(format_error)[:50]})\n\n"
                                    )

                            logger.info(
                                "web_search_summary_created",
                                request_id=request_id,
                                summary_length=len(search_summary),
                                summary_preview=search_summary[:500],  # è®°å½•å‰500å­—ç¬¦
                            )

                            # æ·»åŠ  print è°ƒè¯•è¾“å‡º
                            print(
                                f"[DEBUG] web_search_summary_created: length={len(search_summary)}"
                            )
                            print(f"[DEBUG] search_summary_preview:\n{search_summary[:500]}")

                            # æ›´æ–°æœ€å¾Œä¸€æ¢ç”¨æˆ¶æ¶ˆæ¯ï¼Œæ·»åŠ æœç´¢çµæœ
                            # åœ¨æœç´¢çµæœå‰æ·»åŠ æ˜ç¡®çš„æç¤ºï¼Œè®©AIçŸ¥é“è¿™æ˜¯çœŸå®æœç´¢ç»“æœ
                            search_summary_with_note = (
                                "\n\nã€é‡è¦æç¤ºï¼šä»¥ä¸‹æ˜¯çœŸå¯¦çš„ç¶²çµ¡æœç´¢çµæœï¼Œè«‹åŸºæ–¼é€™äº›çµæœå›ç­”å•é¡Œã€‚"
                                "å¦‚æœæœç´¢çµæœä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹æ˜ç¢ºèªªæ˜ï¼Œä¸è¦ç·¨é€ å…§å®¹ã€‚ã€‘\n" + search_summary
                            )

                            if windowed_history:
                                windowed_history[-1]["content"] = (
                                    windowed_history[-1].get("content", "")
                                    + search_summary_with_note
                                )
                            else:
                                # å¦‚æœæ²’æœ‰æ­·å²æ¶ˆæ¯ï¼Œå‰µå»ºä¸€æ¢åŒ…å«æœç´¢çµæœçš„æ¶ˆæ¯
                                windowed_history.append(
                                    {
                                        "role": "user",
                                        "content": last_user_text + search_summary_with_note,
                                    }
                                )

                            logger.info(
                                "web_search_completed",
                                request_id=request_id,
                                results_count=len(search_result.results),
                            )
                        else:
                            logger.warning(
                                "web_search_failed",
                                request_id=request_id,
                                status=search_result.status,
                            )
                    except Exception as tool_error:
                        logger.error(
                            "web_search_error",
                            request_id=request_id,
                            error=str(tool_error),
                            exc_info=True,
                        )
                        # å·¥å…·èª¿ç”¨å¤±æ•—ä¸å½±éŸ¿æ­£å¸¸æµç¨‹ï¼Œç¹¼çºŒåŸ·è¡Œ

            # G6ï¼šData consent gateï¼ˆAI_PROCESSINGï¼‰- æœªåŒæ„å‰‡ä¸æª¢ç´¢/ä¸æ³¨å…¥/ä¸å¯«å…¥
            has_ai_consent = False
            try:
                from services.api.models.data_consent import ConsentType

                consent_service = get_consent_service()
                has_ai_consent = consent_service.check_consent(
                    current_user.user_id, ConsentType.AI_PROCESSING
                )
            except Exception as exc:  # noqa: BLE001
                logger.debug(
                    "Failed to check AI consent, assuming no consent", error=str(exc), exc_info=True
                )
                has_ai_consent = False

            if has_ai_consent:
                memory_result = await memory_service.retrieve_for_prompt(
                    user_id=current_user.user_id,
                    session_id=session_id,
                    task_id=task_id,
                    request_id=request_id,
                    query=last_user_text,
                    attachments=request_body.attachments,
                )
            else:
                from services.api.services.chat_memory_service import MemoryRetrievalResult

                memory_result = MemoryRetrievalResult(
                    injection_messages=[],
                    memory_hit_count=0,
                    memory_sources=[],
                    retrieval_latency_ms=0.0,
                )

            base_system = system_messages[:1] if system_messages else []
            messages_for_llm = base_system + memory_result.injection_messages + windowed_history

            # æº–å‚™ MoE context
            task_classification = None
            provider = None
            model = None

            if model_selector.mode == "auto":
                # G6ï¼šprovider allowlistï¼ˆAutoï¼‰- å°‡ allowlist å‚³çµ¦ MoE åš provider éæ¿¾
                allowed_providers = policy_gate.get_allowed_providers()

                # ç²å–ç”¨æˆ¶çš„æ”¶è—æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨æ–¼ Auto æ¨¡å¼å„ªå…ˆé¸æ“‡ï¼‰
                favorite_model_ids: List[str] = []
                try:
                    from services.api.services.user_preference_service import (
                        get_user_preference_service,
                    )

                    preference_service = get_user_preference_service()
                    favorite_model_ids = preference_service.get_favorite_models(
                        user_id=current_user.user_id
                    )
                except Exception as exc:  # noqa: BLE001
                    logger.debug(
                        f"Failed to get favorite models for user {current_user.user_id}: {exc}"
                    )
                    # fallback åˆ°å…§å­˜ç·©å­˜
                    favorite_model_ids = _favorite_models_by_user.get(current_user.user_id, [])

                # ä»»åŠ¡åˆ†æï¼šä½¿ç”¨ TaskClassifier å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œåˆ†ç±»
                logger.info(
                    "task_analysis_start",
                    request_id=request_id,
                    user_text=last_user_text[:200],
                    user_id=current_user.user_id,
                    session_id=session_id,
                    task_id=task_id,
                )

                task_classification = classifier.classify(
                    last_user_text,
                    context={
                        "user_id": current_user.user_id,
                        "session_id": session_id,
                        "task_id": task_id,
                    },
                )

                logger.info(
                    "task_analysis_completed",
                    request_id=request_id,
                    task_type=task_classification.type.value if task_classification else None,
                    confidence=task_classification.confidence if task_classification else None,
                    reasoning=(
                        task_classification.reasoning[:200]
                        if task_classification and task_classification.reasoning
                        else None
                    ),
                )

                # æ·»åŠ  print è°ƒè¯•è¾“å‡º
                print("\n[DEBUG] Task Analysis Result:")
                print(
                    f"  Type: {task_classification.type.value if task_classification else 'None'}"
                )
                print(
                    f"  Confidence: {task_classification.confidence if task_classification else 'None'}"
                )
                print(
                    f"  Reasoning: {task_classification.reasoning[:200] if task_classification and task_classification.reasoning else 'None'}"
                )
                print()

                # ç²å– API keysï¼ˆæ‰€æœ‰å…è¨±çš„ providersï¼‰
                llm_api_keys = config_resolver.resolve_api_keys_map(
                    tenant_id=tenant_id,
                    user_id=current_user.user_id,
                    providers=allowed_providers,
                )
            else:
                # manual/favoriteï¼šprovider/model override
                selected_model_id = model_selector.model_id or ""
                provider = _infer_provider_from_model_id(selected_model_id)
                model = selected_model_id

                # G6ï¼šmanual/favorite allowlist gate
                if not policy_gate.is_model_allowed(provider.value, selected_model_id):
                    yield f"data: {json.dumps({'type': 'error', 'data': {'error': f'Model {selected_model_id} is not allowed by policy'}})}\n\n"
                    return

                # ç²å– API keysï¼ˆæŒ‡å®šçš„ providerï¼‰
                llm_api_keys = config_resolver.resolve_api_keys_map(
                    tenant_id=tenant_id,
                    user_id=current_user.user_id,
                    providers=[provider.value],
                )

            # æ§‹å»º MoE context
            moe_context: Dict[str, Any] = {
                "user_id": current_user.user_id,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "task_id": task_id,
                "llm_api_keys": llm_api_keys,
            }

            if model_selector.mode == "auto":
                moe_context["allowed_providers"] = allowed_providers
                moe_context["favorite_models"] = favorite_model_ids

            # æ·»åŠ å·¥å…·ä¿¡æ¯åˆ° context
            # allowed_tools å·²åœ¨å‡½æ•°å¼€å§‹å¤„å®šä¹‰ï¼ˆç¬¬1069è¡Œï¼‰
            if allowed_tools:
                moe_context["allowed_tools"] = allowed_tools
                logger.debug(
                    "tools_enabled",
                    request_id=request_id,
                    allowed_tools=allowed_tools,
                )

            # ç™¼é€é–‹å§‹æ¶ˆæ¯ï¼ˆæ­¤æ™‚é‚„ä¸çŸ¥é“ provider å’Œ modelï¼Œå…ˆç™¼é€åŸºæœ¬ä¿¡æ¯ï¼‰
            logger.info(
                "stream_start",
                request_id=request_id,
                messages_count=len(messages_for_llm),
                has_web_search_results=any(
                    "ç¶²çµ¡æœç´¢çµæœ" in str(m.get("content", "")) for m in messages_for_llm
                ),
            )
            yield f"data: {json.dumps({'type': 'start', 'data': {'request_id': request_id, 'session_id': session_id}})}\n\n"

            # ç´¯ç©å®Œæ•´å…§å®¹ï¼ˆç”¨æ–¼å¾ŒçºŒè¨˜éŒ„ï¼‰
            full_content = ""
            chunk_count = 0

            # èª¿ç”¨ MoE Manager çš„ chat_stream æ–¹æ³•
            try:
                logger.info(
                    "moe_chat_stream_start",
                    request_id=request_id,
                    provider=provider.value if provider else None,
                    model=model,
                    task_classification=task_classification.type if task_classification else None,
                )
                async for chunk in moe.chat_stream(
                    messages_for_llm,
                    task_classification=task_classification,
                    provider=provider,
                    model=model,
                    temperature=0.7,
                    max_tokens=2000,
                    context=moe_context,
                ):
                    chunk_count += 1
                    full_content += chunk
                    # ç™¼é€å…§å®¹å¡Š
                    yield f"data: {json.dumps({'type': 'content', 'data': {'chunk': chunk}})}\n\n"

                logger.info(
                    "moe_chat_stream_completed",
                    request_id=request_id,
                    chunk_count=chunk_count,
                    content_length=len(full_content),
                )
            except Exception as stream_exc:
                logger.error(
                    "moe_chat_stream_error",
                    request_id=request_id,
                    error=str(stream_exc),
                    chunk_count=chunk_count,
                    content_length=len(full_content),
                    exc_info=True,
                )
                yield f"data: {json.dumps({'type': 'error', 'data': {'error': str(stream_exc)}})}\n\n"
                return

            # ç™¼é€çµæŸæ¶ˆæ¯
            yield f"data: {json.dumps({'type': 'done', 'data': {'request_id': request_id}})}\n\n"

            # è¨˜éŒ„ assistant messageï¼ˆç•°æ­¥åŸ·è¡Œï¼Œä¸é˜»å¡æµå¼éŸ¿æ‡‰ï¼‰
            _record_if_changed(
                context_manager=context_manager,
                session_id=session_id,
                role="assistant",
                content=full_content,
                metadata={
                    "user_id": current_user.user_id,
                    "session_id": session_id,
                    "task_id": task_id,
                    "request_id": request_id,
                },
            )

        except Exception as exc:
            logger.error(f"Streaming chat error: {exc}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'data': {'error': str(exc)}})}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç·©è¡
        },
    )


@router.post("", status_code=status.HTTP_200_OK)
async def chat_product(
    request_body: ChatRequest,
    request: Request,
    tenant_id: str = Depends(get_current_tenant_id),
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """
    ç”¢å“ç´š Chat å…¥å£ï¼šå‰ç«¯è¼¸å…¥æ¡†çµ±ä¸€å…¥å£ã€‚

    - Autoï¼šTaskClassifier â†’ task_classification â†’ MoE chat
    - Manual/Favoriteï¼šä»¥ model_id æ¨å° providerï¼Œä¸¦åš provider/model override
    """
    moe = get_moe_manager()
    classifier = get_task_classifier()
    context_manager = get_context_manager()
    memory_service = get_chat_memory_service()
    trace_store = get_genai_trace_store_service()
    metrics = get_genai_metrics_service()
    policy_gate = get_genai_policy_gate_service()
    file_permission_service = get_file_permission_service()

    session_id = request_body.session_id or str(uuid.uuid4())
    task_id = request_body.task_id
    request_id = getattr(request.state, "request_id", None) or str(uuid.uuid4())

    messages = [m.model_dump() for m in request_body.messages]
    model_selector = request_body.model_selector

    routing: Dict[str, Any] = {}
    observability = ObservabilityInfo(
        request_id=request_id,
        session_id=session_id,
        task_id=task_id,
        token_input=None,  # type: ignore[call-arg]  # token_input æœ‰é»˜èªå€¼
    )

    start_time = time.perf_counter()

    try:
        # æ–°å¢ï¼šæŠ½å‡ºå¯é‡ç”¨ pipelineï¼ˆä¾› /chat èˆ‡ /chat/requests å…±ç”¨ï¼‰
        response = await _process_chat_request(
            request_body=request_body,
            request_id=request_id,
            tenant_id=tenant_id,
            current_user=current_user,
        )
        return APIResponse.success(
            data=response.model_dump(mode="json"),
            message="Chat success",
        )

        # G5ï¼šå…¥å£äº‹ä»¶ï¼ˆlog + traceï¼‰
        logger.info(
            "genai_chat_request_received",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            model_selector_mode=model_selector.mode,
            model_id=model_selector.model_id,
        )
        trace_store.add_event(
            GenAITraceEvent(
                event="chat.request_received",
                request_id=request_id,
                session_id=session_id,
                task_id=task_id,
                user_id=current_user.user_id,
                status="ok",
            )
        )

        # å–æœ€å¾Œä¸€å‰‡ user messageï¼ˆè‹¥æ‰¾ä¸åˆ°å°±å–æœ€å¾Œä¸€å‰‡ï¼‰
        user_messages = [m for m in messages if m.get("role") == "user"]
        last_user_text = str(user_messages[-1].get("content", "")) if user_messages else ""
        if not last_user_text and messages:
            last_user_text = str(messages[-1].get("content", ""))

        # G3ï¼šå…ˆè¨˜éŒ„ user messageï¼ˆé¿å…é‡è¤‡å¯«å…¥ï¼‰
        _record_if_changed(
            context_manager=context_manager,
            session_id=session_id,
            role="user",
            content=last_user_text,
            metadata={
                "user_id": current_user.user_id,
                "session_id": session_id,
                "task_id": task_id,
                "request_id": request_id,
            },
        )

        # G3ï¼šç”¨ windowed history ä½œç‚º MoE çš„ messagesï¼ˆä¸¦ä¿ç•™å‰ç«¯æä¾›çš„ system messageï¼‰
        system_messages = [m for m in messages if m.get("role") == "system"]
        windowed_history = context_manager.get_context_with_window(session_id=session_id)
        observability.context_message_count = (
            len(windowed_history) if isinstance(windowed_history, list) else 0
        )
        # G6ï¼šé™„ä»¶ file_id æ¬Šé™æª¢æŸ¥ï¼ˆé¿å…é€é RAG è®€åˆ°ä¸å±¬æ–¼è‡ªå·±çš„æ–‡ä»¶ï¼‰
        if request_body.attachments:
            for att in request_body.attachments:
                file_permission_service.check_file_access(
                    user=current_user,
                    file_id=att.file_id,
                    required_permission=Permission.FILE_READ.value,
                )

        # G6ï¼šData consent gateï¼ˆAI_PROCESSINGï¼‰- æœªåŒæ„å‰‡ä¸æª¢ç´¢/ä¸æ³¨å…¥/ä¸å¯«å…¥
        has_ai_consent = False
        try:
            from services.api.models.data_consent import ConsentType

            consent_service = get_consent_service()
            has_ai_consent = consent_service.check_consent(
                current_user.user_id, ConsentType.AI_PROCESSING
            )
        except Exception as exc:  # noqa: BLE001
            logger.warning(
                "genai_consent_check_failed",
                error=str(exc),
                request_id=request_id,
                user_id=current_user.user_id,
            )
            has_ai_consent = False

        if has_ai_consent:
            memory_result = await memory_service.retrieve_for_prompt(
                user_id=current_user.user_id,
                session_id=session_id,
                task_id=task_id,
                request_id=request_id,
                query=last_user_text,
                attachments=request_body.attachments,
            )
            observability.memory_hit_count = memory_result.memory_hit_count
            observability.memory_sources = memory_result.memory_sources
            observability.retrieval_latency_ms = memory_result.retrieval_latency_ms
        else:
            from services.api.services.chat_memory_service import MemoryRetrievalResult

            memory_result = MemoryRetrievalResult(
                injection_messages=[],
                memory_hit_count=0,
                memory_sources=[],
                retrieval_latency_ms=0.0,
            )
            observability.memory_hit_count = 0
            observability.memory_sources = []
            observability.retrieval_latency_ms = 0.0

        base_system = system_messages[:1] if system_messages else []
        messages_for_llm = base_system + memory_result.injection_messages + windowed_history

        trace_store.add_event(
            GenAITraceEvent(
                event="chat.memory_retrieved",
                request_id=request_id,
                session_id=session_id,
                task_id=task_id,
                user_id=current_user.user_id,
                memory_hit_count=observability.memory_hit_count,
                memory_sources=observability.memory_sources,
                retrieval_latency_ms=observability.retrieval_latency_ms,
                context_message_count=observability.context_message_count,
                status="ok",
            )
        )

        llm_call_start = time.perf_counter()
        if model_selector.mode == "auto":
            # G6ï¼šprovider allowlistï¼ˆAutoï¼‰- å°‡ allowlist å‚³çµ¦ MoE åš provider éæ¿¾
            allowed_providers = policy_gate.get_allowed_providers()

            # ç²å–ç”¨æˆ¶çš„æ”¶è—æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨æ–¼ Auto æ¨¡å¼å„ªå…ˆé¸æ“‡ï¼‰
            favorite_model_ids: List[str] = []
            try:
                from services.api.services.user_preference_service import (
                    get_user_preference_service,
                )

                preference_service = get_user_preference_service()
                favorite_model_ids = preference_service.get_favorite_models(
                    user_id=current_user.user_id
                )
            except Exception as exc:  # noqa: BLE001
                logger.debug(
                    f"Failed to get favorite models for user {current_user.user_id}: {exc}"
                )
                # fallback åˆ°å…§å­˜ç·©å­˜
                favorite_model_ids = _favorite_models_by_user.get(current_user.user_id, [])

            task_classification = classifier.classify(
                last_user_text,
                context={
                    "user_id": current_user.user_id,
                    "session_id": session_id,
                    "task_id": task_id,
                },
            )

            result = await moe.chat(
                messages_for_llm,
                task_classification=task_classification,
                context={
                    "user_id": current_user.user_id,
                    "session_id": session_id,
                    "task_id": task_id,
                    "allowed_providers": allowed_providers,
                    "favorite_models": favorite_model_ids,  # å‚³éæ”¶è—æ¨¡å‹åˆ—è¡¨
                },
            )

        else:
            # manual/favoriteï¼šprovider/model override
            selected_model_id = model_selector.model_id or ""
            provider = _infer_provider_from_model_id(selected_model_id)
            # G6ï¼šmanual/favorite allowlist gate
            if not policy_gate.is_model_allowed(provider.value, selected_model_id):
                return APIResponse.error(
                    message="Model is not allowed by policy",
                    error_code="MODEL_NOT_ALLOWED",
                    details={
                        "provider": provider.value,
                        "model_id": selected_model_id,
                    },
                    status_code=status.HTTP_403_FORBIDDEN,
                )
            result = await moe.chat(
                messages_for_llm,
                provider=provider,
                model=selected_model_id,
                context={
                    "user_id": current_user.user_id,
                    "session_id": session_id,
                    "task_id": task_id,
                },
            )

        content = _extract_content(result)
        routing = (result.get("_routing", {}) if isinstance(result, dict) else {}) or {}

        llm_latency_ms = (time.perf_counter() - llm_call_start) * 1000.0
        total_latency_ms = (time.perf_counter() - start_time) * 1000.0

        trace_store.add_event(
            GenAITraceEvent(
                event="chat.llm_completed",
                request_id=request_id,
                session_id=session_id,
                task_id=task_id,
                user_id=current_user.user_id,
                provider=str(routing.get("provider") or "unknown"),
                model=routing.get("model"),
                strategy=str(routing.get("strategy") or "unknown"),
                failover_used=bool(routing.get("failover_used") or False),
                fallback_provider=routing.get("fallback_provider"),
                memory_hit_count=observability.memory_hit_count,
                memory_sources=observability.memory_sources,
                retrieval_latency_ms=observability.retrieval_latency_ms,
                context_message_count=observability.context_message_count,
                total_latency_ms=total_latency_ms,
                llm_latency_ms=llm_latency_ms,
                status="ok",
            )
        )

        # G3ï¼šè¨˜éŒ„ assistant messageï¼ˆé¿å…é‡è¤‡å¯«å…¥ï¼‰
        _record_if_changed(
            context_manager=context_manager,
            session_id=session_id,
            role="assistant",
            content=content,
            metadata={
                "user_id": current_user.user_id,
                "session_id": session_id,
                "task_id": task_id,
                "request_id": request_id,
                "routing": routing,
            },
        )

        # G6ï¼šåŒæ„æ‰å¯«å…¥ long-termï¼ˆMVPï¼šsnippetï¼›å¤±æ•—ä¸é˜»æ“‹å›è¦†ï¼‰
        if has_ai_consent:
            await memory_service.write_from_turn(
                user_id=current_user.user_id,
                session_id=session_id,
                task_id=task_id,
                request_id=request_id,
                user_text=last_user_text,
                assistant_text=content,
            )

        routing_info = RoutingInfo(
            provider=str(routing.get("provider") or "unknown"),
            model=routing.get("model"),
            strategy=str(routing.get("strategy") or "unknown"),
            latency_ms=routing.get("latency_ms"),
            failover_used=bool(routing.get("failover_used") or False),
            fallback_provider=routing.get("fallback_provider"),
        )

        response = ChatResponse(
            content=content,
            session_id=session_id,
            task_id=task_id,
            routing=routing_info,
            observability=observability,
        )

        final_event = GenAITraceEvent(
            event="chat.response_sent",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            provider=routing_info.provider,
            model=routing_info.model,
            strategy=routing_info.strategy,
            failover_used=routing_info.failover_used,
            fallback_provider=routing_info.fallback_provider,
            memory_hit_count=observability.memory_hit_count,
            memory_sources=observability.memory_sources,
            retrieval_latency_ms=observability.retrieval_latency_ms,
            context_message_count=observability.context_message_count,
            total_latency_ms=total_latency_ms,
            llm_latency_ms=llm_latency_ms,
            status="ok",
        )
        trace_store.add_event(final_event)
        metrics.record_final_event(final_event)

        logger.info(
            "genai_chat_response_sent",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            provider=routing_info.provider,
            model=routing_info.model,
            strategy=routing_info.strategy,
            failover_used=routing_info.failover_used,
            fallback_provider=routing_info.fallback_provider,
            memory_hit_count=observability.memory_hit_count,
            memory_sources=observability.memory_sources,
            retrieval_latency_ms=observability.retrieval_latency_ms,
            context_message_count=observability.context_message_count,
            total_latency_ms=total_latency_ms,
            llm_latency_ms=llm_latency_ms,
        )

        return APIResponse.success(
            data=response.model_dump(mode="json"),
            message="Chat success",
        )
    except HTTPException as exc:
        total_latency_ms = (time.perf_counter() - start_time) * 1000.0
        detail = exc.detail
        if isinstance(detail, dict):
            message = str(detail.get("message") or "Request failed")
            error_code = str(detail.get("error_code") or "CHAT_HTTP_ERROR")

            failed_event = GenAITraceEvent(
                event="chat.failed",
                request_id=request_id,
                session_id=session_id,
                task_id=task_id,
                user_id=current_user.user_id,
                status="error",
                error_code=error_code,
                error_message=message,
                total_latency_ms=total_latency_ms,
            )
            trace_store.add_event(failed_event)
            metrics.record_final_event(failed_event)

            return APIResponse.error(
                message=message,
                error_code=error_code,
                details=detail,
                status_code=exc.status_code,
            )

        logger.warning(
            "chat_product_http_error",
            error=str(detail),
            status_code=exc.status_code,
            user_id=current_user.user_id,
            session_id=session_id,
            task_id=task_id,
            request_id=request_id,
        )
        failed_event = GenAITraceEvent(
            event="chat.failed",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            status="error",
            error_code="CHAT_HTTP_ERROR",
            error_message=str(detail),
            total_latency_ms=total_latency_ms,
            memory_hit_count=observability.memory_hit_count,
            memory_sources=observability.memory_sources,
            retrieval_latency_ms=observability.retrieval_latency_ms,
            context_message_count=observability.context_message_count,
        )
        trace_store.add_event(failed_event)
        metrics.record_final_event(failed_event)
        return APIResponse.error(
            message=str(detail),
            error_code="CHAT_HTTP_ERROR",
            status_code=exc.status_code,
        )
    except Exception as exc:  # noqa: BLE001
        total_latency_ms = (time.perf_counter() - start_time) * 1000.0
        logger.error(
            "chat_product_failed",
            error=str(exc),
            user_id=current_user.user_id,
            session_id=session_id,
            task_id=task_id,
            request_id=request_id,
        )

        failed_event = GenAITraceEvent(
            event="chat.failed",
            request_id=request_id,
            session_id=session_id,
            task_id=task_id,
            user_id=current_user.user_id,
            status="error",
            error_code="CHAT_PRODUCT_FAILED",
            error_message=str(exc),
            total_latency_ms=total_latency_ms,
            memory_hit_count=observability.memory_hit_count,
            memory_sources=observability.memory_sources,
            retrieval_latency_ms=observability.retrieval_latency_ms,
            context_message_count=observability.context_message_count,
        )
        trace_store.add_event(failed_event)
        metrics.record_final_event(failed_event)

        return APIResponse.error(
            message=f"Chat failed: {exc}",
            error_code="CHAT_PRODUCT_FAILED",
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        )


async def _run_async_request(
    *, request_id: str, request_body: ChatRequest, tenant_id: str, current_user: User
) -> None:
    store = get_genai_chat_request_store_service()

    try:
        if store.is_aborted(request_id=request_id):
            store.update(request_id=request_id, status=GenAIRequestStatus.aborted)
            return

        store.update(request_id=request_id, status=GenAIRequestStatus.running)

        response = await _process_chat_request(
            request_body=request_body,
            request_id=request_id,
            tenant_id=tenant_id,
            current_user=current_user,
        )

        store.update(
            request_id=request_id,
            status=GenAIRequestStatus.succeeded,
            response=response.model_dump(mode="json"),
        )
    except asyncio.CancelledError:
        store.set_abort(request_id=request_id)
        store.update(request_id=request_id, status=GenAIRequestStatus.aborted)
        raise
    except HTTPException as exc:
        # abortï¼šä½¿ç”¨ 409 çµ±ä¸€è¦–ç‚º aborted
        if exc.status_code == status.HTTP_409_CONFLICT:
            store.set_abort(request_id=request_id)
            store.update(request_id=request_id, status=GenAIRequestStatus.aborted)
            return

        detail = exc.detail
        if isinstance(detail, dict):
            store.update(
                request_id=request_id,
                status=GenAIRequestStatus.failed,
                error_code=str(detail.get("error_code") or "CHAT_HTTP_ERROR"),
                error_message=str(detail.get("message") or "Request failed"),
            )
        else:
            store.update(
                request_id=request_id,
                status=GenAIRequestStatus.failed,
                error_code="CHAT_HTTP_ERROR",
                error_message=str(detail),
            )
    except Exception as exc:  # noqa: BLE001
        store.update(
            request_id=request_id,
            status=GenAIRequestStatus.failed,
            error_code="CHAT_REQUEST_FAILED",
            error_message=str(exc),
        )
    finally:
        _pop_request_task(request_id=request_id)


@router.post("/requests", status_code=status.HTTP_202_ACCEPTED)
async def create_chat_request(
    request_body: ChatRequest,
    background_tasks: BackgroundTasks,
    executor: str = "local",  # local/rq
    tenant_id: str = Depends(get_current_tenant_id),
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """
    éåŒæ­¥ Chat è«‹æ±‚ï¼šç«‹å³å› request_idï¼Œå¾Œç«¯åœ¨èƒŒæ™¯è™•ç†ï¼›å‰ç«¯å¯è¼ªè©¢ç‹€æ…‹æˆ– abortã€‚
    """
    store = get_genai_chat_request_store_service()

    request_id = str(uuid.uuid4())
    session_id = request_body.session_id or str(uuid.uuid4())

    request_dict = request_body.model_dump(mode="json")
    request_dict["session_id"] = session_id

    record = GenAIChatRequestRecord(
        request_id=request_id,
        tenant_id=tenant_id,
        user_id=current_user.user_id,
        session_id=session_id,
        task_id=request_body.task_id,
        status=GenAIRequestStatus.queued,
        request=request_dict,
    )
    store.create(record)

    # executor=rqï¼šé©åˆé•·ä»»å‹™/Agentï¼ˆè‹¥ rq ä¸å¯ç”¨å‰‡è‡ªå‹• fallback to localï¼‰
    if executor == "rq":
        try:
            from database.rq.queue import GENAI_CHAT_QUEUE, get_task_queue
            from workers.genai_chat_job import run_genai_chat_request

            q = get_task_queue(queue_name=GENAI_CHAT_QUEUE)
            q.enqueue(
                run_genai_chat_request,
                request_id,
                request_dict,
                tenant_id,
                current_user.to_dict(),
            )
        except Exception as exc:  # noqa: BLE001
            logger.warning(
                "genai_chat_request_enqueue_failed_fallback_local",
                request_id=request_id,
                error=str(exc),
            )
            executor = "local"

    # executor=localï¼šåŒä¸€é€²ç¨‹å…§èƒŒæ™¯ä»»å‹™ï¼ˆMVPï¼‰
    if executor != "rq":
        background_tasks.add_task(
            _run_async_request,
            request_id=request_id,
            request_body=ChatRequest.model_validate(request_dict),
            tenant_id=tenant_id,
            current_user=current_user,
        )

    payload = GenAIChatRequestCreateResponse(
        request_id=request_id,
        session_id=session_id,
        status=GenAIRequestStatus.queued,
    )
    return APIResponse.success(
        data=payload.model_dump(mode="json"),
        message="Chat request created",
        status_code=status.HTTP_202_ACCEPTED,
    )


@router.get("/requests/{request_id}", status_code=status.HTTP_200_OK)
async def get_chat_request_state(
    request_id: str,
    tenant_id: str = Depends(get_current_tenant_id),
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    store = get_genai_chat_request_store_service()
    record = store.get(request_id=str(request_id))
    if (
        record is None
        or record.user_id != current_user.user_id
        or str(record.tenant_id) != str(tenant_id)
    ):
        return APIResponse.error(
            message="Request not found",
            error_code="REQUEST_NOT_FOUND",
            status_code=status.HTTP_404_NOT_FOUND,
        )

    payload = GenAIChatRequestStateResponse(
        request_id=record.request_id,
        status=record.status,
        created_at_ms=record.created_at_ms,
        updated_at_ms=record.updated_at_ms,
        tenant_id=record.tenant_id,
        session_id=record.session_id,
        task_id=record.task_id,
        response=record.response,
        error_code=record.error_code,
        error_message=record.error_message,
    )
    return APIResponse.success(
        data=payload.model_dump(mode="json"),
        message="Chat request state retrieved",
    )


@router.post("/requests/{request_id}/abort", status_code=status.HTTP_200_OK)
async def abort_chat_request(
    request_id: str,
    tenant_id: str = Depends(get_current_tenant_id),
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    store = get_genai_chat_request_store_service()
    record = store.get(request_id=str(request_id))
    if (
        record is None
        or record.user_id != current_user.user_id
        or str(record.tenant_id) != str(tenant_id)
    ):
        return APIResponse.error(
            message="Request not found",
            error_code="REQUEST_NOT_FOUND",
            status_code=status.HTTP_404_NOT_FOUND,
        )

    store.set_abort(request_id=record.request_id)

    task = _get_request_task(request_id=record.request_id)
    if task is not None and not task.done():
        task.cancel()

    store.update(request_id=record.request_id, status=GenAIRequestStatus.aborted)
    return APIResponse.success(
        data={"request_id": record.request_id, "status": "aborted"},
        message="Chat request aborted",
    )


@router.get("/observability/stats", status_code=status.HTTP_200_OK)
async def get_chat_observability_stats(
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """G5ï¼šç”¢å“ç´š Chat æŒ‡æ¨™å½™ç¸½ï¼ˆJSONï¼‰ã€‚"""
    metrics = get_genai_metrics_service()
    return APIResponse.success(
        data={"stats": metrics.get_stats(), "user_id": current_user.user_id},
        message="Chat observability stats retrieved",
    )


@router.get("/observability/traces/{request_id}", status_code=status.HTTP_200_OK)
async def get_chat_observability_trace(
    request_id: str,
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """G5ï¼šä¾ request_id å›æ”¾äº‹ä»¶åºåˆ—ï¼ˆMVPï¼šin-memoryï¼‰ã€‚"""
    trace_store = get_genai_trace_store_service()
    events = trace_store.get_trace(request_id=str(request_id))
    return APIResponse.success(
        data={
            "request_id": request_id,
            "events": events,
            "user_id": current_user.user_id,
        },
        message="Chat trace retrieved",
    )


@router.get("/observability/recent", status_code=status.HTTP_200_OK)
async def get_chat_observability_recent(
    limit: int = 50,
    session_id: Optional[str] = None,
    task_id: Optional[str] = None,
    event: Optional[str] = None,
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """G5ï¼šåˆ—å‡ºæœ€è¿‘ N ç­†äº‹ä»¶ï¼ˆMVPï¼šin-memoryï¼‰ã€‚"""
    trace_store = get_genai_trace_store_service()
    items = trace_store.list_recent(
        limit=limit,
        user_id=current_user.user_id,
        session_id=session_id,
        task_id=task_id,
        event=event,
    )
    return APIResponse.success(
        data={"events": items, "user_id": current_user.user_id},
        message="Recent chat observability events retrieved",
    )


@router.get("/models", status_code=status.HTTP_200_OK)
async def list_available_models(
    refresh: bool = False,
    include_disallowed: bool = False,
    tenant_id: str = Depends(get_current_tenant_id),
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """
    ç”¢å“ç´šæ¨¡å‹æ¸…å–®ï¼ˆJSONï¼‰ã€‚

    - config: genai.model_registry.models
    - ollama: (å¯é¸) /api/tags å‹•æ…‹ç™¼ç¾ï¼ˆgenai.model_registry.enable_ollama_discoveryï¼‰
    - é è¨­æœƒå¥—ç”¨ genai.policy allowlist éæ¿¾
    """
    registry = get_genai_model_registry_service()
    config_resolver = get_genai_config_resolver_service()
    policy_gate = config_resolver.get_effective_policy_gate(
        tenant_id=tenant_id,
        user_id=current_user.user_id,
    )

    items = await registry.list_models(refresh=refresh)
    if not include_disallowed:
        items = [
            m
            for m in items
            if policy_gate.is_model_allowed(
                str(m.get("provider") or ""),
                str(m.get("model_id") or ""),
            )
        ]

    return APIResponse.success(
        data={"models": items, "user_id": current_user.user_id},
        message="Model list retrieved",
    )


@router.get("/sessions/{session_id}/messages", status_code=status.HTTP_200_OK)
async def get_session_messages(
    session_id: str,
    limit: Optional[int] = None,
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """G3ï¼šSession å›æ”¾ï¼ˆæœ€å°å¯ç”¨ï¼‰ã€‚"""
    context_manager = get_context_manager()
    safe_limit = int(limit) if isinstance(limit, int) and limit and limit > 0 else None
    try:
        messages = context_manager.get_messages(session_id=session_id, limit=safe_limit)
        payload = [m.model_dump(mode="json") for m in messages]
    except Exception as exc:  # noqa: BLE001
        logger.warning(
            "get_session_messages_failed",
            error=str(exc),
            user_id=current_user.user_id,
            session_id=session_id,
        )
        payload = []
    return APIResponse.success(
        data={"session_id": session_id, "messages": payload},
        message="Session messages retrieved",
    )


class FavoriteModelsUpdateRequest(BaseModel):
    """æ”¶è—æ¨¡å‹æ›´æ–°è«‹æ±‚ï¼ˆMVPï¼‰ã€‚"""

    model_ids: List[str] = Field(default_factory=list, description="æ”¶è— model_id åˆ—è¡¨")


@router.get("/preferences/models", status_code=status.HTTP_200_OK)
async def get_favorite_models(
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    user_id = current_user.user_id
    try:
        from services.api.services.user_preference_service import get_user_preference_service

        service = get_user_preference_service()
        model_ids = service.get_favorite_models(user_id=user_id)
    except Exception as exc:  # noqa: BLE001
        logger.warning("favorite_models_service_failed", user_id=user_id, error=str(exc))
        model_ids = _favorite_models_by_user.get(user_id, [])
    return APIResponse.success(
        data={"model_ids": model_ids},
        message="Favorite models retrieved",
    )


@router.put("/preferences/models", status_code=status.HTTP_200_OK)
async def set_favorite_models(
    request_body: FavoriteModelsUpdateRequest,
    tenant_id: str = Depends(get_current_tenant_id),
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    user_id = current_user.user_id
    try:
        # G6ï¼šæ”¶è—æ¨¡å‹å¯«å…¥å‰å…ˆåš allowlist éæ¿¾ï¼ˆå»é™¤ä¸å…è¨±è€…ï¼‰
        config_resolver = get_genai_config_resolver_service()
        policy_gate = config_resolver.get_effective_policy_gate(
            tenant_id=tenant_id,
            user_id=user_id,
        )
        filtered = policy_gate.filter_favorite_models(request_body.model_ids)

        from services.api.services.user_preference_service import get_user_preference_service

        service = get_user_preference_service()
        normalized = service.set_favorite_models(user_id=user_id, model_ids=filtered)
    except Exception as exc:  # noqa: BLE001
        logger.warning("favorite_models_service_failed", user_id=user_id, error=str(exc))
        # å»é‡ä¸”ä¿åºï¼ˆfallbackï¼‰
        seen: set[str] = set()
        normalized = []
        for mid in request_body.model_ids:
            mid = str(mid).strip()
            if not mid or mid in seen:
                continue
            seen.add(mid)
            normalized.append(mid)
        _favorite_models_by_user[user_id] = normalized
    return APIResponse.success(
        data={"model_ids": normalized},
        message="Favorite models updated",
    )
